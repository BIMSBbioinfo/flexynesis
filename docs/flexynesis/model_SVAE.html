<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>flexynesis.model_SVAE API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>flexynesis.model_SVAE</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Supervised VAE-MMD architecture
import torch
from torch import nn
from torch.nn import functional as F

import pandas as pd
import numpy as np

import pytorch_lightning as pl
from scipy import stats

from .models_shared import *

# Supervised Variational Auto-encoder that can train one or more layers of omics datasets 
# num_layers: number of omics layers in the input
# each layer is encoded separately, encodings are concatenated, and decoded separately 
# depends on MLP, Encoder, Decoder classes in models_shared
class supervised_vae(pl.LightningModule):
    &#34;&#34;&#34;
    Supervised Variational Auto-encoder for multi-omics data fusion and prediction.

    This class implements a deep learning model for fusing and predicting from multiple omics layers/matrices.
    Each omics layer is encoded separately using an Encoder. The resulting latent representations are then
    concatenated and passed through a fully connected network (fusion layer) to make predictions. The model
    also includes a supervisor head for supervised learning.

    Args:
        num_layers (int): Number of omics layers/matrices.
        input_dims (list of int): A list of input dimensions for each omics layer.
        hidden_dims (list of int): A list of hidden dimensions for the Encoder and Decoder.
        latent_dim (int): The dimension of the latent space for each encoder.
        num_class (int): Number of output classes for the prediction task.
        **kwargs: Additional keyword arguments to be passed to the MLP encoders.

    Example:

        # Instantiate a supervised_vae model with 2 omics layers and input dimensions of 100 and 200
        model = supervised_vae(num_layers=2, input_dims=[100, 200], hidden_dims=[64, 32], latent_dim=16, num_class=1)

    &#34;&#34;&#34;
    def __init__(self, num_layers, input_dims, hidden_dims, latent_dim, num_class, **kwargs):
        super(supervised_vae, self).__init__()
        
        # define supervisor head
        self.MLP = MLP(latent_dim, num_class, **kwargs)
        self.latent_dim = latent_dim
        self.num_class = num_class
        
        # create a list of Encoder instances for separately encoding each omics layer
        self.encoders = nn.ModuleList([Encoder(input_dims[i], hidden_dims, latent_dim) for i in range(num_layers)])
        # Fully connected layers for concatenated means and log_vars
        self.FC_mean = nn.Linear(num_layers * latent_dim, latent_dim)
        self.FC_log_var = nn.Linear(num_layers * latent_dim, latent_dim)
        
        # list of decoders to decode each omics layer separately 
        self.decoders = nn.ModuleList([Decoder(latent_dim, hidden_dims[::-1], input_dims[i]) for i in range(num_layers)])

    def multi_encoder(self, x_list):
        &#34;&#34;&#34;
        Encode each input matrix separately using the corresponding Encoder.

        Args:
            x_list (list of torch.Tensor): List of input matrices for each omics layer.

        Returns:
            tuple: Tuple containing:
                - mean (torch.Tensor): Concatenated mean values from each encoder.
                - log_var (torch.Tensor): Concatenated log variance values from each encoder.
        &#34;&#34;&#34;
        means, log_vars = [], []
        # Process each input matrix with its corresponding Encoder
        for i, x in enumerate(x_list):
            mean, log_var = self.encoders[i](x)
            means.append(mean)
            log_vars.append(log_var)

        # Concatenate means and log_vars
        # Push concatenated means and log_vars through the fully connected layers
        mean = self.FC_mean(torch.cat(means, dim=1))
        log_var = self.FC_log_var(torch.cat(log_vars, dim=1))
        return mean, log_var
    
    def forward(self, x_list):
        &#34;&#34;&#34;
        Forward pass through the model.

        Args:
            x_list (list of torch.Tensor): List of input matrices for each omics layer.

        Returns:
            tuple: Tuple containing:
                - x_hat_list (list of torch.Tensor): List of reconstructed matrices for each omics layer.
                - z (torch.Tensor): Latent representation.
                - mean (torch.Tensor): Concatenated mean values from each encoder.
                - log_var (torch.Tensor): Concatenated log variance values from each encoder.
                - y_pred (torch.Tensor): Predicted output.
        &#34;&#34;&#34;
        mean, log_var = self.multi_encoder(x_list)
        
        # generate latent layer
        z = self.reparameterization(mean, log_var)

        # Decode each latent variable with its corresponding Decoder
        x_hat_list = [self.decoders[i](z) for i in range(len(x_list))]

        #run the supervisor 
        y_pred = self.MLP(z)
        
        return x_hat_list, z, mean, log_var, y_pred
        
    def reparameterization(self, mean, var):
        &#34;&#34;&#34;
        Reparameterize the mean and variance values.

        Args:
            mean (torch.Tensor): Mean values from the encoders.
            var (torch.Tensor): Variance values from the encoders.

        Returns:
            torch.Tensor: Latent representation.
        &#34;&#34;&#34;
        epsilon = torch.randn_like(var)       
        z = mean + var*epsilon                         
        return z
    
    def configure_optimizers(self):
        &#34;&#34;&#34;
        Configure the optimizer for the model.

        Returns:
            torch.optim.Adam: Adam optimizer with learning rate 1e-3.
        &#34;&#34;&#34;
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
        return optimizer

    def training_step(self, train_batch, batch_idx):
        &#34;&#34;&#34;
        Perform a training step.

        Args:
            train_batch (tuple): Tuple containing:
                - dat (dict): Dictionary containing input matrices for each omics layer.
                - y (torch.Tensor): Ground truth labels.
            batch_idx (int): Index of the current batch.

        Returns:
            torch.Tensor: Loss value for the current training step.
        &#34;&#34;&#34;
        dat, y = train_batch
        layers = dat.keys()
        x_list = [dat[x] for x in layers]
        mean, log_var = self.multi_encoder(x_list)
        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -&gt; var)
        x_hat_list = [self.decoders[i](z) for i in range(len(x_list))]
        y_pred = self.MLP(z)
        
        # compute mmd loss for each layer and take average
        mmd_loss_list = [self.MMD_loss(z.shape[1], z, x_hat_list[i], x_list[i]) for i in range(len(layers))]
        mmd_loss = torch.mean(torch.stack(mmd_loss_list))

        sp_loss = F.mse_loss(y_pred, y)
        loss = mmd_loss + sp_loss
        
        self.log(&#39;train_loss&#39;, loss)
        return loss
    
    def validation_step(self, val_batch, batch_idx):
        &#34;&#34;&#34;
        Perform a validation step.

        Args:
            val_batch (tuple): Tuple containing:
                - dat (dict): Dictionary containing input matrices for each omics layer.
                - y (torch.Tensor): Ground truth labels.
            batch_idx (int): Index of the current batch.

        Returns:
            torch.Tensor: Loss value for the current validation step.
        &#34;&#34;&#34;

        dat, y = val_batch
        layers = dat.keys()
        x_list = [dat[x] for x in layers]
        mean, log_var = self.multi_encoder(x_list)
        
        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -&gt; var)
        x_hat_list = [self.decoders[i](z) for i in range(len(x_list))]
        y_pred = self.MLP(z)
        
        # compute mmd loss for each layer and take average
        mmd_loss_list = [self.MMD_loss(z.shape[1], z, x_hat_list[i], x_list[i]) for i in range(len(layers))]
        mmd_loss = torch.mean(torch.stack(mmd_loss_list))

        sp_loss = F.mse_loss(y_pred, y)
        loss = mmd_loss + sp_loss
        
        self.log(&#39;val_loss&#39;, loss)
        return loss
    
    def transform(self, dataset):
        &#34;&#34;&#34;
        Transform the input dataset to latent representation.

        Args:
            dataset (MultiOmicDataset): MultiOmicDataset containing input matrices for each omics layer.

        Returns:
            pd.DataFrame: Transformed dataset as a pandas DataFrame.
        &#34;&#34;&#34;
        self.eval()
        layers = list(dataset.dat.keys())
        x_list = [dataset.dat[x] for x in layers]
        M = self.forward(x_list)[1].detach().numpy()
        z = pd.DataFrame(M)
        z.columns = [&#39;&#39;.join([&#39;LF&#39;, str(x+1)]) for x in z.columns]
        z.index = dataset.samples
        return z
    
    def evaluate(self, dataset):
        &#34;&#34;&#34;
        Evaluate the model on a dataset.

        Args:
            dataset (CustomDataset): Custom dataset containing input matrices for each omics layer.

        Returns:
            float: Pearson correlation coefficient between true and predicted values.
        &#34;&#34;&#34;
        self.eval()
        layers = list(dataset.dat.keys())
        x_list = [dataset.dat[x] for x in layers]
        X_hat, z, mean, log_var, y_pred = self.forward(x_list)
        r_value = stats.linregress(dataset.y.detach().numpy(),
                                   torch.flatten(y_pred).detach().numpy())[2]
        return r_value
    
    def compute_kernel(self, x, y):
        &#34;&#34;&#34;
        Compute the Gaussian kernel matrix between two sets of vectors.

        Args:
            x (torch.Tensor): A tensor of shape (x_size, dim) representing the first set of vectors.
            y (torch.Tensor): A tensor of shape (y_size, dim) representing the second set of vectors.

        Returns:
            torch.Tensor: The Gaussian kernel matrix of shape (x_size, y_size) computed between x and y.
        &#34;&#34;&#34;
        x_size = x.size(0)
        y_size = y.size(0)
        dim = x.size(1)
        x = x.unsqueeze(1) # (x_size, 1, dim)
        y = y.unsqueeze(0) # (1, y_size, dim)
        tiled_x = x.expand(x_size, y_size, dim)
        tiled_y = y.expand(x_size, y_size, dim)
        kernel_input = (tiled_x - tiled_y).pow(2).mean(2)/float(dim)
        return torch.exp(-kernel_input) # (x_size, y_size)

    def compute_mmd(self, x, y):
        &#34;&#34;&#34;
        Compute the maximum mean discrepancy (MMD) between two sets of vectors.

        Args:
            x (torch.Tensor): A tensor of shape (x_size, dim) representing the first set of vectors.
            y (torch.Tensor): A tensor of shape (y_size, dim) representing the second set of vectors.

        Returns:
            torch.Tensor: A scalar tensor representing the MMD between x and y.
        &#34;&#34;&#34;
        x_kernel = self.compute_kernel(x, x)
        y_kernel = self.compute_kernel(y, y)
        xy_kernel = self.compute_kernel(x, y)
        mmd = x_kernel.mean() + y_kernel.mean() - 2*xy_kernel.mean()
        return mmd

    def MMD_loss(self, latent_dim, z, xhat, x):
        &#34;&#34;&#34;
        Compute the loss function based on maximum mean discrepancy (MMD) and negative log likelihood (NLL).

        Args:
            latent_dim (int): The dimensionality of the latent space.
            z (torch.Tensor): A tensor of shape (batch_size, latent_dim) representing the latent codes.
            xhat (torch.Tensor): A tensor of shape (batch_size, dim) representing the reconstructed data.
            x (torch.Tensor): A tensor of shape (batch_size, dim) representing the original data.

        Returns:
            torch.Tensor: A scalar tensor representing the MMD loss.
        &#34;&#34;&#34;
        true_samples = torch.randn(200, latent_dim)
        mmd = self.compute_mmd(true_samples, z) # compute maximum mean discrepancy (MMD)
        nll = (xhat - x).pow(2).mean() #negative log likelihood
        return mmd+nll


    </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="flexynesis.model_SVAE.supervised_vae"><code class="flex name class">
<span>class <span class="ident">supervised_vae</span></span>
<span>(</span><span>num_layers, input_dims, hidden_dims, latent_dim, num_class, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Supervised Variational Auto-encoder for multi-omics data fusion and prediction.</p>
<p>This class implements a deep learning model for fusing and predicting from multiple omics layers/matrices.
Each omics layer is encoded separately using an Encoder. The resulting latent representations are then
concatenated and passed through a fully connected network (fusion layer) to make predictions. The model
also includes a supervisor head for supervised learning.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>num_layers</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of omics layers/matrices.</dd>
<dt><strong><code>input_dims</code></strong> :&ensp;<code>list</code> of <code>int</code></dt>
<dd>A list of input dimensions for each omics layer.</dd>
<dt><strong><code>hidden_dims</code></strong> :&ensp;<code>list</code> of <code>int</code></dt>
<dd>A list of hidden dimensions for the Encoder and Decoder.</dd>
<dt><strong><code>latent_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>The dimension of the latent space for each encoder.</dd>
<dt><strong><code>num_class</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output classes for the prediction task.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments to be passed to the MLP encoders.</dd>
</dl>
<h2 id="example">Example</h2>
<h1 id="instantiate-a-supervised_vae-model-with-2-omics-layers-and-input-dimensions-of-100-and-200">Instantiate a supervised_vae model with 2 omics layers and input dimensions of 100 and 200</h1>
<p>model = supervised_vae(num_layers=2, input_dims=[100, 200], hidden_dims=[64, 32], latent_dim=16, num_class=1)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class supervised_vae(pl.LightningModule):
    &#34;&#34;&#34;
    Supervised Variational Auto-encoder for multi-omics data fusion and prediction.

    This class implements a deep learning model for fusing and predicting from multiple omics layers/matrices.
    Each omics layer is encoded separately using an Encoder. The resulting latent representations are then
    concatenated and passed through a fully connected network (fusion layer) to make predictions. The model
    also includes a supervisor head for supervised learning.

    Args:
        num_layers (int): Number of omics layers/matrices.
        input_dims (list of int): A list of input dimensions for each omics layer.
        hidden_dims (list of int): A list of hidden dimensions for the Encoder and Decoder.
        latent_dim (int): The dimension of the latent space for each encoder.
        num_class (int): Number of output classes for the prediction task.
        **kwargs: Additional keyword arguments to be passed to the MLP encoders.

    Example:

        # Instantiate a supervised_vae model with 2 omics layers and input dimensions of 100 and 200
        model = supervised_vae(num_layers=2, input_dims=[100, 200], hidden_dims=[64, 32], latent_dim=16, num_class=1)

    &#34;&#34;&#34;
    def __init__(self, num_layers, input_dims, hidden_dims, latent_dim, num_class, **kwargs):
        super(supervised_vae, self).__init__()
        
        # define supervisor head
        self.MLP = MLP(latent_dim, num_class, **kwargs)
        self.latent_dim = latent_dim
        self.num_class = num_class
        
        # create a list of Encoder instances for separately encoding each omics layer
        self.encoders = nn.ModuleList([Encoder(input_dims[i], hidden_dims, latent_dim) for i in range(num_layers)])
        # Fully connected layers for concatenated means and log_vars
        self.FC_mean = nn.Linear(num_layers * latent_dim, latent_dim)
        self.FC_log_var = nn.Linear(num_layers * latent_dim, latent_dim)
        
        # list of decoders to decode each omics layer separately 
        self.decoders = nn.ModuleList([Decoder(latent_dim, hidden_dims[::-1], input_dims[i]) for i in range(num_layers)])

    def multi_encoder(self, x_list):
        &#34;&#34;&#34;
        Encode each input matrix separately using the corresponding Encoder.

        Args:
            x_list (list of torch.Tensor): List of input matrices for each omics layer.

        Returns:
            tuple: Tuple containing:
                - mean (torch.Tensor): Concatenated mean values from each encoder.
                - log_var (torch.Tensor): Concatenated log variance values from each encoder.
        &#34;&#34;&#34;
        means, log_vars = [], []
        # Process each input matrix with its corresponding Encoder
        for i, x in enumerate(x_list):
            mean, log_var = self.encoders[i](x)
            means.append(mean)
            log_vars.append(log_var)

        # Concatenate means and log_vars
        # Push concatenated means and log_vars through the fully connected layers
        mean = self.FC_mean(torch.cat(means, dim=1))
        log_var = self.FC_log_var(torch.cat(log_vars, dim=1))
        return mean, log_var
    
    def forward(self, x_list):
        &#34;&#34;&#34;
        Forward pass through the model.

        Args:
            x_list (list of torch.Tensor): List of input matrices for each omics layer.

        Returns:
            tuple: Tuple containing:
                - x_hat_list (list of torch.Tensor): List of reconstructed matrices for each omics layer.
                - z (torch.Tensor): Latent representation.
                - mean (torch.Tensor): Concatenated mean values from each encoder.
                - log_var (torch.Tensor): Concatenated log variance values from each encoder.
                - y_pred (torch.Tensor): Predicted output.
        &#34;&#34;&#34;
        mean, log_var = self.multi_encoder(x_list)
        
        # generate latent layer
        z = self.reparameterization(mean, log_var)

        # Decode each latent variable with its corresponding Decoder
        x_hat_list = [self.decoders[i](z) for i in range(len(x_list))]

        #run the supervisor 
        y_pred = self.MLP(z)
        
        return x_hat_list, z, mean, log_var, y_pred
        
    def reparameterization(self, mean, var):
        &#34;&#34;&#34;
        Reparameterize the mean and variance values.

        Args:
            mean (torch.Tensor): Mean values from the encoders.
            var (torch.Tensor): Variance values from the encoders.

        Returns:
            torch.Tensor: Latent representation.
        &#34;&#34;&#34;
        epsilon = torch.randn_like(var)       
        z = mean + var*epsilon                         
        return z
    
    def configure_optimizers(self):
        &#34;&#34;&#34;
        Configure the optimizer for the model.

        Returns:
            torch.optim.Adam: Adam optimizer with learning rate 1e-3.
        &#34;&#34;&#34;
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
        return optimizer

    def training_step(self, train_batch, batch_idx):
        &#34;&#34;&#34;
        Perform a training step.

        Args:
            train_batch (tuple): Tuple containing:
                - dat (dict): Dictionary containing input matrices for each omics layer.
                - y (torch.Tensor): Ground truth labels.
            batch_idx (int): Index of the current batch.

        Returns:
            torch.Tensor: Loss value for the current training step.
        &#34;&#34;&#34;
        dat, y = train_batch
        layers = dat.keys()
        x_list = [dat[x] for x in layers]
        mean, log_var = self.multi_encoder(x_list)
        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -&gt; var)
        x_hat_list = [self.decoders[i](z) for i in range(len(x_list))]
        y_pred = self.MLP(z)
        
        # compute mmd loss for each layer and take average
        mmd_loss_list = [self.MMD_loss(z.shape[1], z, x_hat_list[i], x_list[i]) for i in range(len(layers))]
        mmd_loss = torch.mean(torch.stack(mmd_loss_list))

        sp_loss = F.mse_loss(y_pred, y)
        loss = mmd_loss + sp_loss
        
        self.log(&#39;train_loss&#39;, loss)
        return loss
    
    def validation_step(self, val_batch, batch_idx):
        &#34;&#34;&#34;
        Perform a validation step.

        Args:
            val_batch (tuple): Tuple containing:
                - dat (dict): Dictionary containing input matrices for each omics layer.
                - y (torch.Tensor): Ground truth labels.
            batch_idx (int): Index of the current batch.

        Returns:
            torch.Tensor: Loss value for the current validation step.
        &#34;&#34;&#34;

        dat, y = val_batch
        layers = dat.keys()
        x_list = [dat[x] for x in layers]
        mean, log_var = self.multi_encoder(x_list)
        
        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -&gt; var)
        x_hat_list = [self.decoders[i](z) for i in range(len(x_list))]
        y_pred = self.MLP(z)
        
        # compute mmd loss for each layer and take average
        mmd_loss_list = [self.MMD_loss(z.shape[1], z, x_hat_list[i], x_list[i]) for i in range(len(layers))]
        mmd_loss = torch.mean(torch.stack(mmd_loss_list))

        sp_loss = F.mse_loss(y_pred, y)
        loss = mmd_loss + sp_loss
        
        self.log(&#39;val_loss&#39;, loss)
        return loss
    
    def transform(self, dataset):
        &#34;&#34;&#34;
        Transform the input dataset to latent representation.

        Args:
            dataset (MultiOmicDataset): MultiOmicDataset containing input matrices for each omics layer.

        Returns:
            pd.DataFrame: Transformed dataset as a pandas DataFrame.
        &#34;&#34;&#34;
        self.eval()
        layers = list(dataset.dat.keys())
        x_list = [dataset.dat[x] for x in layers]
        M = self.forward(x_list)[1].detach().numpy()
        z = pd.DataFrame(M)
        z.columns = [&#39;&#39;.join([&#39;LF&#39;, str(x+1)]) for x in z.columns]
        z.index = dataset.samples
        return z
    
    def evaluate(self, dataset):
        &#34;&#34;&#34;
        Evaluate the model on a dataset.

        Args:
            dataset (CustomDataset): Custom dataset containing input matrices for each omics layer.

        Returns:
            float: Pearson correlation coefficient between true and predicted values.
        &#34;&#34;&#34;
        self.eval()
        layers = list(dataset.dat.keys())
        x_list = [dataset.dat[x] for x in layers]
        X_hat, z, mean, log_var, y_pred = self.forward(x_list)
        r_value = stats.linregress(dataset.y.detach().numpy(),
                                   torch.flatten(y_pred).detach().numpy())[2]
        return r_value
    
    def compute_kernel(self, x, y):
        &#34;&#34;&#34;
        Compute the Gaussian kernel matrix between two sets of vectors.

        Args:
            x (torch.Tensor): A tensor of shape (x_size, dim) representing the first set of vectors.
            y (torch.Tensor): A tensor of shape (y_size, dim) representing the second set of vectors.

        Returns:
            torch.Tensor: The Gaussian kernel matrix of shape (x_size, y_size) computed between x and y.
        &#34;&#34;&#34;
        x_size = x.size(0)
        y_size = y.size(0)
        dim = x.size(1)
        x = x.unsqueeze(1) # (x_size, 1, dim)
        y = y.unsqueeze(0) # (1, y_size, dim)
        tiled_x = x.expand(x_size, y_size, dim)
        tiled_y = y.expand(x_size, y_size, dim)
        kernel_input = (tiled_x - tiled_y).pow(2).mean(2)/float(dim)
        return torch.exp(-kernel_input) # (x_size, y_size)

    def compute_mmd(self, x, y):
        &#34;&#34;&#34;
        Compute the maximum mean discrepancy (MMD) between two sets of vectors.

        Args:
            x (torch.Tensor): A tensor of shape (x_size, dim) representing the first set of vectors.
            y (torch.Tensor): A tensor of shape (y_size, dim) representing the second set of vectors.

        Returns:
            torch.Tensor: A scalar tensor representing the MMD between x and y.
        &#34;&#34;&#34;
        x_kernel = self.compute_kernel(x, x)
        y_kernel = self.compute_kernel(y, y)
        xy_kernel = self.compute_kernel(x, y)
        mmd = x_kernel.mean() + y_kernel.mean() - 2*xy_kernel.mean()
        return mmd

    def MMD_loss(self, latent_dim, z, xhat, x):
        &#34;&#34;&#34;
        Compute the loss function based on maximum mean discrepancy (MMD) and negative log likelihood (NLL).

        Args:
            latent_dim (int): The dimensionality of the latent space.
            z (torch.Tensor): A tensor of shape (batch_size, latent_dim) representing the latent codes.
            xhat (torch.Tensor): A tensor of shape (batch_size, dim) representing the reconstructed data.
            x (torch.Tensor): A tensor of shape (batch_size, dim) representing the original data.

        Returns:
            torch.Tensor: A scalar tensor representing the MMD loss.
        &#34;&#34;&#34;
        true_samples = torch.randn(200, latent_dim)
        mmd = self.compute_mmd(true_samples, z) # compute maximum mean discrepancy (MMD)
        nll = (xhat - x).pow(2).mean() #negative log likelihood
        return mmd+nll</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pytorch_lightning.core.module.LightningModule</li>
<li>lightning_lite.utilities.device_dtype_mixin._DeviceDtypeModuleMixin</li>
<li>pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin</li>
<li>pytorch_lightning.core.saving.ModelIO</li>
<li>pytorch_lightning.core.hooks.ModelHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="flexynesis.model_SVAE.supervised_vae.MMD_loss"><code class="name flex">
<span>def <span class="ident">MMD_loss</span></span>(<span>self, latent_dim, z, xhat, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the loss function based on maximum mean discrepancy (MMD) and negative log likelihood (NLL).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>latent_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>The dimensionality of the latent space.</dd>
<dt><strong><code>z</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>A tensor of shape (batch_size, latent_dim) representing the latent codes.</dd>
<dt><strong><code>xhat</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>A tensor of shape (batch_size, dim) representing the reconstructed data.</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>A tensor of shape (batch_size, dim) representing the original data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>A scalar tensor representing the MMD loss.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def MMD_loss(self, latent_dim, z, xhat, x):
    &#34;&#34;&#34;
    Compute the loss function based on maximum mean discrepancy (MMD) and negative log likelihood (NLL).

    Args:
        latent_dim (int): The dimensionality of the latent space.
        z (torch.Tensor): A tensor of shape (batch_size, latent_dim) representing the latent codes.
        xhat (torch.Tensor): A tensor of shape (batch_size, dim) representing the reconstructed data.
        x (torch.Tensor): A tensor of shape (batch_size, dim) representing the original data.

    Returns:
        torch.Tensor: A scalar tensor representing the MMD loss.
    &#34;&#34;&#34;
    true_samples = torch.randn(200, latent_dim)
    mmd = self.compute_mmd(true_samples, z) # compute maximum mean discrepancy (MMD)
    nll = (xhat - x).pow(2).mean() #negative log likelihood
    return mmd+nll</code></pre>
</details>
</dd>
<dt id="flexynesis.model_SVAE.supervised_vae.compute_kernel"><code class="name flex">
<span>def <span class="ident">compute_kernel</span></span>(<span>self, x, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the Gaussian kernel matrix between two sets of vectors.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>A tensor of shape (x_size, dim) representing the first set of vectors.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>A tensor of shape (y_size, dim) representing the second set of vectors.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>The Gaussian kernel matrix of shape (x_size, y_size) computed between x and y.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_kernel(self, x, y):
    &#34;&#34;&#34;
    Compute the Gaussian kernel matrix between two sets of vectors.

    Args:
        x (torch.Tensor): A tensor of shape (x_size, dim) representing the first set of vectors.
        y (torch.Tensor): A tensor of shape (y_size, dim) representing the second set of vectors.

    Returns:
        torch.Tensor: The Gaussian kernel matrix of shape (x_size, y_size) computed between x and y.
    &#34;&#34;&#34;
    x_size = x.size(0)
    y_size = y.size(0)
    dim = x.size(1)
    x = x.unsqueeze(1) # (x_size, 1, dim)
    y = y.unsqueeze(0) # (1, y_size, dim)
    tiled_x = x.expand(x_size, y_size, dim)
    tiled_y = y.expand(x_size, y_size, dim)
    kernel_input = (tiled_x - tiled_y).pow(2).mean(2)/float(dim)
    return torch.exp(-kernel_input) # (x_size, y_size)</code></pre>
</details>
</dd>
<dt id="flexynesis.model_SVAE.supervised_vae.compute_mmd"><code class="name flex">
<span>def <span class="ident">compute_mmd</span></span>(<span>self, x, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the maximum mean discrepancy (MMD) between two sets of vectors.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>A tensor of shape (x_size, dim) representing the first set of vectors.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>A tensor of shape (y_size, dim) representing the second set of vectors.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>A scalar tensor representing the MMD between x and y.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_mmd(self, x, y):
    &#34;&#34;&#34;
    Compute the maximum mean discrepancy (MMD) between two sets of vectors.

    Args:
        x (torch.Tensor): A tensor of shape (x_size, dim) representing the first set of vectors.
        y (torch.Tensor): A tensor of shape (y_size, dim) representing the second set of vectors.

    Returns:
        torch.Tensor: A scalar tensor representing the MMD between x and y.
    &#34;&#34;&#34;
    x_kernel = self.compute_kernel(x, x)
    y_kernel = self.compute_kernel(y, y)
    xy_kernel = self.compute_kernel(x, y)
    mmd = x_kernel.mean() + y_kernel.mean() - 2*xy_kernel.mean()
    return mmd</code></pre>
</details>
</dd>
<dt id="flexynesis.model_SVAE.supervised_vae.configure_optimizers"><code class="name flex">
<span>def <span class="ident">configure_optimizers</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Configure the optimizer for the model.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.optim.Adam</code></dt>
<dd>Adam optimizer with learning rate 1e-3.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure_optimizers(self):
    &#34;&#34;&#34;
    Configure the optimizer for the model.

    Returns:
        torch.optim.Adam: Adam optimizer with learning rate 1e-3.
    &#34;&#34;&#34;
    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
    return optimizer</code></pre>
</details>
</dd>
<dt id="flexynesis.model_SVAE.supervised_vae.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self, dataset)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the model on a dataset.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>CustomDataset</code></dt>
<dd>Custom dataset containing input matrices for each omics layer.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>Pearson correlation coefficient between true and predicted values.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate(self, dataset):
    &#34;&#34;&#34;
    Evaluate the model on a dataset.

    Args:
        dataset (CustomDataset): Custom dataset containing input matrices for each omics layer.

    Returns:
        float: Pearson correlation coefficient between true and predicted values.
    &#34;&#34;&#34;
    self.eval()
    layers = list(dataset.dat.keys())
    x_list = [dataset.dat[x] for x in layers]
    X_hat, z, mean, log_var, y_pred = self.forward(x_list)
    r_value = stats.linregress(dataset.y.detach().numpy(),
                               torch.flatten(y_pred).detach().numpy())[2]
    return r_value</code></pre>
</details>
</dd>
<dt id="flexynesis.model_SVAE.supervised_vae.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x_list) -> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass through the model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x_list</code></strong> :&ensp;<code>list</code> of <code>torch.Tensor</code></dt>
<dd>List of input matrices for each omics layer.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>Tuple containing:
- x_hat_list (list of torch.Tensor): List of reconstructed matrices for each omics layer.
- z (torch.Tensor): Latent representation.
- mean (torch.Tensor): Concatenated mean values from each encoder.
- log_var (torch.Tensor): Concatenated log variance values from each encoder.
- y_pred (torch.Tensor): Predicted output.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x_list):
    &#34;&#34;&#34;
    Forward pass through the model.

    Args:
        x_list (list of torch.Tensor): List of input matrices for each omics layer.

    Returns:
        tuple: Tuple containing:
            - x_hat_list (list of torch.Tensor): List of reconstructed matrices for each omics layer.
            - z (torch.Tensor): Latent representation.
            - mean (torch.Tensor): Concatenated mean values from each encoder.
            - log_var (torch.Tensor): Concatenated log variance values from each encoder.
            - y_pred (torch.Tensor): Predicted output.
    &#34;&#34;&#34;
    mean, log_var = self.multi_encoder(x_list)
    
    # generate latent layer
    z = self.reparameterization(mean, log_var)

    # Decode each latent variable with its corresponding Decoder
    x_hat_list = [self.decoders[i](z) for i in range(len(x_list))]

    #run the supervisor 
    y_pred = self.MLP(z)
    
    return x_hat_list, z, mean, log_var, y_pred</code></pre>
</details>
</dd>
<dt id="flexynesis.model_SVAE.supervised_vae.multi_encoder"><code class="name flex">
<span>def <span class="ident">multi_encoder</span></span>(<span>self, x_list)</span>
</code></dt>
<dd>
<div class="desc"><p>Encode each input matrix separately using the corresponding Encoder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x_list</code></strong> :&ensp;<code>list</code> of <code>torch.Tensor</code></dt>
<dd>List of input matrices for each omics layer.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>Tuple containing:
- mean (torch.Tensor): Concatenated mean values from each encoder.
- log_var (torch.Tensor): Concatenated log variance values from each encoder.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def multi_encoder(self, x_list):
    &#34;&#34;&#34;
    Encode each input matrix separately using the corresponding Encoder.

    Args:
        x_list (list of torch.Tensor): List of input matrices for each omics layer.

    Returns:
        tuple: Tuple containing:
            - mean (torch.Tensor): Concatenated mean values from each encoder.
            - log_var (torch.Tensor): Concatenated log variance values from each encoder.
    &#34;&#34;&#34;
    means, log_vars = [], []
    # Process each input matrix with its corresponding Encoder
    for i, x in enumerate(x_list):
        mean, log_var = self.encoders[i](x)
        means.append(mean)
        log_vars.append(log_var)

    # Concatenate means and log_vars
    # Push concatenated means and log_vars through the fully connected layers
    mean = self.FC_mean(torch.cat(means, dim=1))
    log_var = self.FC_log_var(torch.cat(log_vars, dim=1))
    return mean, log_var</code></pre>
</details>
</dd>
<dt id="flexynesis.model_SVAE.supervised_vae.reparameterization"><code class="name flex">
<span>def <span class="ident">reparameterization</span></span>(<span>self, mean, var)</span>
</code></dt>
<dd>
<div class="desc"><p>Reparameterize the mean and variance values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mean</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Mean values from the encoders.</dd>
<dt><strong><code>var</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Variance values from the encoders.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Latent representation.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reparameterization(self, mean, var):
    &#34;&#34;&#34;
    Reparameterize the mean and variance values.

    Args:
        mean (torch.Tensor): Mean values from the encoders.
        var (torch.Tensor): Variance values from the encoders.

    Returns:
        torch.Tensor: Latent representation.
    &#34;&#34;&#34;
    epsilon = torch.randn_like(var)       
    z = mean + var*epsilon                         
    return z</code></pre>
</details>
</dd>
<dt id="flexynesis.model_SVAE.supervised_vae.training_step"><code class="name flex">
<span>def <span class="ident">training_step</span></span>(<span>self, train_batch, batch_idx)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform a training step.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>train_batch</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Tuple containing:
- dat (dict): Dictionary containing input matrices for each omics layer.
- y (torch.Tensor): Ground truth labels.</dd>
<dt><strong><code>batch_idx</code></strong> :&ensp;<code>int</code></dt>
<dd>Index of the current batch.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Loss value for the current training step.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def training_step(self, train_batch, batch_idx):
    &#34;&#34;&#34;
    Perform a training step.

    Args:
        train_batch (tuple): Tuple containing:
            - dat (dict): Dictionary containing input matrices for each omics layer.
            - y (torch.Tensor): Ground truth labels.
        batch_idx (int): Index of the current batch.

    Returns:
        torch.Tensor: Loss value for the current training step.
    &#34;&#34;&#34;
    dat, y = train_batch
    layers = dat.keys()
    x_list = [dat[x] for x in layers]
    mean, log_var = self.multi_encoder(x_list)
    z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -&gt; var)
    x_hat_list = [self.decoders[i](z) for i in range(len(x_list))]
    y_pred = self.MLP(z)
    
    # compute mmd loss for each layer and take average
    mmd_loss_list = [self.MMD_loss(z.shape[1], z, x_hat_list[i], x_list[i]) for i in range(len(layers))]
    mmd_loss = torch.mean(torch.stack(mmd_loss_list))

    sp_loss = F.mse_loss(y_pred, y)
    loss = mmd_loss + sp_loss
    
    self.log(&#39;train_loss&#39;, loss)
    return loss</code></pre>
</details>
</dd>
<dt id="flexynesis.model_SVAE.supervised_vae.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, dataset)</span>
</code></dt>
<dd>
<div class="desc"><p>Transform the input dataset to latent representation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>MultiOmicDataset</code></dt>
<dd>MultiOmicDataset containing input matrices for each omics layer.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>Transformed dataset as a pandas DataFrame.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, dataset):
    &#34;&#34;&#34;
    Transform the input dataset to latent representation.

    Args:
        dataset (MultiOmicDataset): MultiOmicDataset containing input matrices for each omics layer.

    Returns:
        pd.DataFrame: Transformed dataset as a pandas DataFrame.
    &#34;&#34;&#34;
    self.eval()
    layers = list(dataset.dat.keys())
    x_list = [dataset.dat[x] for x in layers]
    M = self.forward(x_list)[1].detach().numpy()
    z = pd.DataFrame(M)
    z.columns = [&#39;&#39;.join([&#39;LF&#39;, str(x+1)]) for x in z.columns]
    z.index = dataset.samples
    return z</code></pre>
</details>
</dd>
<dt id="flexynesis.model_SVAE.supervised_vae.validation_step"><code class="name flex">
<span>def <span class="ident">validation_step</span></span>(<span>self, val_batch, batch_idx)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform a validation step.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>val_batch</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Tuple containing:
- dat (dict): Dictionary containing input matrices for each omics layer.
- y (torch.Tensor): Ground truth labels.</dd>
<dt><strong><code>batch_idx</code></strong> :&ensp;<code>int</code></dt>
<dd>Index of the current batch.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Loss value for the current validation step.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validation_step(self, val_batch, batch_idx):
    &#34;&#34;&#34;
    Perform a validation step.

    Args:
        val_batch (tuple): Tuple containing:
            - dat (dict): Dictionary containing input matrices for each omics layer.
            - y (torch.Tensor): Ground truth labels.
        batch_idx (int): Index of the current batch.

    Returns:
        torch.Tensor: Loss value for the current validation step.
    &#34;&#34;&#34;

    dat, y = val_batch
    layers = dat.keys()
    x_list = [dat[x] for x in layers]
    mean, log_var = self.multi_encoder(x_list)
    
    z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -&gt; var)
    x_hat_list = [self.decoders[i](z) for i in range(len(x_list))]
    y_pred = self.MLP(z)
    
    # compute mmd loss for each layer and take average
    mmd_loss_list = [self.MMD_loss(z.shape[1], z, x_hat_list[i], x_list[i]) for i in range(len(layers))]
    mmd_loss = torch.mean(torch.stack(mmd_loss_list))

    sp_loss = F.mse_loss(y_pred, y)
    loss = mmd_loss + sp_loss
    
    self.log(&#39;val_loss&#39;, loss)
    return loss</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="flexynesis" href="index.html">flexynesis</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="flexynesis.model_SVAE.supervised_vae" href="#flexynesis.model_SVAE.supervised_vae">supervised_vae</a></code></h4>
<ul class="">
<li><code><a title="flexynesis.model_SVAE.supervised_vae.MMD_loss" href="#flexynesis.model_SVAE.supervised_vae.MMD_loss">MMD_loss</a></code></li>
<li><code><a title="flexynesis.model_SVAE.supervised_vae.compute_kernel" href="#flexynesis.model_SVAE.supervised_vae.compute_kernel">compute_kernel</a></code></li>
<li><code><a title="flexynesis.model_SVAE.supervised_vae.compute_mmd" href="#flexynesis.model_SVAE.supervised_vae.compute_mmd">compute_mmd</a></code></li>
<li><code><a title="flexynesis.model_SVAE.supervised_vae.configure_optimizers" href="#flexynesis.model_SVAE.supervised_vae.configure_optimizers">configure_optimizers</a></code></li>
<li><code><a title="flexynesis.model_SVAE.supervised_vae.evaluate" href="#flexynesis.model_SVAE.supervised_vae.evaluate">evaluate</a></code></li>
<li><code><a title="flexynesis.model_SVAE.supervised_vae.forward" href="#flexynesis.model_SVAE.supervised_vae.forward">forward</a></code></li>
<li><code><a title="flexynesis.model_SVAE.supervised_vae.multi_encoder" href="#flexynesis.model_SVAE.supervised_vae.multi_encoder">multi_encoder</a></code></li>
<li><code><a title="flexynesis.model_SVAE.supervised_vae.reparameterization" href="#flexynesis.model_SVAE.supervised_vae.reparameterization">reparameterization</a></code></li>
<li><code><a title="flexynesis.model_SVAE.supervised_vae.training_step" href="#flexynesis.model_SVAE.supervised_vae.training_step">training_step</a></code></li>
<li><code><a title="flexynesis.model_SVAE.supervised_vae.transform" href="#flexynesis.model_SVAE.supervised_vae.transform">transform</a></code></li>
<li><code><a title="flexynesis.model_SVAE.supervised_vae.validation_step" href="#flexynesis.model_SVAE.supervised_vae.validation_step">validation_step</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>