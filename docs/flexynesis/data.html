<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>flexynesis.data API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>flexynesis.data</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from torch.utils.data import Dataset, DataLoader
from torch_geometric.data import Data
from torch_geometric.data import Dataset as PYGDataset

import numpy as np
import pandas as pd
from functools import reduce
import torch
import os

from tqdm import tqdm


from sklearn.preprocessing import OrdinalEncoder, StandardScaler, MinMaxScaler, PowerTransformer
from .feature_selection import filter_by_laplacian

from itertools import chain

# given a MultiOmicDataset object, convert to Triplets (anchor,positive,negative)
class TripletMultiOmicDataset(Dataset):
    &#34;&#34;&#34;
    For each sample (anchor) randomly chooses a positive and negative samples
    &#34;&#34;&#34;

    def __init__(self, mydataset, main_var):
        self.dataset = mydataset
        self.main_var = main_var
        self.labels_set, self.label_to_indices = self.get_label_indices(self.dataset.ann[self.main_var])
    def __getitem__(self, index):
        # get anchor sample and its label
        anchor, y_dict = self.dataset[index][0], self.dataset[index][1] 
        # choose another sample with same label
        label = y_dict[self.main_var].item()
        positive_index = index
        while positive_index == index:
            positive_index = np.random.choice(self.label_to_indices[label])
        # choose another sample with a different label 
        negative_label = np.random.choice(list(self.labels_set - set([label])))
        negative_index = np.random.choice(self.label_to_indices[negative_label])
        pos = self.dataset[positive_index][0] # positive example
        neg = self.dataset[negative_index][0] # negative example
        return anchor, pos, neg, y_dict

    def __len__(self):
        return len(self.dataset)
    
    def get_label_indices(self, labels):
        labels_set = set(labels.numpy())
        label_to_indices = {label: np.where(labels.numpy() == label)[0]
                             for label in labels_set}
        return labels_set, label_to_indices   

class MultiomicDataset(Dataset):
    &#34;&#34;&#34;A PyTorch dataset for multiomic data.

    Args:
        dat (dict): A dictionary with keys corresponding to different types of data and values corresponding to matrices of the same shape. All matrices must have the same number of samples (rows).
        ann (data.frame): Data frame with samples on the rows, sample annotations on the columns 
        features (list or np.array): A 1D array of feature names with length equal to the number of columns in each matrix.
        samples (list or np.array): A 1D array of sample names with length equal to the number of rows in each matrix.

    Returns:
        A PyTorch dataset that can be used for training or evaluation.
    &#34;&#34;&#34;

    def __init__(self, dat, ann, variable_types, features, samples, label_mappings, feature_ann=None):
        &#34;&#34;&#34;Initialize the dataset.&#34;&#34;&#34;
        self.dat = dat
        self.ann = ann
        self.variable_types = variable_types
        self.features = features
        self.samples = samples
        self.label_mappings = label_mappings
        self.feature_ann = feature_ann or {}

    def __getitem__(self, index):
        &#34;&#34;&#34;Get a single data sample from the dataset.

        Args:
            index (int): The index of the sample to retrieve.

        Returns:
            A tuple of two elements: 
                1. A dictionary with keys corresponding to the different types of data in the input dictionary `dat`, and values corresponding to the data for the given sample.
                2. The label for the given sample.
        &#34;&#34;&#34;
        subset_dat = {x: self.dat[x][index] for x in self.dat.keys()}
        subset_ann = {x: self.ann[x][index] for x in self.ann.keys()}
        return subset_dat, subset_ann
    
    def __len__ (self):
        &#34;&#34;&#34;Get the total number of samples in the dataset.

        Returns:
            An integer representing the number of samples in the dataset.
        &#34;&#34;&#34;
        return len(self.samples)
    
    def get_feature_subset(self, feature_df):
        &#34;&#34;&#34;Get a subset of data matrices corresponding to specified features and concatenate them into a pandas DataFrame.

        Args:
            feature_df (pandas.DataFrame): A DataFrame which contains at least two columns: &#39;layer&#39; and &#39;name&#39;. 

        Returns:
            A pandas DataFrame that concatenates the data matrices for the specified features from all layers. 
        &#34;&#34;&#34;
        # Convert the DataFrame to a dictionary
        feature_dict = feature_df.groupby(&#39;layer&#39;)[&#39;name&#39;].apply(list).to_dict()

        dfs = []
        for layer, features in feature_dict.items():
            if layer in self.dat:
                # Create a dictionary to look up indices by feature name for each layer
                feature_index_dict = {feature: i for i, feature in enumerate(self.features[layer])}
                # Get the indices for the requested features
                indices = [feature_index_dict[feature] for feature in features if feature in feature_index_dict]
                # Subset the data matrix for the current layer using the indices
                subset = self.dat[layer][:, indices]
                # Convert the subset to a pandas DataFrame, add the layer name as a prefix to each column name
                df = pd.DataFrame(subset, columns=[f&#39;{layer}_{feature}&#39; for feature in features if feature in feature_index_dict])
                dfs.append(df)
            else:
                print(f&#34;Layer {layer} not found in the dataset.&#34;)

        # Concatenate the dataframes along the columns axis
        result = pd.concat(dfs, axis=1)

        # Set the sample names as the row index
        result.index = self.samples

        return result
    
    def get_dataset_stats(self):
        stats = {&#39;: &#39;.join([&#39;feature_count in&#39;, x]): self.dat[x].shape[1] for x in self.dat.keys()}
        stats[&#39;sample_count&#39;] = len(self.samples)
        return(stats)


class MultiomicPYGDataset(MultiomicDataset, PYGDataset):
    &#34;&#34;&#34;Multiomic pyg dataset.
    &#34;&#34;&#34;
    def __init__(
        self,
        dat,
        ann,
        variable_types,
        features,
        samples,
        label_mappings,
        feature_ann=None,
        root=None,
        transform=None,
        pre_transform=None,
        pre_filter=None,
        log=True,
    ):
        &#34;&#34;&#34;Initialize dataset.
        &#34;&#34;&#34;
        super(MultiomicPYGDataset, self).__init__(dat, ann, variable_types, features, samples, label_mappings, feature_ann)
        super(MultiomicDataset, self).__init__(root, transform, pre_transform, pre_filter, log)
        self._transform = transform
        self.transform = None

    def __getitem__(self, idx):
        return super(MultiomicDataset, self).__getitem__(idx)

    def get(self, idx: int):
        subset_dat = {}
        for k, v in self.dat.items():
            x = v[idx]
            edge_index = self.feature_ann[k][&#34;edge_index&#34;]

            # If number of node features is 1, insert a new dim:
            if v[idx].ndim == 1:
                x = x.unsqueeze(1)

            data = Data(x=x, edge_index=edge_index)

            # Apply pyg transforms here:
            if self._transform is not None:
                data = self._transform(data)

            subset_dat[k] = data

        subset_ann = {k: v[idx] for k, v in self.ann.items()}
        return subset_dat, subset_ann

    def __len__ (self):
        return super(MultiomicPYGDataset, self).__len__()

    def len(self):
        return self.__len__()


def read_stringdb_links(fname):
    df = pd.read_csv(fname, header=0, sep=&#34; &#34;)
    df = df[df.combined_score &gt; 400]
    df = df[df.combined_score &gt; df.combined_score.quantile(0.9)]
    df[[&#34;protein1&#34;, &#34;protein2&#34;]] = df[[&#34;protein1&#34;, &#34;protein2&#34;]].applymap(lambda a: a.split(&#34;.&#34;)[-1])
    return df


def stringdb_links_to_list(df):
    lst = df[[&#34;protein1&#34;, &#34;protein2&#34;]].to_numpy().tolist()
    return lst


def read_stringdb_aliases(fname: str, node_name: str):
    protein_id_to_gene_id = {}
    with open(fname, &#34;r&#34;) as f:
        next(f)
        for line in f:
            data = line.split()
            if node_name == &#34;gene_id&#34;:
                if data[-1].endswith(&#34;Ensembl_HGNC_ensembl_gene_id&#34;):
                    protein_id_to_gene_id[data[0].split(&#34;.&#34;)[1]] = data[1]
                elif data[-1].endswith(&#34;Ensembl_gene&#34;):
                    # TODO: Check here if the values are the same
                    if protein_id_to_gene_id.get(data[0].split(&#34;.&#34;)[1], None) is None:
                        protein_id_to_gene_id[data[0].split(&#34;.&#34;)[1]] = data[1]
                    else:
                        continue
                else:
                    continue
            elif node_name == &#34;gene_name&#34;:
                if data[-1].endswith(&#34;Ensembl_EntrezGene&#34;):
                    protein_id_to_gene_id[data[0].split(&#34;.&#34;)[1]] = data[1]
                elif data[-1].endswith(&#34;Ensembl_HGNC_symbol&#34;):
                    # TODO: Check here if the values are the same
                    if protein_id_to_gene_id.get(data[0].split(&#34;.&#34;)[1], None) is None:
                        protein_id_to_gene_id[data[0].split(&#34;.&#34;)[1]] = data[1]
                    else:
                        continue    
                else:
                    continue
            else:
                raise NotImplementedError
    return protein_id_to_gene_id

    
# convert_to_labels: if true, given a numeric list, convert to binary labels by median value 
class DataImporter:

    protein_links = &#34;9606.protein.links.v12.0.txt&#34;
    protein_aliases = &#34;9606.protein.aliases.v12.0.txt&#34;

    def __init__(self, path, data_types, log_transform = False, concatenate = False, min_features=None, 
                 top_percentile=None, variance_threshold=1e-5, na_threshold=0.1, use_graph=False, node_name=&#34;gene_symbol&#34;, transform=None):
        self.path = path
        self.data_types = data_types
        self.concatenate = concatenate
        self.min_features = min_features
        self.top_percentile = top_percentile
        self.variance_threshold = variance_threshold
        self.na_threshold = na_threshold
        self.log_transform = log_transform
        # Initialize a dictionary to store the label encoders
        self.encoders = {} # used if labels are categorical 
        # initialize data scalers
        self.scalers = None
        # initialize data transformers
        self.transformers = None

        self.use_graph = use_graph
        self.node_name = node_name  # &#34;gene_symbol&#34; | &#34;gene_id&#34;
        self.transform = transform
        
    def read_data(self, folder_path):
        data = {}
        required_files = {&#39;clin.csv&#39;} | {f&#34;{dt}.csv&#34; for dt in self.data_types}
        print(&#34;\n[INFO] ----------------- Reading Data -----------------&#34;)
        for file in required_files:
            file_path = os.path.join(folder_path, file)
            file_name = os.path.splitext(file)[0]
            print(f&#34;[INFO] Importing {file_path}...&#34;)
            data[file_name] = pd.read_csv(file_path, index_col=0)
        return data

    def read_graph(self, fname=None):
        # NOTE: read stringdb for now
        # fname = fname or os.path.join(self.path, &#34;9606.protein.links.v12.0.txt&#34;)
        df = read_stringdb_links(fname)
        return df

    def cleanup_data(self, df_dict):
        print(&#34;\n[INFO] --------------- Cleaning Up Data ---------------&#34;)
        cleaned_dfs = {}
        sample_masks = []

        # First pass: remove near-zero-variation features and create masks for informative samples
        for key, df in df_dict.items():
            original_features_count = df.shape[0]

            # Step 1: Remove near-zero-variation features
            # Compute variances of features (along rows)
            feature_variances = df.var(axis=1)
            # Keep only features with variance above the threshold
            df = df.loc[feature_variances &gt; self.variance_threshold, :]
            
            # Step 2: Remove features with too many NA values
            # Compute percentage of NA values for each feature
            na_percentages = df.isna().mean(axis=1)
            # Keep only features with percentage of NA values below the threshold
            df = df.loc[na_percentages &lt; self.na_threshold, :]
            
            # Step 3: Fill NA values with the median of the feature
            # Check if there are any NA values in the DataFrame
            
            if np.sum(df.isna().sum()) &gt; 0:
                # Identify rows that contain missing values
                missing_rows = df.isna().any(axis=1)
                print(&#34;Imputing NA values to median of features, affected # of features &#34;, np.sum(df.isna().sum()), &#34; # of rows:&#34;,sum(missing_rows))

                # Only calculate the median for rows with missing values
                medians = df[missing_rows].median(axis=1)

                # Iterate over the index using tqdm to display a progress bar
                for i in tqdm(medians.index):
                    # Replace missing values in the row with the corresponding median
                    df.loc[i] = df.loc[i].fillna(medians[i])
                    
            print(&#34;Number of NA values: &#34;,np.sum(df.isna().sum()))
                                   
            removed_features_count = original_features_count - df.shape[0]
            print(f&#34;[INFO] DataFrame {key} - Removed {removed_features_count} features.&#34;)
        
            # Step 2: Create masks for informative samples
            # Compute standard deviation of samples (along columns)
            sample_stdevs = df.std(axis=0)
            # Create mask for samples that do not have std dev of 0 or NaN
            mask = np.logical_and(sample_stdevs != 0, np.logical_not(np.isnan(sample_stdevs)))
            sample_masks.append(mask)

            cleaned_dfs[key] = df

        # Find samples that are informative in all dataframes
        common_mask = pd.DataFrame(sample_masks).all()

        # Second pass: apply common mask to all dataframes
        for key in cleaned_dfs.keys():
            original_samples_count = cleaned_dfs[key].shape[1]
            cleaned_dfs[key] = cleaned_dfs[key].loc[:, common_mask]
            removed_samples_count = original_samples_count - cleaned_dfs[key].shape[1]
            print(f&#34;DataFrame {key} - Removed {removed_samples_count} samples ({removed_samples_count / original_samples_count * 100:.2f}%).&#34;)

        return cleaned_dfs

    def import_data(self):
        print(&#34;\n[INFO] ================= Importing Data =================&#34;)
        training_path = os.path.join(self.path, &#39;train&#39;)
        testing_path = os.path.join(self.path, &#39;test&#39;)
        # NOTE: stringdb file hardcoded for now
        edges_data_path = os.path.join(self.path, self.protein_links)
        node_data_path = os.path.join(self.path, self.protein_aliases)

        self.validate_data_folders(training_path, testing_path)
        
        training_data = self.read_data(training_path)
        testing_data = self.read_data(testing_path)

        # Import graph here:
        if self.use_graph:
            # Read graph from the file
            graph_df = self.read_graph(edges_data_path)
            # Convert graph nodes names accordingly
            if self.node_name == &#34;gene_name&#34;:
                node_name_mapping = read_stringdb_aliases(node_data_path, self.node_name)
            elif self.node_name == &#34;gene_id&#34;:
                node_name_mapping = read_stringdb_aliases(node_data_path, self.node_name)
            else:
                raise NotImplementedError
            
            def fn(a):
                try:
                    # lambda a: node_name_mapping[a]
                    out = node_name_mapping[a]
                except KeyError:
                    # print(f&#34;MISSING: [{a}]&#34;)
                    out = &#34;&#34;
                return out

            graph_df[[&#34;protein1&#34;, &#34;protein2&#34;]] = graph_df[[&#34;protein1&#34;, &#34;protein2&#34;]].applymap(fn)

            available_genes: list[str] = np.unique(graph_df[[&#34;protein1&#34;, &#34;protein2&#34;]].to_numpy()).tolist()

            # If use graph, filter genes that are not in provided data
            provided_genes = []
            # Iterate over data modalities to collect all available genes
            for _df in training_data.values():
                for g in _df.index:
                    if g in available_genes:
                        if g not in provided_genes:
                            provided_genes.append(g)
            # Same for testing data
            for _df in testing_data.values():
                for g in _df.index:
                    if g in available_genes:
                        if g not in provided_genes:
                            provided_genes.append(g)

            initial_edge_list = stringdb_links_to_list(graph_df)
            # Now filter the graph edges based on provided_genes
            edge_list = []
            for edge in initial_edge_list:
                src, dst = edge
                if (src in provided_genes) and (dst in provided_genes):
                    edge_list.append(edge)

        # cleanup uninformative features/samples, subset annotation data, do feature selection on training data
        train_dat, train_ann, train_samples, train_features = self.process_data(training_data, split = &#39;train&#39;)
        test_dat, test_ann, test_samples, test_features = self.process_data(testing_data, split = &#39;test&#39;)
        
        # harmonize feature sets in train/test
        train_dat, test_dat = self.harmonize(train_dat, test_dat)

        train_feature_ann = {}
        test_feature_ann = {}
        if self.use_graph:
            # Now filter the graph edges based on provided_genes
            # But this time separately for each modality
            for k, v in train_dat.items():
                mod_gene_list = v.index.to_list()
                node_to_idx = {node: i for i, node in enumerate(mod_gene_list)}
                mod_edge_list = []
                for edge in edge_list:
                    src, dst = edge
                    if (src in mod_gene_list) and (dst in mod_gene_list):
                        mod_edge_list.append([node_to_idx[src], node_to_idx[dst]])
                train_feature_ann[k] = {&#34;edge_index&#34;: torch.tensor(mod_edge_list).T}
            # Repeat the same for test data
            for k, v in train_dat.items():
                mod_gene_list = v.index.to_list()
                node_to_idx = {node: i for i, node in enumerate(mod_gene_list)}
                mod_edge_list = []
                for edge in edge_list:
                    src, dst = edge
                    if (src in mod_gene_list) and (dst in mod_gene_list):
                        mod_edge_list.append([node_to_idx[src], node_to_idx[dst]])
                test_feature_ann[k] = {&#34;edge_index&#34;: torch.tensor(mod_edge_list).T}

        # log_transform 
        if self.log_transform:
            print(&#34;transforming data to log scale&#34;)
            train_dat = self.transform_data(train_dat)
            test_dat = self.transform_data(test_dat)
        
        # Normalize the training data (for testing data, use normalisation factors
        # learned from training data to apply on test data (see fit = False)
        train_dat = self.normalize_data(train_dat, scaler_type=&#34;standard&#34;, fit=True)
        test_dat = self.normalize_data(test_dat, scaler_type=&#34;standard&#34;, fit=False)
        
        # encode the variable annotations, convert data matrices and annotations pytorch datasets 
        training_dataset = self.get_torch_dataset(train_dat, train_ann, train_samples, train_feature_ann)
        testing_dataset = self.get_torch_dataset(test_dat, test_ann, test_samples, test_feature_ann)
       
        # for early fusion, concatenate all data matrices and feature lists 
        if self.concatenate:
            training_dataset.dat = {&#39;all&#39;: torch.cat([training_dataset.dat[x] for x in training_dataset.dat.keys()], dim = 1)}
            training_dataset.features = {&#39;all&#39;: list(chain(*training_dataset.features.values()))}
            
            testing_dataset.dat = {&#39;all&#39;: torch.cat([testing_dataset.dat[x] for x in testing_dataset.dat.keys()], dim = 1)}
            testing_dataset.features = {&#39;all&#39;: list(chain(*testing_dataset.features.values()))}
        
        print(&#34;[INFO] Training Data Stats:\n&#34;, training_dataset.get_dataset_stats())
        print(&#34;[INFO] Test Data Stats:\n&#34;, testing_dataset.get_dataset_stats())
        
        print(&#34;[INFO] Data import successful.&#34;)
        
        
        
        return training_dataset, testing_dataset
    
    def validate_data_folders(self, training_path, testing_path):
        print(&#34;[INFO] Validating data folders...&#34;)
        training_files = set(os.listdir(training_path))
        testing_files = set(os.listdir(testing_path))

        required_files = {&#39;clin.csv&#39;} | {f&#34;{dt}.csv&#34; for dt in self.data_types}

        if not required_files.issubset(training_files):
            missing_files = required_files - training_files
            raise ValueError(f&#34;Missing files in training folder: {&#39;, &#39;.join(missing_files)}&#34;)

        if not required_files.issubset(testing_files):
            missing_files = required_files - testing_files
            raise ValueError(f&#34;Missing files in testing folder: {&#39;, &#39;.join(missing_files)}&#34;)


    def process_data(self, data, split = &#39;train&#39;):
        print(f&#34;\n[INFO] ---------- Processing Data ({split}) ----------&#34;)
        # remove uninformative features and samples with no information (from data matrices)
        dat = self.cleanup_data({x: data[x] for x in self.data_types})
        ann = data[&#39;clin&#39;]
        dat, ann, samples = self.get_labels(dat, ann)
        # do feature selection: only applied to training data
        if split == &#39;train&#39;: 
            if self.min_features or self.top_percentile:
                dat = self.filter(dat, self.min_features, self.top_percentile)
        features = {x: dat[x].index for x in dat.keys()}
        return dat, ann, samples, features

    def get_labels(self, dat, ann):
        # subset samples and reorder annotations for the samples 
        samples = list(reduce(set.intersection, [set(item) for item in [dat[x].columns for x in dat.keys()]]))
        samples = list(set(ann.index).intersection(samples))
        dat = {x: dat[x][samples] for x in dat.keys()}
        ann = ann.loc[samples]
        return dat, ann, samples

    def encode_labels(self, df):
        label_mappings = {}
        def encode_column(series):
            nonlocal label_mappings  # Declare as nonlocal so that we can modify it
            # Fill NA values with &#39;missing&#39; 
            # series = series.fillna(&#39;missing&#39;)
            if series.name not in self.encoders:
                self.encoders[series.name] = OrdinalEncoder(handle_unknown=&#39;use_encoded_value&#39;, unknown_value=-1)
                encoded_series = self.encoders[series.name].fit_transform(series.to_frame())
            else:
                encoded_series = self.encoders[series.name].transform(series.to_frame())
            
            # also save label mappings 
            label_mappings[series.name] = {
                    int(code): label for code, label in enumerate(self.encoders[series.name].categories_[0])
                }
            return encoded_series.ravel()

        # Select only the categorical columns
        df_categorical = df.select_dtypes(include=[&#39;object&#39;, &#39;category&#39;]).apply(encode_column)

        # Combine the encoded categorical data with the numerical data
        df_encoded = pd.concat([df.select_dtypes(exclude=[&#39;object&#39;, &#39;category&#39;]), df_categorical], axis=1)

        # Store the variable types
        variable_types = {col: &#39;categorical&#39; for col in df_categorical.columns}
        variable_types.update({col: &#39;numerical&#39; for col in df.select_dtypes(exclude=[&#39;object&#39;, &#39;category&#39;]).columns})

        return df_encoded, variable_types, label_mappings

    def get_torch_dataset(self, dat, ann, samples, feature_ann):
        features = {x: dat[x].index for x in dat.keys()}
        dat = {x: torch.from_numpy(np.array(dat[x].T)).float() for x in dat.keys()}

        ann, variable_types, label_mappings = self.encode_labels(ann)

        # Convert DataFrame to tensor
        ann = {col: torch.from_numpy(ann[col].values) for col in ann.columns}
        if not self.use_graph:
            return MultiomicDataset(dat, ann, variable_types, features, samples, label_mappings)
        else:
            return MultiomicPYGDataset(dat, ann, variable_types, features, samples, label_mappings, feature_ann, transform=self.transform)
    
    def normalize_data(self, data, scaler_type=&#34;standard&#34;, fit=True):
        print(&#34;\n[INFO] --------------- Normalizing Data ---------------&#34;)
        # notice matrix transpositions during fit and finally after transformation
        # because data matrices have features on rows, 
        # while scaling methods assume features to be on the columns. 
        if fit:
            if scaler_type == &#34;standard&#34;:
                self.scalers = {x: StandardScaler().fit(data[x].T) for x in data.keys()}
            elif scaler_type == &#34;min_max&#34;:
                self.scalers = {x: MinMaxScaler().fit(data[x].T) for x in data.keys()}
            else:
                raise ValueError(&#34;Invalid scaler_type. Choose &#39;standard&#39; or &#39;min_max&#39;.&#34;)
        
        normalized_data = {x: pd.DataFrame(self.scalers[x].transform(data[x].T), 
                                           index=data[x].columns, 
                                           columns=data[x].index).T 
                           for x in data.keys()}
        return normalized_data
    
    def transform_data(self, data):
        transformed_data = {x: np.log1p(data[x].T).T for x in data.keys()}
        return transformed_data    

    def filter(self, dat, min_features, top_percentile):
        counts = {x: max(int(dat[x].shape[0] * top_percentile / 100), min_features) for x in dat.keys()}
        dat = {x: filter_by_laplacian(dat[x].T, x, topN=counts[x]).T for x in dat.keys()}
        return dat

    def harmonize(self, dat1, dat2):
        print(&#34;\n[INFO] ------------ Harmonizing Data Sets ------------&#34;)
        # Get common features
        common_features = {x: dat1[x].index.intersection(dat2[x].index) for x in self.data_types}
        # Subset both datasets to only include common features
        dat1 = {x: dat1[x].loc[common_features[x]] for x in dat1.keys()}
        dat2 = {x: dat2[x].loc[common_features[x]] for x in dat2.keys()}
        return dat1, dat2
    

def split_by_median(tensor_dict):
    new_dict = {}
    for key, tensor in tensor_dict.items():
        # Check if the tensor is of a floating point type (i.e., it&#39;s numerical)
        if tensor.dtype in {torch.float16, torch.float32, torch.float64}:
            # Remove NaNs and compute median
            tensor_no_nan = tensor[torch.isfinite(tensor)]
            median_val = tensor_no_nan.sort().values[tensor_no_nan.numel() // 2]
            
            # Convert to categorical, but preserve NaNs
            tensor_cat = (tensor &gt; median_val).float()
            tensor_cat[torch.isnan(tensor)] = float(&#39;nan&#39;)
            new_dict[key] = tensor_cat
        else:
            # If tensor is not numerical, leave it as it is
            new_dict[key] = tensor
    return new_dict</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="flexynesis.data.read_stringdb_aliases"><code class="name flex">
<span>def <span class="ident">read_stringdb_aliases</span></span>(<span>fname: str, node_name: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_stringdb_aliases(fname: str, node_name: str):
    protein_id_to_gene_id = {}
    with open(fname, &#34;r&#34;) as f:
        next(f)
        for line in f:
            data = line.split()
            if node_name == &#34;gene_id&#34;:
                if data[-1].endswith(&#34;Ensembl_HGNC_ensembl_gene_id&#34;):
                    protein_id_to_gene_id[data[0].split(&#34;.&#34;)[1]] = data[1]
                elif data[-1].endswith(&#34;Ensembl_gene&#34;):
                    # TODO: Check here if the values are the same
                    if protein_id_to_gene_id.get(data[0].split(&#34;.&#34;)[1], None) is None:
                        protein_id_to_gene_id[data[0].split(&#34;.&#34;)[1]] = data[1]
                    else:
                        continue
                else:
                    continue
            elif node_name == &#34;gene_name&#34;:
                if data[-1].endswith(&#34;Ensembl_EntrezGene&#34;):
                    protein_id_to_gene_id[data[0].split(&#34;.&#34;)[1]] = data[1]
                elif data[-1].endswith(&#34;Ensembl_HGNC_symbol&#34;):
                    # TODO: Check here if the values are the same
                    if protein_id_to_gene_id.get(data[0].split(&#34;.&#34;)[1], None) is None:
                        protein_id_to_gene_id[data[0].split(&#34;.&#34;)[1]] = data[1]
                    else:
                        continue    
                else:
                    continue
            else:
                raise NotImplementedError
    return protein_id_to_gene_id</code></pre>
</details>
</dd>
<dt id="flexynesis.data.read_stringdb_links"><code class="name flex">
<span>def <span class="ident">read_stringdb_links</span></span>(<span>fname)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_stringdb_links(fname):
    df = pd.read_csv(fname, header=0, sep=&#34; &#34;)
    df = df[df.combined_score &gt; 400]
    df = df[df.combined_score &gt; df.combined_score.quantile(0.9)]
    df[[&#34;protein1&#34;, &#34;protein2&#34;]] = df[[&#34;protein1&#34;, &#34;protein2&#34;]].applymap(lambda a: a.split(&#34;.&#34;)[-1])
    return df</code></pre>
</details>
</dd>
<dt id="flexynesis.data.split_by_median"><code class="name flex">
<span>def <span class="ident">split_by_median</span></span>(<span>tensor_dict)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def split_by_median(tensor_dict):
    new_dict = {}
    for key, tensor in tensor_dict.items():
        # Check if the tensor is of a floating point type (i.e., it&#39;s numerical)
        if tensor.dtype in {torch.float16, torch.float32, torch.float64}:
            # Remove NaNs and compute median
            tensor_no_nan = tensor[torch.isfinite(tensor)]
            median_val = tensor_no_nan.sort().values[tensor_no_nan.numel() // 2]
            
            # Convert to categorical, but preserve NaNs
            tensor_cat = (tensor &gt; median_val).float()
            tensor_cat[torch.isnan(tensor)] = float(&#39;nan&#39;)
            new_dict[key] = tensor_cat
        else:
            # If tensor is not numerical, leave it as it is
            new_dict[key] = tensor
    return new_dict</code></pre>
</details>
</dd>
<dt id="flexynesis.data.stringdb_links_to_list"><code class="name flex">
<span>def <span class="ident">stringdb_links_to_list</span></span>(<span>df)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stringdb_links_to_list(df):
    lst = df[[&#34;protein1&#34;, &#34;protein2&#34;]].to_numpy().tolist()
    return lst</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="flexynesis.data.DataImporter"><code class="flex name class">
<span>class <span class="ident">DataImporter</span></span>
<span>(</span><span>path, data_types, log_transform=False, concatenate=False, min_features=None, top_percentile=None, variance_threshold=1e-05, na_threshold=0.1, use_graph=False, node_name='gene_symbol', transform=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataImporter:

    protein_links = &#34;9606.protein.links.v12.0.txt&#34;
    protein_aliases = &#34;9606.protein.aliases.v12.0.txt&#34;

    def __init__(self, path, data_types, log_transform = False, concatenate = False, min_features=None, 
                 top_percentile=None, variance_threshold=1e-5, na_threshold=0.1, use_graph=False, node_name=&#34;gene_symbol&#34;, transform=None):
        self.path = path
        self.data_types = data_types
        self.concatenate = concatenate
        self.min_features = min_features
        self.top_percentile = top_percentile
        self.variance_threshold = variance_threshold
        self.na_threshold = na_threshold
        self.log_transform = log_transform
        # Initialize a dictionary to store the label encoders
        self.encoders = {} # used if labels are categorical 
        # initialize data scalers
        self.scalers = None
        # initialize data transformers
        self.transformers = None

        self.use_graph = use_graph
        self.node_name = node_name  # &#34;gene_symbol&#34; | &#34;gene_id&#34;
        self.transform = transform
        
    def read_data(self, folder_path):
        data = {}
        required_files = {&#39;clin.csv&#39;} | {f&#34;{dt}.csv&#34; for dt in self.data_types}
        print(&#34;\n[INFO] ----------------- Reading Data -----------------&#34;)
        for file in required_files:
            file_path = os.path.join(folder_path, file)
            file_name = os.path.splitext(file)[0]
            print(f&#34;[INFO] Importing {file_path}...&#34;)
            data[file_name] = pd.read_csv(file_path, index_col=0)
        return data

    def read_graph(self, fname=None):
        # NOTE: read stringdb for now
        # fname = fname or os.path.join(self.path, &#34;9606.protein.links.v12.0.txt&#34;)
        df = read_stringdb_links(fname)
        return df

    def cleanup_data(self, df_dict):
        print(&#34;\n[INFO] --------------- Cleaning Up Data ---------------&#34;)
        cleaned_dfs = {}
        sample_masks = []

        # First pass: remove near-zero-variation features and create masks for informative samples
        for key, df in df_dict.items():
            original_features_count = df.shape[0]

            # Step 1: Remove near-zero-variation features
            # Compute variances of features (along rows)
            feature_variances = df.var(axis=1)
            # Keep only features with variance above the threshold
            df = df.loc[feature_variances &gt; self.variance_threshold, :]
            
            # Step 2: Remove features with too many NA values
            # Compute percentage of NA values for each feature
            na_percentages = df.isna().mean(axis=1)
            # Keep only features with percentage of NA values below the threshold
            df = df.loc[na_percentages &lt; self.na_threshold, :]
            
            # Step 3: Fill NA values with the median of the feature
            # Check if there are any NA values in the DataFrame
            
            if np.sum(df.isna().sum()) &gt; 0:
                # Identify rows that contain missing values
                missing_rows = df.isna().any(axis=1)
                print(&#34;Imputing NA values to median of features, affected # of features &#34;, np.sum(df.isna().sum()), &#34; # of rows:&#34;,sum(missing_rows))

                # Only calculate the median for rows with missing values
                medians = df[missing_rows].median(axis=1)

                # Iterate over the index using tqdm to display a progress bar
                for i in tqdm(medians.index):
                    # Replace missing values in the row with the corresponding median
                    df.loc[i] = df.loc[i].fillna(medians[i])
                    
            print(&#34;Number of NA values: &#34;,np.sum(df.isna().sum()))
                                   
            removed_features_count = original_features_count - df.shape[0]
            print(f&#34;[INFO] DataFrame {key} - Removed {removed_features_count} features.&#34;)
        
            # Step 2: Create masks for informative samples
            # Compute standard deviation of samples (along columns)
            sample_stdevs = df.std(axis=0)
            # Create mask for samples that do not have std dev of 0 or NaN
            mask = np.logical_and(sample_stdevs != 0, np.logical_not(np.isnan(sample_stdevs)))
            sample_masks.append(mask)

            cleaned_dfs[key] = df

        # Find samples that are informative in all dataframes
        common_mask = pd.DataFrame(sample_masks).all()

        # Second pass: apply common mask to all dataframes
        for key in cleaned_dfs.keys():
            original_samples_count = cleaned_dfs[key].shape[1]
            cleaned_dfs[key] = cleaned_dfs[key].loc[:, common_mask]
            removed_samples_count = original_samples_count - cleaned_dfs[key].shape[1]
            print(f&#34;DataFrame {key} - Removed {removed_samples_count} samples ({removed_samples_count / original_samples_count * 100:.2f}%).&#34;)

        return cleaned_dfs

    def import_data(self):
        print(&#34;\n[INFO] ================= Importing Data =================&#34;)
        training_path = os.path.join(self.path, &#39;train&#39;)
        testing_path = os.path.join(self.path, &#39;test&#39;)
        # NOTE: stringdb file hardcoded for now
        edges_data_path = os.path.join(self.path, self.protein_links)
        node_data_path = os.path.join(self.path, self.protein_aliases)

        self.validate_data_folders(training_path, testing_path)
        
        training_data = self.read_data(training_path)
        testing_data = self.read_data(testing_path)

        # Import graph here:
        if self.use_graph:
            # Read graph from the file
            graph_df = self.read_graph(edges_data_path)
            # Convert graph nodes names accordingly
            if self.node_name == &#34;gene_name&#34;:
                node_name_mapping = read_stringdb_aliases(node_data_path, self.node_name)
            elif self.node_name == &#34;gene_id&#34;:
                node_name_mapping = read_stringdb_aliases(node_data_path, self.node_name)
            else:
                raise NotImplementedError
            
            def fn(a):
                try:
                    # lambda a: node_name_mapping[a]
                    out = node_name_mapping[a]
                except KeyError:
                    # print(f&#34;MISSING: [{a}]&#34;)
                    out = &#34;&#34;
                return out

            graph_df[[&#34;protein1&#34;, &#34;protein2&#34;]] = graph_df[[&#34;protein1&#34;, &#34;protein2&#34;]].applymap(fn)

            available_genes: list[str] = np.unique(graph_df[[&#34;protein1&#34;, &#34;protein2&#34;]].to_numpy()).tolist()

            # If use graph, filter genes that are not in provided data
            provided_genes = []
            # Iterate over data modalities to collect all available genes
            for _df in training_data.values():
                for g in _df.index:
                    if g in available_genes:
                        if g not in provided_genes:
                            provided_genes.append(g)
            # Same for testing data
            for _df in testing_data.values():
                for g in _df.index:
                    if g in available_genes:
                        if g not in provided_genes:
                            provided_genes.append(g)

            initial_edge_list = stringdb_links_to_list(graph_df)
            # Now filter the graph edges based on provided_genes
            edge_list = []
            for edge in initial_edge_list:
                src, dst = edge
                if (src in provided_genes) and (dst in provided_genes):
                    edge_list.append(edge)

        # cleanup uninformative features/samples, subset annotation data, do feature selection on training data
        train_dat, train_ann, train_samples, train_features = self.process_data(training_data, split = &#39;train&#39;)
        test_dat, test_ann, test_samples, test_features = self.process_data(testing_data, split = &#39;test&#39;)
        
        # harmonize feature sets in train/test
        train_dat, test_dat = self.harmonize(train_dat, test_dat)

        train_feature_ann = {}
        test_feature_ann = {}
        if self.use_graph:
            # Now filter the graph edges based on provided_genes
            # But this time separately for each modality
            for k, v in train_dat.items():
                mod_gene_list = v.index.to_list()
                node_to_idx = {node: i for i, node in enumerate(mod_gene_list)}
                mod_edge_list = []
                for edge in edge_list:
                    src, dst = edge
                    if (src in mod_gene_list) and (dst in mod_gene_list):
                        mod_edge_list.append([node_to_idx[src], node_to_idx[dst]])
                train_feature_ann[k] = {&#34;edge_index&#34;: torch.tensor(mod_edge_list).T}
            # Repeat the same for test data
            for k, v in train_dat.items():
                mod_gene_list = v.index.to_list()
                node_to_idx = {node: i for i, node in enumerate(mod_gene_list)}
                mod_edge_list = []
                for edge in edge_list:
                    src, dst = edge
                    if (src in mod_gene_list) and (dst in mod_gene_list):
                        mod_edge_list.append([node_to_idx[src], node_to_idx[dst]])
                test_feature_ann[k] = {&#34;edge_index&#34;: torch.tensor(mod_edge_list).T}

        # log_transform 
        if self.log_transform:
            print(&#34;transforming data to log scale&#34;)
            train_dat = self.transform_data(train_dat)
            test_dat = self.transform_data(test_dat)
        
        # Normalize the training data (for testing data, use normalisation factors
        # learned from training data to apply on test data (see fit = False)
        train_dat = self.normalize_data(train_dat, scaler_type=&#34;standard&#34;, fit=True)
        test_dat = self.normalize_data(test_dat, scaler_type=&#34;standard&#34;, fit=False)
        
        # encode the variable annotations, convert data matrices and annotations pytorch datasets 
        training_dataset = self.get_torch_dataset(train_dat, train_ann, train_samples, train_feature_ann)
        testing_dataset = self.get_torch_dataset(test_dat, test_ann, test_samples, test_feature_ann)
       
        # for early fusion, concatenate all data matrices and feature lists 
        if self.concatenate:
            training_dataset.dat = {&#39;all&#39;: torch.cat([training_dataset.dat[x] for x in training_dataset.dat.keys()], dim = 1)}
            training_dataset.features = {&#39;all&#39;: list(chain(*training_dataset.features.values()))}
            
            testing_dataset.dat = {&#39;all&#39;: torch.cat([testing_dataset.dat[x] for x in testing_dataset.dat.keys()], dim = 1)}
            testing_dataset.features = {&#39;all&#39;: list(chain(*testing_dataset.features.values()))}
        
        print(&#34;[INFO] Training Data Stats:\n&#34;, training_dataset.get_dataset_stats())
        print(&#34;[INFO] Test Data Stats:\n&#34;, testing_dataset.get_dataset_stats())
        
        print(&#34;[INFO] Data import successful.&#34;)
        
        
        
        return training_dataset, testing_dataset
    
    def validate_data_folders(self, training_path, testing_path):
        print(&#34;[INFO] Validating data folders...&#34;)
        training_files = set(os.listdir(training_path))
        testing_files = set(os.listdir(testing_path))

        required_files = {&#39;clin.csv&#39;} | {f&#34;{dt}.csv&#34; for dt in self.data_types}

        if not required_files.issubset(training_files):
            missing_files = required_files - training_files
            raise ValueError(f&#34;Missing files in training folder: {&#39;, &#39;.join(missing_files)}&#34;)

        if not required_files.issubset(testing_files):
            missing_files = required_files - testing_files
            raise ValueError(f&#34;Missing files in testing folder: {&#39;, &#39;.join(missing_files)}&#34;)


    def process_data(self, data, split = &#39;train&#39;):
        print(f&#34;\n[INFO] ---------- Processing Data ({split}) ----------&#34;)
        # remove uninformative features and samples with no information (from data matrices)
        dat = self.cleanup_data({x: data[x] for x in self.data_types})
        ann = data[&#39;clin&#39;]
        dat, ann, samples = self.get_labels(dat, ann)
        # do feature selection: only applied to training data
        if split == &#39;train&#39;: 
            if self.min_features or self.top_percentile:
                dat = self.filter(dat, self.min_features, self.top_percentile)
        features = {x: dat[x].index for x in dat.keys()}
        return dat, ann, samples, features

    def get_labels(self, dat, ann):
        # subset samples and reorder annotations for the samples 
        samples = list(reduce(set.intersection, [set(item) for item in [dat[x].columns for x in dat.keys()]]))
        samples = list(set(ann.index).intersection(samples))
        dat = {x: dat[x][samples] for x in dat.keys()}
        ann = ann.loc[samples]
        return dat, ann, samples

    def encode_labels(self, df):
        label_mappings = {}
        def encode_column(series):
            nonlocal label_mappings  # Declare as nonlocal so that we can modify it
            # Fill NA values with &#39;missing&#39; 
            # series = series.fillna(&#39;missing&#39;)
            if series.name not in self.encoders:
                self.encoders[series.name] = OrdinalEncoder(handle_unknown=&#39;use_encoded_value&#39;, unknown_value=-1)
                encoded_series = self.encoders[series.name].fit_transform(series.to_frame())
            else:
                encoded_series = self.encoders[series.name].transform(series.to_frame())
            
            # also save label mappings 
            label_mappings[series.name] = {
                    int(code): label for code, label in enumerate(self.encoders[series.name].categories_[0])
                }
            return encoded_series.ravel()

        # Select only the categorical columns
        df_categorical = df.select_dtypes(include=[&#39;object&#39;, &#39;category&#39;]).apply(encode_column)

        # Combine the encoded categorical data with the numerical data
        df_encoded = pd.concat([df.select_dtypes(exclude=[&#39;object&#39;, &#39;category&#39;]), df_categorical], axis=1)

        # Store the variable types
        variable_types = {col: &#39;categorical&#39; for col in df_categorical.columns}
        variable_types.update({col: &#39;numerical&#39; for col in df.select_dtypes(exclude=[&#39;object&#39;, &#39;category&#39;]).columns})

        return df_encoded, variable_types, label_mappings

    def get_torch_dataset(self, dat, ann, samples, feature_ann):
        features = {x: dat[x].index for x in dat.keys()}
        dat = {x: torch.from_numpy(np.array(dat[x].T)).float() for x in dat.keys()}

        ann, variable_types, label_mappings = self.encode_labels(ann)

        # Convert DataFrame to tensor
        ann = {col: torch.from_numpy(ann[col].values) for col in ann.columns}
        if not self.use_graph:
            return MultiomicDataset(dat, ann, variable_types, features, samples, label_mappings)
        else:
            return MultiomicPYGDataset(dat, ann, variable_types, features, samples, label_mappings, feature_ann, transform=self.transform)
    
    def normalize_data(self, data, scaler_type=&#34;standard&#34;, fit=True):
        print(&#34;\n[INFO] --------------- Normalizing Data ---------------&#34;)
        # notice matrix transpositions during fit and finally after transformation
        # because data matrices have features on rows, 
        # while scaling methods assume features to be on the columns. 
        if fit:
            if scaler_type == &#34;standard&#34;:
                self.scalers = {x: StandardScaler().fit(data[x].T) for x in data.keys()}
            elif scaler_type == &#34;min_max&#34;:
                self.scalers = {x: MinMaxScaler().fit(data[x].T) for x in data.keys()}
            else:
                raise ValueError(&#34;Invalid scaler_type. Choose &#39;standard&#39; or &#39;min_max&#39;.&#34;)
        
        normalized_data = {x: pd.DataFrame(self.scalers[x].transform(data[x].T), 
                                           index=data[x].columns, 
                                           columns=data[x].index).T 
                           for x in data.keys()}
        return normalized_data
    
    def transform_data(self, data):
        transformed_data = {x: np.log1p(data[x].T).T for x in data.keys()}
        return transformed_data    

    def filter(self, dat, min_features, top_percentile):
        counts = {x: max(int(dat[x].shape[0] * top_percentile / 100), min_features) for x in dat.keys()}
        dat = {x: filter_by_laplacian(dat[x].T, x, topN=counts[x]).T for x in dat.keys()}
        return dat

    def harmonize(self, dat1, dat2):
        print(&#34;\n[INFO] ------------ Harmonizing Data Sets ------------&#34;)
        # Get common features
        common_features = {x: dat1[x].index.intersection(dat2[x].index) for x in self.data_types}
        # Subset both datasets to only include common features
        dat1 = {x: dat1[x].loc[common_features[x]] for x in dat1.keys()}
        dat2 = {x: dat2[x].loc[common_features[x]] for x in dat2.keys()}
        return dat1, dat2</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="flexynesis.data.DataImporter.protein_aliases"><code class="name">var <span class="ident">protein_aliases</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="flexynesis.data.DataImporter.protein_links"><code class="name">var <span class="ident">protein_links</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="flexynesis.data.DataImporter.cleanup_data"><code class="name flex">
<span>def <span class="ident">cleanup_data</span></span>(<span>self, df_dict)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cleanup_data(self, df_dict):
    print(&#34;\n[INFO] --------------- Cleaning Up Data ---------------&#34;)
    cleaned_dfs = {}
    sample_masks = []

    # First pass: remove near-zero-variation features and create masks for informative samples
    for key, df in df_dict.items():
        original_features_count = df.shape[0]

        # Step 1: Remove near-zero-variation features
        # Compute variances of features (along rows)
        feature_variances = df.var(axis=1)
        # Keep only features with variance above the threshold
        df = df.loc[feature_variances &gt; self.variance_threshold, :]
        
        # Step 2: Remove features with too many NA values
        # Compute percentage of NA values for each feature
        na_percentages = df.isna().mean(axis=1)
        # Keep only features with percentage of NA values below the threshold
        df = df.loc[na_percentages &lt; self.na_threshold, :]
        
        # Step 3: Fill NA values with the median of the feature
        # Check if there are any NA values in the DataFrame
        
        if np.sum(df.isna().sum()) &gt; 0:
            # Identify rows that contain missing values
            missing_rows = df.isna().any(axis=1)
            print(&#34;Imputing NA values to median of features, affected # of features &#34;, np.sum(df.isna().sum()), &#34; # of rows:&#34;,sum(missing_rows))

            # Only calculate the median for rows with missing values
            medians = df[missing_rows].median(axis=1)

            # Iterate over the index using tqdm to display a progress bar
            for i in tqdm(medians.index):
                # Replace missing values in the row with the corresponding median
                df.loc[i] = df.loc[i].fillna(medians[i])
                
        print(&#34;Number of NA values: &#34;,np.sum(df.isna().sum()))
                               
        removed_features_count = original_features_count - df.shape[0]
        print(f&#34;[INFO] DataFrame {key} - Removed {removed_features_count} features.&#34;)
    
        # Step 2: Create masks for informative samples
        # Compute standard deviation of samples (along columns)
        sample_stdevs = df.std(axis=0)
        # Create mask for samples that do not have std dev of 0 or NaN
        mask = np.logical_and(sample_stdevs != 0, np.logical_not(np.isnan(sample_stdevs)))
        sample_masks.append(mask)

        cleaned_dfs[key] = df

    # Find samples that are informative in all dataframes
    common_mask = pd.DataFrame(sample_masks).all()

    # Second pass: apply common mask to all dataframes
    for key in cleaned_dfs.keys():
        original_samples_count = cleaned_dfs[key].shape[1]
        cleaned_dfs[key] = cleaned_dfs[key].loc[:, common_mask]
        removed_samples_count = original_samples_count - cleaned_dfs[key].shape[1]
        print(f&#34;DataFrame {key} - Removed {removed_samples_count} samples ({removed_samples_count / original_samples_count * 100:.2f}%).&#34;)

    return cleaned_dfs</code></pre>
</details>
</dd>
<dt id="flexynesis.data.DataImporter.encode_labels"><code class="name flex">
<span>def <span class="ident">encode_labels</span></span>(<span>self, df)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def encode_labels(self, df):
    label_mappings = {}
    def encode_column(series):
        nonlocal label_mappings  # Declare as nonlocal so that we can modify it
        # Fill NA values with &#39;missing&#39; 
        # series = series.fillna(&#39;missing&#39;)
        if series.name not in self.encoders:
            self.encoders[series.name] = OrdinalEncoder(handle_unknown=&#39;use_encoded_value&#39;, unknown_value=-1)
            encoded_series = self.encoders[series.name].fit_transform(series.to_frame())
        else:
            encoded_series = self.encoders[series.name].transform(series.to_frame())
        
        # also save label mappings 
        label_mappings[series.name] = {
                int(code): label for code, label in enumerate(self.encoders[series.name].categories_[0])
            }
        return encoded_series.ravel()

    # Select only the categorical columns
    df_categorical = df.select_dtypes(include=[&#39;object&#39;, &#39;category&#39;]).apply(encode_column)

    # Combine the encoded categorical data with the numerical data
    df_encoded = pd.concat([df.select_dtypes(exclude=[&#39;object&#39;, &#39;category&#39;]), df_categorical], axis=1)

    # Store the variable types
    variable_types = {col: &#39;categorical&#39; for col in df_categorical.columns}
    variable_types.update({col: &#39;numerical&#39; for col in df.select_dtypes(exclude=[&#39;object&#39;, &#39;category&#39;]).columns})

    return df_encoded, variable_types, label_mappings</code></pre>
</details>
</dd>
<dt id="flexynesis.data.DataImporter.filter"><code class="name flex">
<span>def <span class="ident">filter</span></span>(<span>self, dat, min_features, top_percentile)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter(self, dat, min_features, top_percentile):
    counts = {x: max(int(dat[x].shape[0] * top_percentile / 100), min_features) for x in dat.keys()}
    dat = {x: filter_by_laplacian(dat[x].T, x, topN=counts[x]).T for x in dat.keys()}
    return dat</code></pre>
</details>
</dd>
<dt id="flexynesis.data.DataImporter.get_labels"><code class="name flex">
<span>def <span class="ident">get_labels</span></span>(<span>self, dat, ann)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_labels(self, dat, ann):
    # subset samples and reorder annotations for the samples 
    samples = list(reduce(set.intersection, [set(item) for item in [dat[x].columns for x in dat.keys()]]))
    samples = list(set(ann.index).intersection(samples))
    dat = {x: dat[x][samples] for x in dat.keys()}
    ann = ann.loc[samples]
    return dat, ann, samples</code></pre>
</details>
</dd>
<dt id="flexynesis.data.DataImporter.get_torch_dataset"><code class="name flex">
<span>def <span class="ident">get_torch_dataset</span></span>(<span>self, dat, ann, samples, feature_ann)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_torch_dataset(self, dat, ann, samples, feature_ann):
    features = {x: dat[x].index for x in dat.keys()}
    dat = {x: torch.from_numpy(np.array(dat[x].T)).float() for x in dat.keys()}

    ann, variable_types, label_mappings = self.encode_labels(ann)

    # Convert DataFrame to tensor
    ann = {col: torch.from_numpy(ann[col].values) for col in ann.columns}
    if not self.use_graph:
        return MultiomicDataset(dat, ann, variable_types, features, samples, label_mappings)
    else:
        return MultiomicPYGDataset(dat, ann, variable_types, features, samples, label_mappings, feature_ann, transform=self.transform)</code></pre>
</details>
</dd>
<dt id="flexynesis.data.DataImporter.harmonize"><code class="name flex">
<span>def <span class="ident">harmonize</span></span>(<span>self, dat1, dat2)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def harmonize(self, dat1, dat2):
    print(&#34;\n[INFO] ------------ Harmonizing Data Sets ------------&#34;)
    # Get common features
    common_features = {x: dat1[x].index.intersection(dat2[x].index) for x in self.data_types}
    # Subset both datasets to only include common features
    dat1 = {x: dat1[x].loc[common_features[x]] for x in dat1.keys()}
    dat2 = {x: dat2[x].loc[common_features[x]] for x in dat2.keys()}
    return dat1, dat2</code></pre>
</details>
</dd>
<dt id="flexynesis.data.DataImporter.import_data"><code class="name flex">
<span>def <span class="ident">import_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def import_data(self):
    print(&#34;\n[INFO] ================= Importing Data =================&#34;)
    training_path = os.path.join(self.path, &#39;train&#39;)
    testing_path = os.path.join(self.path, &#39;test&#39;)
    # NOTE: stringdb file hardcoded for now
    edges_data_path = os.path.join(self.path, self.protein_links)
    node_data_path = os.path.join(self.path, self.protein_aliases)

    self.validate_data_folders(training_path, testing_path)
    
    training_data = self.read_data(training_path)
    testing_data = self.read_data(testing_path)

    # Import graph here:
    if self.use_graph:
        # Read graph from the file
        graph_df = self.read_graph(edges_data_path)
        # Convert graph nodes names accordingly
        if self.node_name == &#34;gene_name&#34;:
            node_name_mapping = read_stringdb_aliases(node_data_path, self.node_name)
        elif self.node_name == &#34;gene_id&#34;:
            node_name_mapping = read_stringdb_aliases(node_data_path, self.node_name)
        else:
            raise NotImplementedError
        
        def fn(a):
            try:
                # lambda a: node_name_mapping[a]
                out = node_name_mapping[a]
            except KeyError:
                # print(f&#34;MISSING: [{a}]&#34;)
                out = &#34;&#34;
            return out

        graph_df[[&#34;protein1&#34;, &#34;protein2&#34;]] = graph_df[[&#34;protein1&#34;, &#34;protein2&#34;]].applymap(fn)

        available_genes: list[str] = np.unique(graph_df[[&#34;protein1&#34;, &#34;protein2&#34;]].to_numpy()).tolist()

        # If use graph, filter genes that are not in provided data
        provided_genes = []
        # Iterate over data modalities to collect all available genes
        for _df in training_data.values():
            for g in _df.index:
                if g in available_genes:
                    if g not in provided_genes:
                        provided_genes.append(g)
        # Same for testing data
        for _df in testing_data.values():
            for g in _df.index:
                if g in available_genes:
                    if g not in provided_genes:
                        provided_genes.append(g)

        initial_edge_list = stringdb_links_to_list(graph_df)
        # Now filter the graph edges based on provided_genes
        edge_list = []
        for edge in initial_edge_list:
            src, dst = edge
            if (src in provided_genes) and (dst in provided_genes):
                edge_list.append(edge)

    # cleanup uninformative features/samples, subset annotation data, do feature selection on training data
    train_dat, train_ann, train_samples, train_features = self.process_data(training_data, split = &#39;train&#39;)
    test_dat, test_ann, test_samples, test_features = self.process_data(testing_data, split = &#39;test&#39;)
    
    # harmonize feature sets in train/test
    train_dat, test_dat = self.harmonize(train_dat, test_dat)

    train_feature_ann = {}
    test_feature_ann = {}
    if self.use_graph:
        # Now filter the graph edges based on provided_genes
        # But this time separately for each modality
        for k, v in train_dat.items():
            mod_gene_list = v.index.to_list()
            node_to_idx = {node: i for i, node in enumerate(mod_gene_list)}
            mod_edge_list = []
            for edge in edge_list:
                src, dst = edge
                if (src in mod_gene_list) and (dst in mod_gene_list):
                    mod_edge_list.append([node_to_idx[src], node_to_idx[dst]])
            train_feature_ann[k] = {&#34;edge_index&#34;: torch.tensor(mod_edge_list).T}
        # Repeat the same for test data
        for k, v in train_dat.items():
            mod_gene_list = v.index.to_list()
            node_to_idx = {node: i for i, node in enumerate(mod_gene_list)}
            mod_edge_list = []
            for edge in edge_list:
                src, dst = edge
                if (src in mod_gene_list) and (dst in mod_gene_list):
                    mod_edge_list.append([node_to_idx[src], node_to_idx[dst]])
            test_feature_ann[k] = {&#34;edge_index&#34;: torch.tensor(mod_edge_list).T}

    # log_transform 
    if self.log_transform:
        print(&#34;transforming data to log scale&#34;)
        train_dat = self.transform_data(train_dat)
        test_dat = self.transform_data(test_dat)
    
    # Normalize the training data (for testing data, use normalisation factors
    # learned from training data to apply on test data (see fit = False)
    train_dat = self.normalize_data(train_dat, scaler_type=&#34;standard&#34;, fit=True)
    test_dat = self.normalize_data(test_dat, scaler_type=&#34;standard&#34;, fit=False)
    
    # encode the variable annotations, convert data matrices and annotations pytorch datasets 
    training_dataset = self.get_torch_dataset(train_dat, train_ann, train_samples, train_feature_ann)
    testing_dataset = self.get_torch_dataset(test_dat, test_ann, test_samples, test_feature_ann)
   
    # for early fusion, concatenate all data matrices and feature lists 
    if self.concatenate:
        training_dataset.dat = {&#39;all&#39;: torch.cat([training_dataset.dat[x] for x in training_dataset.dat.keys()], dim = 1)}
        training_dataset.features = {&#39;all&#39;: list(chain(*training_dataset.features.values()))}
        
        testing_dataset.dat = {&#39;all&#39;: torch.cat([testing_dataset.dat[x] for x in testing_dataset.dat.keys()], dim = 1)}
        testing_dataset.features = {&#39;all&#39;: list(chain(*testing_dataset.features.values()))}
    
    print(&#34;[INFO] Training Data Stats:\n&#34;, training_dataset.get_dataset_stats())
    print(&#34;[INFO] Test Data Stats:\n&#34;, testing_dataset.get_dataset_stats())
    
    print(&#34;[INFO] Data import successful.&#34;)
    
    
    
    return training_dataset, testing_dataset</code></pre>
</details>
</dd>
<dt id="flexynesis.data.DataImporter.normalize_data"><code class="name flex">
<span>def <span class="ident">normalize_data</span></span>(<span>self, data, scaler_type='standard', fit=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normalize_data(self, data, scaler_type=&#34;standard&#34;, fit=True):
    print(&#34;\n[INFO] --------------- Normalizing Data ---------------&#34;)
    # notice matrix transpositions during fit and finally after transformation
    # because data matrices have features on rows, 
    # while scaling methods assume features to be on the columns. 
    if fit:
        if scaler_type == &#34;standard&#34;:
            self.scalers = {x: StandardScaler().fit(data[x].T) for x in data.keys()}
        elif scaler_type == &#34;min_max&#34;:
            self.scalers = {x: MinMaxScaler().fit(data[x].T) for x in data.keys()}
        else:
            raise ValueError(&#34;Invalid scaler_type. Choose &#39;standard&#39; or &#39;min_max&#39;.&#34;)
    
    normalized_data = {x: pd.DataFrame(self.scalers[x].transform(data[x].T), 
                                       index=data[x].columns, 
                                       columns=data[x].index).T 
                       for x in data.keys()}
    return normalized_data</code></pre>
</details>
</dd>
<dt id="flexynesis.data.DataImporter.process_data"><code class="name flex">
<span>def <span class="ident">process_data</span></span>(<span>self, data, split='train')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_data(self, data, split = &#39;train&#39;):
    print(f&#34;\n[INFO] ---------- Processing Data ({split}) ----------&#34;)
    # remove uninformative features and samples with no information (from data matrices)
    dat = self.cleanup_data({x: data[x] for x in self.data_types})
    ann = data[&#39;clin&#39;]
    dat, ann, samples = self.get_labels(dat, ann)
    # do feature selection: only applied to training data
    if split == &#39;train&#39;: 
        if self.min_features or self.top_percentile:
            dat = self.filter(dat, self.min_features, self.top_percentile)
    features = {x: dat[x].index for x in dat.keys()}
    return dat, ann, samples, features</code></pre>
</details>
</dd>
<dt id="flexynesis.data.DataImporter.read_data"><code class="name flex">
<span>def <span class="ident">read_data</span></span>(<span>self, folder_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_data(self, folder_path):
    data = {}
    required_files = {&#39;clin.csv&#39;} | {f&#34;{dt}.csv&#34; for dt in self.data_types}
    print(&#34;\n[INFO] ----------------- Reading Data -----------------&#34;)
    for file in required_files:
        file_path = os.path.join(folder_path, file)
        file_name = os.path.splitext(file)[0]
        print(f&#34;[INFO] Importing {file_path}...&#34;)
        data[file_name] = pd.read_csv(file_path, index_col=0)
    return data</code></pre>
</details>
</dd>
<dt id="flexynesis.data.DataImporter.read_graph"><code class="name flex">
<span>def <span class="ident">read_graph</span></span>(<span>self, fname=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_graph(self, fname=None):
    # NOTE: read stringdb for now
    # fname = fname or os.path.join(self.path, &#34;9606.protein.links.v12.0.txt&#34;)
    df = read_stringdb_links(fname)
    return df</code></pre>
</details>
</dd>
<dt id="flexynesis.data.DataImporter.transform_data"><code class="name flex">
<span>def <span class="ident">transform_data</span></span>(<span>self, data)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform_data(self, data):
    transformed_data = {x: np.log1p(data[x].T).T for x in data.keys()}
    return transformed_data    </code></pre>
</details>
</dd>
<dt id="flexynesis.data.DataImporter.validate_data_folders"><code class="name flex">
<span>def <span class="ident">validate_data_folders</span></span>(<span>self, training_path, testing_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate_data_folders(self, training_path, testing_path):
    print(&#34;[INFO] Validating data folders...&#34;)
    training_files = set(os.listdir(training_path))
    testing_files = set(os.listdir(testing_path))

    required_files = {&#39;clin.csv&#39;} | {f&#34;{dt}.csv&#34; for dt in self.data_types}

    if not required_files.issubset(training_files):
        missing_files = required_files - training_files
        raise ValueError(f&#34;Missing files in training folder: {&#39;, &#39;.join(missing_files)}&#34;)

    if not required_files.issubset(testing_files):
        missing_files = required_files - testing_files
        raise ValueError(f&#34;Missing files in testing folder: {&#39;, &#39;.join(missing_files)}&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="flexynesis.data.MultiomicDataset"><code class="flex name class">
<span>class <span class="ident">MultiomicDataset</span></span>
<span>(</span><span>dat, ann, variable_types, features, samples, label_mappings, feature_ann=None)</span>
</code></dt>
<dd>
<div class="desc"><p>A PyTorch dataset for multiomic data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dat</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary with keys corresponding to different types of data and values corresponding to matrices of the same shape. All matrices must have the same number of samples (rows).</dd>
<dt><strong><code>ann</code></strong> :&ensp;<code>data.frame</code></dt>
<dd>Data frame with samples on the rows, sample annotations on the columns </dd>
<dt><strong><code>features</code></strong> :&ensp;<code>list</code> or <code>np.array</code></dt>
<dd>A 1D array of feature names with length equal to the number of columns in each matrix.</dd>
<dt><strong><code>samples</code></strong> :&ensp;<code>list</code> or <code>np.array</code></dt>
<dd>A 1D array of sample names with length equal to the number of rows in each matrix.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A PyTorch dataset that can be used for training or evaluation.
Initialize the dataset.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiomicDataset(Dataset):
    &#34;&#34;&#34;A PyTorch dataset for multiomic data.

    Args:
        dat (dict): A dictionary with keys corresponding to different types of data and values corresponding to matrices of the same shape. All matrices must have the same number of samples (rows).
        ann (data.frame): Data frame with samples on the rows, sample annotations on the columns 
        features (list or np.array): A 1D array of feature names with length equal to the number of columns in each matrix.
        samples (list or np.array): A 1D array of sample names with length equal to the number of rows in each matrix.

    Returns:
        A PyTorch dataset that can be used for training or evaluation.
    &#34;&#34;&#34;

    def __init__(self, dat, ann, variable_types, features, samples, label_mappings, feature_ann=None):
        &#34;&#34;&#34;Initialize the dataset.&#34;&#34;&#34;
        self.dat = dat
        self.ann = ann
        self.variable_types = variable_types
        self.features = features
        self.samples = samples
        self.label_mappings = label_mappings
        self.feature_ann = feature_ann or {}

    def __getitem__(self, index):
        &#34;&#34;&#34;Get a single data sample from the dataset.

        Args:
            index (int): The index of the sample to retrieve.

        Returns:
            A tuple of two elements: 
                1. A dictionary with keys corresponding to the different types of data in the input dictionary `dat`, and values corresponding to the data for the given sample.
                2. The label for the given sample.
        &#34;&#34;&#34;
        subset_dat = {x: self.dat[x][index] for x in self.dat.keys()}
        subset_ann = {x: self.ann[x][index] for x in self.ann.keys()}
        return subset_dat, subset_ann
    
    def __len__ (self):
        &#34;&#34;&#34;Get the total number of samples in the dataset.

        Returns:
            An integer representing the number of samples in the dataset.
        &#34;&#34;&#34;
        return len(self.samples)
    
    def get_feature_subset(self, feature_df):
        &#34;&#34;&#34;Get a subset of data matrices corresponding to specified features and concatenate them into a pandas DataFrame.

        Args:
            feature_df (pandas.DataFrame): A DataFrame which contains at least two columns: &#39;layer&#39; and &#39;name&#39;. 

        Returns:
            A pandas DataFrame that concatenates the data matrices for the specified features from all layers. 
        &#34;&#34;&#34;
        # Convert the DataFrame to a dictionary
        feature_dict = feature_df.groupby(&#39;layer&#39;)[&#39;name&#39;].apply(list).to_dict()

        dfs = []
        for layer, features in feature_dict.items():
            if layer in self.dat:
                # Create a dictionary to look up indices by feature name for each layer
                feature_index_dict = {feature: i for i, feature in enumerate(self.features[layer])}
                # Get the indices for the requested features
                indices = [feature_index_dict[feature] for feature in features if feature in feature_index_dict]
                # Subset the data matrix for the current layer using the indices
                subset = self.dat[layer][:, indices]
                # Convert the subset to a pandas DataFrame, add the layer name as a prefix to each column name
                df = pd.DataFrame(subset, columns=[f&#39;{layer}_{feature}&#39; for feature in features if feature in feature_index_dict])
                dfs.append(df)
            else:
                print(f&#34;Layer {layer} not found in the dataset.&#34;)

        # Concatenate the dataframes along the columns axis
        result = pd.concat(dfs, axis=1)

        # Set the sample names as the row index
        result.index = self.samples

        return result
    
    def get_dataset_stats(self):
        stats = {&#39;: &#39;.join([&#39;feature_count in&#39;, x]): self.dat[x].shape[1] for x in self.dat.keys()}
        stats[&#39;sample_count&#39;] = len(self.samples)
        return(stats)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.utils.data.dataset.Dataset</li>
<li>typing.Generic</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="flexynesis.data.MultiomicPYGDataset" href="#flexynesis.data.MultiomicPYGDataset">MultiomicPYGDataset</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="flexynesis.data.MultiomicDataset.get_dataset_stats"><code class="name flex">
<span>def <span class="ident">get_dataset_stats</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dataset_stats(self):
    stats = {&#39;: &#39;.join([&#39;feature_count in&#39;, x]): self.dat[x].shape[1] for x in self.dat.keys()}
    stats[&#39;sample_count&#39;] = len(self.samples)
    return(stats)</code></pre>
</details>
</dd>
<dt id="flexynesis.data.MultiomicDataset.get_feature_subset"><code class="name flex">
<span>def <span class="ident">get_feature_subset</span></span>(<span>self, feature_df)</span>
</code></dt>
<dd>
<div class="desc"><p>Get a subset of data matrices corresponding to specified features and concatenate them into a pandas DataFrame.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>feature_df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>A DataFrame which contains at least two columns: 'layer' and 'name'. </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A pandas DataFrame that concatenates the data matrices for the specified features from all layers.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_feature_subset(self, feature_df):
    &#34;&#34;&#34;Get a subset of data matrices corresponding to specified features and concatenate them into a pandas DataFrame.

    Args:
        feature_df (pandas.DataFrame): A DataFrame which contains at least two columns: &#39;layer&#39; and &#39;name&#39;. 

    Returns:
        A pandas DataFrame that concatenates the data matrices for the specified features from all layers. 
    &#34;&#34;&#34;
    # Convert the DataFrame to a dictionary
    feature_dict = feature_df.groupby(&#39;layer&#39;)[&#39;name&#39;].apply(list).to_dict()

    dfs = []
    for layer, features in feature_dict.items():
        if layer in self.dat:
            # Create a dictionary to look up indices by feature name for each layer
            feature_index_dict = {feature: i for i, feature in enumerate(self.features[layer])}
            # Get the indices for the requested features
            indices = [feature_index_dict[feature] for feature in features if feature in feature_index_dict]
            # Subset the data matrix for the current layer using the indices
            subset = self.dat[layer][:, indices]
            # Convert the subset to a pandas DataFrame, add the layer name as a prefix to each column name
            df = pd.DataFrame(subset, columns=[f&#39;{layer}_{feature}&#39; for feature in features if feature in feature_index_dict])
            dfs.append(df)
        else:
            print(f&#34;Layer {layer} not found in the dataset.&#34;)

    # Concatenate the dataframes along the columns axis
    result = pd.concat(dfs, axis=1)

    # Set the sample names as the row index
    result.index = self.samples

    return result</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="flexynesis.data.MultiomicPYGDataset"><code class="flex name class">
<span>class <span class="ident">MultiomicPYGDataset</span></span>
<span>(</span><span>dat, ann, variable_types, features, samples, label_mappings, feature_ann=None, root=None, transform=None, pre_transform=None, pre_filter=None, log=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Multiomic pyg dataset.</p>
<p>Initialize dataset.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiomicPYGDataset(MultiomicDataset, PYGDataset):
    &#34;&#34;&#34;Multiomic pyg dataset.
    &#34;&#34;&#34;
    def __init__(
        self,
        dat,
        ann,
        variable_types,
        features,
        samples,
        label_mappings,
        feature_ann=None,
        root=None,
        transform=None,
        pre_transform=None,
        pre_filter=None,
        log=True,
    ):
        &#34;&#34;&#34;Initialize dataset.
        &#34;&#34;&#34;
        super(MultiomicPYGDataset, self).__init__(dat, ann, variable_types, features, samples, label_mappings, feature_ann)
        super(MultiomicDataset, self).__init__(root, transform, pre_transform, pre_filter, log)
        self._transform = transform
        self.transform = None

    def __getitem__(self, idx):
        return super(MultiomicDataset, self).__getitem__(idx)

    def get(self, idx: int):
        subset_dat = {}
        for k, v in self.dat.items():
            x = v[idx]
            edge_index = self.feature_ann[k][&#34;edge_index&#34;]

            # If number of node features is 1, insert a new dim:
            if v[idx].ndim == 1:
                x = x.unsqueeze(1)

            data = Data(x=x, edge_index=edge_index)

            # Apply pyg transforms here:
            if self._transform is not None:
                data = self._transform(data)

            subset_dat[k] = data

        subset_ann = {k: v[idx] for k, v in self.ann.items()}
        return subset_dat, subset_ann

    def __len__ (self):
        return super(MultiomicPYGDataset, self).__len__()

    def len(self):
        return self.__len__()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="flexynesis.data.MultiomicDataset" href="#flexynesis.data.MultiomicDataset">MultiomicDataset</a></li>
<li>torch_geometric.data.dataset.Dataset</li>
<li>torch.utils.data.dataset.Dataset</li>
<li>typing.Generic</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="flexynesis.data.MultiomicPYGDataset.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self, idx: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the data object at index :obj:<code>idx</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get(self, idx: int):
    subset_dat = {}
    for k, v in self.dat.items():
        x = v[idx]
        edge_index = self.feature_ann[k][&#34;edge_index&#34;]

        # If number of node features is 1, insert a new dim:
        if v[idx].ndim == 1:
            x = x.unsqueeze(1)

        data = Data(x=x, edge_index=edge_index)

        # Apply pyg transforms here:
        if self._transform is not None:
            data = self._transform(data)

        subset_dat[k] = data

    subset_ann = {k: v[idx] for k, v in self.ann.items()}
    return subset_dat, subset_ann</code></pre>
</details>
</dd>
<dt id="flexynesis.data.MultiomicPYGDataset.len"><code class="name flex">
<span>def <span class="ident">len</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the number of data objects stored in the dataset.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def len(self):
    return self.__len__()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="flexynesis.data.MultiomicDataset" href="#flexynesis.data.MultiomicDataset">MultiomicDataset</a></b></code>:
<ul class="hlist">
<li><code><a title="flexynesis.data.MultiomicDataset.get_feature_subset" href="#flexynesis.data.MultiomicDataset.get_feature_subset">get_feature_subset</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="flexynesis.data.TripletMultiOmicDataset"><code class="flex name class">
<span>class <span class="ident">TripletMultiOmicDataset</span></span>
<span>(</span><span>mydataset, main_var)</span>
</code></dt>
<dd>
<div class="desc"><p>For each sample (anchor) randomly chooses a positive and negative samples</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TripletMultiOmicDataset(Dataset):
    &#34;&#34;&#34;
    For each sample (anchor) randomly chooses a positive and negative samples
    &#34;&#34;&#34;

    def __init__(self, mydataset, main_var):
        self.dataset = mydataset
        self.main_var = main_var
        self.labels_set, self.label_to_indices = self.get_label_indices(self.dataset.ann[self.main_var])
    def __getitem__(self, index):
        # get anchor sample and its label
        anchor, y_dict = self.dataset[index][0], self.dataset[index][1] 
        # choose another sample with same label
        label = y_dict[self.main_var].item()
        positive_index = index
        while positive_index == index:
            positive_index = np.random.choice(self.label_to_indices[label])
        # choose another sample with a different label 
        negative_label = np.random.choice(list(self.labels_set - set([label])))
        negative_index = np.random.choice(self.label_to_indices[negative_label])
        pos = self.dataset[positive_index][0] # positive example
        neg = self.dataset[negative_index][0] # negative example
        return anchor, pos, neg, y_dict

    def __len__(self):
        return len(self.dataset)
    
    def get_label_indices(self, labels):
        labels_set = set(labels.numpy())
        label_to_indices = {label: np.where(labels.numpy() == label)[0]
                             for label in labels_set}
        return labels_set, label_to_indices   </code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.utils.data.dataset.Dataset</li>
<li>typing.Generic</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="flexynesis.data.TripletMultiOmicDataset.get_label_indices"><code class="name flex">
<span>def <span class="ident">get_label_indices</span></span>(<span>self, labels)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_label_indices(self, labels):
    labels_set = set(labels.numpy())
    label_to_indices = {label: np.where(labels.numpy() == label)[0]
                         for label in labels_set}
    return labels_set, label_to_indices   </code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="flexynesis" href="index.html">flexynesis</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="flexynesis.data.read_stringdb_aliases" href="#flexynesis.data.read_stringdb_aliases">read_stringdb_aliases</a></code></li>
<li><code><a title="flexynesis.data.read_stringdb_links" href="#flexynesis.data.read_stringdb_links">read_stringdb_links</a></code></li>
<li><code><a title="flexynesis.data.split_by_median" href="#flexynesis.data.split_by_median">split_by_median</a></code></li>
<li><code><a title="flexynesis.data.stringdb_links_to_list" href="#flexynesis.data.stringdb_links_to_list">stringdb_links_to_list</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="flexynesis.data.DataImporter" href="#flexynesis.data.DataImporter">DataImporter</a></code></h4>
<ul class="">
<li><code><a title="flexynesis.data.DataImporter.cleanup_data" href="#flexynesis.data.DataImporter.cleanup_data">cleanup_data</a></code></li>
<li><code><a title="flexynesis.data.DataImporter.encode_labels" href="#flexynesis.data.DataImporter.encode_labels">encode_labels</a></code></li>
<li><code><a title="flexynesis.data.DataImporter.filter" href="#flexynesis.data.DataImporter.filter">filter</a></code></li>
<li><code><a title="flexynesis.data.DataImporter.get_labels" href="#flexynesis.data.DataImporter.get_labels">get_labels</a></code></li>
<li><code><a title="flexynesis.data.DataImporter.get_torch_dataset" href="#flexynesis.data.DataImporter.get_torch_dataset">get_torch_dataset</a></code></li>
<li><code><a title="flexynesis.data.DataImporter.harmonize" href="#flexynesis.data.DataImporter.harmonize">harmonize</a></code></li>
<li><code><a title="flexynesis.data.DataImporter.import_data" href="#flexynesis.data.DataImporter.import_data">import_data</a></code></li>
<li><code><a title="flexynesis.data.DataImporter.normalize_data" href="#flexynesis.data.DataImporter.normalize_data">normalize_data</a></code></li>
<li><code><a title="flexynesis.data.DataImporter.process_data" href="#flexynesis.data.DataImporter.process_data">process_data</a></code></li>
<li><code><a title="flexynesis.data.DataImporter.protein_aliases" href="#flexynesis.data.DataImporter.protein_aliases">protein_aliases</a></code></li>
<li><code><a title="flexynesis.data.DataImporter.protein_links" href="#flexynesis.data.DataImporter.protein_links">protein_links</a></code></li>
<li><code><a title="flexynesis.data.DataImporter.read_data" href="#flexynesis.data.DataImporter.read_data">read_data</a></code></li>
<li><code><a title="flexynesis.data.DataImporter.read_graph" href="#flexynesis.data.DataImporter.read_graph">read_graph</a></code></li>
<li><code><a title="flexynesis.data.DataImporter.transform_data" href="#flexynesis.data.DataImporter.transform_data">transform_data</a></code></li>
<li><code><a title="flexynesis.data.DataImporter.validate_data_folders" href="#flexynesis.data.DataImporter.validate_data_folders">validate_data_folders</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flexynesis.data.MultiomicDataset" href="#flexynesis.data.MultiomicDataset">MultiomicDataset</a></code></h4>
<ul class="">
<li><code><a title="flexynesis.data.MultiomicDataset.get_dataset_stats" href="#flexynesis.data.MultiomicDataset.get_dataset_stats">get_dataset_stats</a></code></li>
<li><code><a title="flexynesis.data.MultiomicDataset.get_feature_subset" href="#flexynesis.data.MultiomicDataset.get_feature_subset">get_feature_subset</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flexynesis.data.MultiomicPYGDataset" href="#flexynesis.data.MultiomicPYGDataset">MultiomicPYGDataset</a></code></h4>
<ul class="">
<li><code><a title="flexynesis.data.MultiomicPYGDataset.get" href="#flexynesis.data.MultiomicPYGDataset.get">get</a></code></li>
<li><code><a title="flexynesis.data.MultiomicPYGDataset.len" href="#flexynesis.data.MultiomicPYGDataset.len">len</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flexynesis.data.TripletMultiOmicDataset" href="#flexynesis.data.TripletMultiOmicDataset">TripletMultiOmicDataset</a></code></h4>
<ul class="">
<li><code><a title="flexynesis.data.TripletMultiOmicDataset.get_label_indices" href="#flexynesis.data.TripletMultiOmicDataset.get_label_indices">get_label_indices</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>