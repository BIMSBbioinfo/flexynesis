<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>flexynesis.models API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>flexynesis.models</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from .direct_pred import DirectPred
from .direct_pred_cnn import DirectPredCNN
from .direct_pred_gcnn import DirectPredGCNN
from .supervised_vae import supervised_vae
from .triplet_encoder import MultiTripletNetwork

__all__ = [&#34;DirectPred&#34;, &#34;DirectPredCNN&#34;, &#34;DirectPredGCNN&#34;, &#34;supervised_vae&#34;, &#34;MultiTripletNetwork&#34;]</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="flexynesis.models.direct_pred" href="direct_pred.html">flexynesis.models.direct_pred</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="flexynesis.models.direct_pred_cnn" href="direct_pred_cnn.html">flexynesis.models.direct_pred_cnn</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="flexynesis.models.direct_pred_gcnn" href="direct_pred_gcnn.html">flexynesis.models.direct_pred_gcnn</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="flexynesis.models.triplet_encoder" href="triplet_encoder.html">flexynesis.models.triplet_encoder</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="flexynesis.models.DirectPred"><code class="flex name class">
<span>class <span class="ident">DirectPred</span></span>
<span>(</span><span>config, dataset, target_variables, batch_variables=None, val_size=0.2, use_loss_weighting=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Hooks to be used in LightningModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DirectPred(pl.LightningModule):
    def __init__(self, config, dataset, target_variables, batch_variables = None, val_size = 0.2, use_loss_weighting = True):
        super(DirectPred, self).__init__()
        self.config = config
        self.dataset = dataset
        self.target_variables = target_variables
        self.batch_variables = batch_variables
        self.variables = target_variables + batch_variables if batch_variables else target_variables
        self.val_size = val_size
        self.dat_train, self.dat_val = self.prepare_data()
        self.feature_importances = {}
        self.use_loss_weighting = use_loss_weighting

        if self.use_loss_weighting:
            # Initialize log variance parameters for uncertainty weighting
            self.log_vars = nn.ParameterDict()
            for var in self.variables:
                self.log_vars[var] = nn.Parameter(torch.zeros(1))

        layers = list(dataset.dat.keys())
        input_dims = [len(dataset.features[layers[i]]) for i in range(len(layers))]

        self.encoders = nn.ModuleList([
            MLP(input_dim=input_dims[i],
                hidden_dim=self.config[&#39;hidden_dim&#39;],
                output_dim=self.config[&#39;latent_dim&#39;]) for i in range(len(layers))])

        self.MLPs = nn.ModuleDict()  # using ModuleDict to store multiple MLPs
        for var in self.variables:
            if self.dataset.variable_types[var] == &#39;numerical&#39;:
                num_class = 1
            else:
                num_class = len(np.unique(self.dataset.ann[var]))
            self.MLPs[var] = MLP(input_dim=self.config[&#39;latent_dim&#39;] * len(layers),
                                 hidden_dim=self.config[&#39;hidden_dim&#39;],
                                 output_dim=num_class)

    def forward(self, x_list):
        &#34;&#34;&#34;
        Forward pass of the DirectPred model.

        Args:
            x_list (list of torch.Tensor): A list of input matrices (omics layers), one for each layer.

        Returns:
            dict: A dictionary where each key-value pair corresponds to the target variable name and its predicted output respectively.
        &#34;&#34;&#34;
        embeddings_list = []
        # Process each input matrix with its corresponding Encoder
        for i, x in enumerate(x_list):
            embeddings_list.append(self.encoders[i](x))
        embeddings_concat = torch.cat(embeddings_list, dim=1)

        outputs = {}
        for var, mlp in self.MLPs.items():
            outputs[var] = mlp(embeddings_concat)
        return outputs  
    
    
    def configure_optimizers(self):
        &#34;&#34;&#34;
        Configure the optimizer for the DirectPred model.

        Returns:
            torch.optim.Optimizer: The configured optimizer.
        &#34;&#34;&#34;

        optimizer = torch.optim.Adam(self.parameters(), lr=self.config[&#39;lr&#39;])
        return optimizer
    
    def compute_loss(self, var, y, y_hat):
        if self.dataset.variable_types[var] == &#39;numerical&#39;:
            # Ignore instances with missing labels for numerical variables
            valid_indices = ~torch.isnan(y)
            if valid_indices.sum() &gt; 0:  # only calculate loss if there are valid targets
                y_hat = y_hat[valid_indices]
                y = y[valid_indices]
                loss = F.mse_loss(torch.flatten(y_hat), y.float())
            else:
                loss = torch.tensor(0.0, device=y_hat.device, requires_grad=True) # if no valid labels, set loss to 0
        else:
            # Ignore instances with missing labels for categorical variables
            # Assuming that missing values were encoded as -1
            valid_indices = (y != -1) &amp; (~torch.isnan(y))
            if valid_indices.sum() &gt; 0:  # only calculate loss if there are valid targets
                y_hat = y_hat[valid_indices]
                y = y[valid_indices]
                loss = F.cross_entropy(y_hat, y.long())
            else: 
                loss = torch.tensor(0.0, device=y_hat.device, requires_grad=True)
        return loss
    
    def compute_total_loss(self, losses):
        if self.use_loss_weighting and len(losses) &gt; 1:
            # Compute weighted loss for each loss 
            # Weighted loss = precision * loss + log-variance
            total_loss = sum(torch.exp(-self.log_vars[name]) * loss + self.log_vars[name] for name, loss in losses.items())
        else:
            # Compute unweighted total loss
            total_loss = sum(losses.values())
        return total_loss

    def training_step(self, train_batch, batch_idx):
        &#34;&#34;&#34;
        Perform a single training step.
        Args:
            train_batch (tuple): A tuple containing the input data and labels for the current batch.
            batch_idx (int): The index of the current batch.
        Returns:
            torch.Tensor: The total loss for the current training step.
        &#34;&#34;&#34;
        
        dat, y_dict = train_batch       
        layers = dat.keys()
        x_list = [dat[x] for x in layers]
        outputs = self.forward(x_list)
        losses = {}
        for var in self.variables:
            y_hat = outputs[var]
            y = y_dict[var]
            loss = self.compute_loss(var, y, y_hat)
            losses[var] = loss
            
        total_loss = self.compute_total_loss(losses)
        # add train loss for logging
        losses[&#39;train_loss&#39;] = total_loss
        self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True)
        return total_loss
    
    def validation_step(self, val_batch, batch_idx):
        &#34;&#34;&#34;
        Perform a single validation step.

        Args:
            val_batch (tuple): A tuple containing the input data and labels for the current batch.
            batch_idx (int): The index of the current batch.

        Returns:
            torch.Tensor: The total loss for the current validation step.
        &#34;&#34;&#34;
        dat, y_dict = val_batch       
        layers = dat.keys()
        x_list = [dat[x] for x in layers]
        outputs = self.forward(x_list)
        losses = {}
        for var in self.variables:
            y_hat = outputs[var]
            y = y_dict[var]
            loss = self.compute_loss(var, y, y_hat)
            losses[var] = loss
        total_loss = sum(losses.values())
        losses[&#39;val_loss&#39;] = total_loss
        self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True)
        return total_loss

    
    def prepare_data(self):
        lt = int(len(self.dataset)*(1-self.val_size))
        lv = len(self.dataset)-lt
        dat_train, dat_val = random_split(self.dataset, [lt, lv], 
                                          generator=torch.Generator().manual_seed(42))
        return dat_train, dat_val
    
    def train_dataloader(self):
        return DataLoader(self.dat_train, batch_size=int(self.config[&#39;batch_size&#39;]), num_workers=0, pin_memory=True, shuffle=True, drop_last=True)

    def val_dataloader(self):
        return DataLoader(self.dat_val, batch_size=int(self.config[&#39;batch_size&#39;]), num_workers=0, pin_memory=True, shuffle=False)
    
    def predict(self, dataset):
        &#34;&#34;&#34;
        Evaluate the DirectPred model on a given dataset.

        Args:
            dataset: The dataset to evaluate the model on.

        Returns:
            A dictionary where each key is a target variable and the corresponding value is the predicted output for that variable.
        &#34;&#34;&#34;
        self.eval()
        layers = dataset.dat.keys()
        x_list = [dataset.dat[x] for x in layers]
        outputs = self.forward(x_list)

        predictions = {}
        for var in self.variables:
            y_pred = outputs[var].detach().numpy()
            if self.dataset.variable_types[var] == &#39;categorical&#39;:
                predictions[var] = np.argmax(y_pred, axis=1)
            else:
                predictions[var] = y_pred
        return predictions

    def transform(self, dataset):
        &#34;&#34;&#34;
        Transform the input data into a lower-dimensional space using the trained encoders.

        Args:
            dataset: The input dataset containing the omics data.

        Returns:
            pd.DataFrame: A dataframe of embeddings where the row indices are 
                          dataset.samples and the column names are created by appending 
                          the substring &#34;E&#34; to each dimension index.
        &#34;&#34;&#34;
        self.eval()
        embeddings_list = []
        # Process each input matrix with its corresponding Encoder
        for i, x in enumerate(dataset.dat.values()):
            embeddings_list.append(self.encoders[i](x))
        embeddings_concat = torch.cat(embeddings_list, dim=1)

        # Converting tensor to numpy array and then to DataFrame
        embeddings_df = pd.DataFrame(embeddings_concat.detach().numpy(), 
                                     index=dataset.samples,
                                     columns=[f&#34;E{dim}&#34; for dim in range(embeddings_concat.shape[1])])
        return embeddings_df
        
    # Adaptor forward function for captum integrated gradients. 
    def forward_target(self, *args):
        input_data = list(args[:-2])  # one or more tensors (one per omics layer)
        target_var = args[-2]  # target variable of interest
        steps = args[-1]  # number of steps for IntegratedGradients().attribute 
        outputs_list = []
        for i in range(steps):
            # get list of tensors for each step into a list of tensors
            x_step = [input_data[j][i] for j in range(len(input_data))]
            out = self.forward(x_step)
            outputs_list.append(out[target_var])
        return torch.cat(outputs_list, dim = 0)
        
    def compute_feature_importance(self, target_var, steps = 5):
        &#34;&#34;&#34;
        Compute the feature importance.

        Args:
            input_data (torch.Tensor): The input data to compute the feature importance for.
            target_var (str): The target variable to compute the feature importance for.
        Returns:
            attributions (list of torch.Tensor): The feature importances for each class.
        &#34;&#34;&#34;
        x_list = [self.dataset.dat[x] for x in self.dataset.dat.keys()]
                
        # Initialize the Integrated Gradients method
        ig = IntegratedGradients(self.forward_target)

        input_data = tuple([data.unsqueeze(0).requires_grad_() for data in x_list])

        # Define a baseline (you might need to adjust this depending on your actual data)
        baseline = tuple([torch.zeros_like(data) for data in input_data])

        # Get the number of classes for the target variable
        if self.dataset.variable_types[target_var] == &#39;numerical&#39;:
            num_class = 1
        else:
            num_class = len(np.unique(self.dataset.ann[target_var]))

        # Compute the feature importance for each class
        attributions = []
        if num_class &gt; 1:
            for target_class in range(num_class):
                attributions.append(ig.attribute(input_data, baseline, additional_forward_args=(target_var, steps), target=target_class, n_steps=steps))
        else:
            attributions.append(ig.attribute(input_data, baseline, additional_forward_args=(target_var, steps), n_steps=steps))

        # summarize feature importances
        # Compute absolute attributions
        abs_attr = [[torch.abs(a) for a in attr_class] for attr_class in attributions]
        # average over samples 
        imp = [[a.mean(dim=1) for a in attr_class] for attr_class in abs_attr]

        # combine into a single data frame 
        df_list = []
        layers = list(self.dataset.dat.keys())
        for i in range(num_class):
            for j in range(len(layers)):
                features = self.dataset.features[layers[j]]
                importances = imp[i][j][0].detach().numpy()
                df_list.append(pd.DataFrame({&#39;target_variable&#39;: target_var, &#39;target_class&#39;: i, &#39;layer&#39;: layers[j], &#39;name&#39;: features, &#39;importance&#39;: importances}))    
        df_imp = pd.concat(df_list, ignore_index = True)
        
        # save the computed scores in the model
        self.feature_importances[target_var] = df_imp</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pytorch_lightning.core.module.LightningModule</li>
<li>lightning_fabric.utilities.device_dtype_mixin._DeviceDtypeModuleMixin</li>
<li>pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin</li>
<li>pytorch_lightning.core.hooks.ModelHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="flexynesis.models.DirectPred.compute_feature_importance"><code class="name flex">
<span>def <span class="ident">compute_feature_importance</span></span>(<span>self, target_var, steps=5)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the feature importance.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_data</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The input data to compute the feature importance for.</dd>
<dt><strong><code>target_var</code></strong> :&ensp;<code>str</code></dt>
<dd>The target variable to compute the feature importance for.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>attributions (list of torch.Tensor): The feature importances for each class.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_feature_importance(self, target_var, steps = 5):
    &#34;&#34;&#34;
    Compute the feature importance.

    Args:
        input_data (torch.Tensor): The input data to compute the feature importance for.
        target_var (str): The target variable to compute the feature importance for.
    Returns:
        attributions (list of torch.Tensor): The feature importances for each class.
    &#34;&#34;&#34;
    x_list = [self.dataset.dat[x] for x in self.dataset.dat.keys()]
            
    # Initialize the Integrated Gradients method
    ig = IntegratedGradients(self.forward_target)

    input_data = tuple([data.unsqueeze(0).requires_grad_() for data in x_list])

    # Define a baseline (you might need to adjust this depending on your actual data)
    baseline = tuple([torch.zeros_like(data) for data in input_data])

    # Get the number of classes for the target variable
    if self.dataset.variable_types[target_var] == &#39;numerical&#39;:
        num_class = 1
    else:
        num_class = len(np.unique(self.dataset.ann[target_var]))

    # Compute the feature importance for each class
    attributions = []
    if num_class &gt; 1:
        for target_class in range(num_class):
            attributions.append(ig.attribute(input_data, baseline, additional_forward_args=(target_var, steps), target=target_class, n_steps=steps))
    else:
        attributions.append(ig.attribute(input_data, baseline, additional_forward_args=(target_var, steps), n_steps=steps))

    # summarize feature importances
    # Compute absolute attributions
    abs_attr = [[torch.abs(a) for a in attr_class] for attr_class in attributions]
    # average over samples 
    imp = [[a.mean(dim=1) for a in attr_class] for attr_class in abs_attr]

    # combine into a single data frame 
    df_list = []
    layers = list(self.dataset.dat.keys())
    for i in range(num_class):
        for j in range(len(layers)):
            features = self.dataset.features[layers[j]]
            importances = imp[i][j][0].detach().numpy()
            df_list.append(pd.DataFrame({&#39;target_variable&#39;: target_var, &#39;target_class&#39;: i, &#39;layer&#39;: layers[j], &#39;name&#39;: features, &#39;importance&#39;: importances}))    
    df_imp = pd.concat(df_list, ignore_index = True)
    
    # save the computed scores in the model
    self.feature_importances[target_var] = df_imp</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPred.compute_loss"><code class="name flex">
<span>def <span class="ident">compute_loss</span></span>(<span>self, var, y, y_hat)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_loss(self, var, y, y_hat):
    if self.dataset.variable_types[var] == &#39;numerical&#39;:
        # Ignore instances with missing labels for numerical variables
        valid_indices = ~torch.isnan(y)
        if valid_indices.sum() &gt; 0:  # only calculate loss if there are valid targets
            y_hat = y_hat[valid_indices]
            y = y[valid_indices]
            loss = F.mse_loss(torch.flatten(y_hat), y.float())
        else:
            loss = torch.tensor(0.0, device=y_hat.device, requires_grad=True) # if no valid labels, set loss to 0
    else:
        # Ignore instances with missing labels for categorical variables
        # Assuming that missing values were encoded as -1
        valid_indices = (y != -1) &amp; (~torch.isnan(y))
        if valid_indices.sum() &gt; 0:  # only calculate loss if there are valid targets
            y_hat = y_hat[valid_indices]
            y = y[valid_indices]
            loss = F.cross_entropy(y_hat, y.long())
        else: 
            loss = torch.tensor(0.0, device=y_hat.device, requires_grad=True)
    return loss</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPred.compute_total_loss"><code class="name flex">
<span>def <span class="ident">compute_total_loss</span></span>(<span>self, losses)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_total_loss(self, losses):
    if self.use_loss_weighting and len(losses) &gt; 1:
        # Compute weighted loss for each loss 
        # Weighted loss = precision * loss + log-variance
        total_loss = sum(torch.exp(-self.log_vars[name]) * loss + self.log_vars[name] for name, loss in losses.items())
    else:
        # Compute unweighted total loss
        total_loss = sum(losses.values())
    return total_loss</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPred.configure_optimizers"><code class="name flex">
<span>def <span class="ident">configure_optimizers</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Configure the optimizer for the DirectPred model.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.optim.Optimizer</code></dt>
<dd>The configured optimizer.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure_optimizers(self):
    &#34;&#34;&#34;
    Configure the optimizer for the DirectPred model.

    Returns:
        torch.optim.Optimizer: The configured optimizer.
    &#34;&#34;&#34;

    optimizer = torch.optim.Adam(self.parameters(), lr=self.config[&#39;lr&#39;])
    return optimizer</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPred.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x_list) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass of the DirectPred model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x_list</code></strong> :&ensp;<code>list</code> of <code>torch.Tensor</code></dt>
<dd>A list of input matrices (omics layers), one for each layer.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>A dictionary where each key-value pair corresponds to the target variable name and its predicted output respectively.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x_list):
    &#34;&#34;&#34;
    Forward pass of the DirectPred model.

    Args:
        x_list (list of torch.Tensor): A list of input matrices (omics layers), one for each layer.

    Returns:
        dict: A dictionary where each key-value pair corresponds to the target variable name and its predicted output respectively.
    &#34;&#34;&#34;
    embeddings_list = []
    # Process each input matrix with its corresponding Encoder
    for i, x in enumerate(x_list):
        embeddings_list.append(self.encoders[i](x))
    embeddings_concat = torch.cat(embeddings_list, dim=1)

    outputs = {}
    for var, mlp in self.MLPs.items():
        outputs[var] = mlp(embeddings_concat)
    return outputs  </code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPred.forward_target"><code class="name flex">
<span>def <span class="ident">forward_target</span></span>(<span>self, *args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_target(self, *args):
    input_data = list(args[:-2])  # one or more tensors (one per omics layer)
    target_var = args[-2]  # target variable of interest
    steps = args[-1]  # number of steps for IntegratedGradients().attribute 
    outputs_list = []
    for i in range(steps):
        # get list of tensors for each step into a list of tensors
        x_step = [input_data[j][i] for j in range(len(input_data))]
        out = self.forward(x_step)
        outputs_list.append(out[target_var])
    return torch.cat(outputs_list, dim = 0)</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPred.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, dataset)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the DirectPred model on a given dataset.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset</code></strong></dt>
<dd>The dataset to evaluate the model on.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A dictionary where each key is a target variable and the corresponding value is the predicted output for that variable.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, dataset):
    &#34;&#34;&#34;
    Evaluate the DirectPred model on a given dataset.

    Args:
        dataset: The dataset to evaluate the model on.

    Returns:
        A dictionary where each key is a target variable and the corresponding value is the predicted output for that variable.
    &#34;&#34;&#34;
    self.eval()
    layers = dataset.dat.keys()
    x_list = [dataset.dat[x] for x in layers]
    outputs = self.forward(x_list)

    predictions = {}
    for var in self.variables:
        y_pred = outputs[var].detach().numpy()
        if self.dataset.variable_types[var] == &#39;categorical&#39;:
            predictions[var] = np.argmax(y_pred, axis=1)
        else:
            predictions[var] = y_pred
    return predictions</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPred.prepare_data"><code class="name flex">
<span>def <span class="ident">prepare_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Use this to download and prepare data. Downloading and saving data with multiple processes (distributed
settings) will result in corrupted data. Lightning ensures this method is called only within a single process,
so you can safely add your downloading logic within.</p>
<div class="admonition warning">
<p class="admonition-title">Warning:&ensp;DO NOT set state to the model (use <code>setup</code> instead)</p>
<p>since this is NOT called on every device</p>
</div>
<p>Example::</p>
<pre><code>def prepare_data(self):
    # good
    download_data()
    tokenize()
    etc()

    # bad
    self.split = data_split
    self.some_state = some_other_state()
</code></pre>
<p>In a distributed environment, <code>prepare_data</code> can be called in two ways
(using :ref:<code>prepare_data_per_node&lt;common/lightning_module:prepare_data_per_node&gt;</code>)</p>
<ol>
<li>Once per node. This is the default and is only called on LOCAL_RANK=0.</li>
<li>Once in total. Only called on GLOBAL_RANK=0.</li>
</ol>
<p>Example::</p>
<pre><code># DEFAULT
# called once per node on LOCAL_RANK=0 of that node
class LitDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()
        self.prepare_data_per_node = True


# call on GLOBAL_RANK=0 (great for shared file systems)
class LitDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()
        self.prepare_data_per_node = False
</code></pre>
<p>This is called before requesting the dataloaders:</p>
<p>.. code-block:: python</p>
<pre><code>model.prepare_data()
initialize_distributed()
model.setup(stage)
model.train_dataloader()
model.val_dataloader()
model.test_dataloader()
model.predict_dataloader()
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_data(self):
    lt = int(len(self.dataset)*(1-self.val_size))
    lv = len(self.dataset)-lt
    dat_train, dat_val = random_split(self.dataset, [lt, lv], 
                                      generator=torch.Generator().manual_seed(42))
    return dat_train, dat_val</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPred.train_dataloader"><code class="name flex">
<span>def <span class="ident">train_dataloader</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>An iterable or collection of iterables specifying training samples.</p>
<p>For more information about multiple dataloaders, see this :ref:<code>section &lt;multiple-dataloaders&gt;</code>.</p>
<p>The dataloader you return will not be reloaded unless you set
:paramref:<code>~pytorch_lightning.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs</code> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<pre><code>- download in :meth:&lt;code&gt;prepare\_data&lt;/code&gt;
- process and split in :meth:&lt;code&gt;setup&lt;/code&gt;
</code></pre>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning:&ensp;do not assign state in prepare_data</p>
</div>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.trainer.Trainer.fit</code></li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_dataloader(self):
    return DataLoader(self.dat_train, batch_size=int(self.config[&#39;batch_size&#39;]), num_workers=0, pin_memory=True, shuffle=True, drop_last=True)</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPred.training_step"><code class="name flex">
<span>def <span class="ident">training_step</span></span>(<span>self, train_batch, batch_idx)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform a single training step.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>train_batch</code></strong> :&ensp;<code>tuple</code></dt>
<dd>A tuple containing the input data and labels for the current batch.</dd>
<dt><strong><code>batch_idx</code></strong> :&ensp;<code>int</code></dt>
<dd>The index of the current batch.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>The total loss for the current training step.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def training_step(self, train_batch, batch_idx):
    &#34;&#34;&#34;
    Perform a single training step.
    Args:
        train_batch (tuple): A tuple containing the input data and labels for the current batch.
        batch_idx (int): The index of the current batch.
    Returns:
        torch.Tensor: The total loss for the current training step.
    &#34;&#34;&#34;
    
    dat, y_dict = train_batch       
    layers = dat.keys()
    x_list = [dat[x] for x in layers]
    outputs = self.forward(x_list)
    losses = {}
    for var in self.variables:
        y_hat = outputs[var]
        y = y_dict[var]
        loss = self.compute_loss(var, y, y_hat)
        losses[var] = loss
        
    total_loss = self.compute_total_loss(losses)
    # add train loss for logging
    losses[&#39;train_loss&#39;] = total_loss
    self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True)
    return total_loss</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPred.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, dataset)</span>
</code></dt>
<dd>
<div class="desc"><p>Transform the input data into a lower-dimensional space using the trained encoders.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset</code></strong></dt>
<dd>The input dataset containing the omics data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>A dataframe of embeddings where the row indices are
dataset.samples and the column names are created by appending
the substring "E" to each dimension index.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, dataset):
    &#34;&#34;&#34;
    Transform the input data into a lower-dimensional space using the trained encoders.

    Args:
        dataset: The input dataset containing the omics data.

    Returns:
        pd.DataFrame: A dataframe of embeddings where the row indices are 
                      dataset.samples and the column names are created by appending 
                      the substring &#34;E&#34; to each dimension index.
    &#34;&#34;&#34;
    self.eval()
    embeddings_list = []
    # Process each input matrix with its corresponding Encoder
    for i, x in enumerate(dataset.dat.values()):
        embeddings_list.append(self.encoders[i](x))
    embeddings_concat = torch.cat(embeddings_list, dim=1)

    # Converting tensor to numpy array and then to DataFrame
    embeddings_df = pd.DataFrame(embeddings_concat.detach().numpy(), 
                                 index=dataset.samples,
                                 columns=[f&#34;E{dim}&#34; for dim in range(embeddings_concat.shape[1])])
    return embeddings_df</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPred.val_dataloader"><code class="name flex">
<span>def <span class="ident">val_dataloader</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>An iterable or collection of iterables specifying validation samples.</p>
<p>For more information about multiple dataloaders, see this :ref:<code>section &lt;multiple-dataloaders&gt;</code>.</p>
<p>The dataloader you return will not be reloaded unless you set
:paramref:<code>~pytorch_lightning.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs</code> to
a positive integer.</p>
<p>It's recommended that all data downloads and preparation happen in :meth:<code>prepare_data</code>.</p>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.trainer.Trainer.fit</code></li>
<li>:meth:<code>~pytorch_lightning.trainer.trainer.Trainer.validate</code></li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
<h2 id="note_1">Note</h2>
<p>If you don't need a validation dataset and a :meth:<code>validation_step</code>, you don't need to
implement this method.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def val_dataloader(self):
    return DataLoader(self.dat_val, batch_size=int(self.config[&#39;batch_size&#39;]), num_workers=0, pin_memory=True, shuffle=False)</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPred.validation_step"><code class="name flex">
<span>def <span class="ident">validation_step</span></span>(<span>self, val_batch, batch_idx)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform a single validation step.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>val_batch</code></strong> :&ensp;<code>tuple</code></dt>
<dd>A tuple containing the input data and labels for the current batch.</dd>
<dt><strong><code>batch_idx</code></strong> :&ensp;<code>int</code></dt>
<dd>The index of the current batch.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>The total loss for the current validation step.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validation_step(self, val_batch, batch_idx):
    &#34;&#34;&#34;
    Perform a single validation step.

    Args:
        val_batch (tuple): A tuple containing the input data and labels for the current batch.
        batch_idx (int): The index of the current batch.

    Returns:
        torch.Tensor: The total loss for the current validation step.
    &#34;&#34;&#34;
    dat, y_dict = val_batch       
    layers = dat.keys()
    x_list = [dat[x] for x in layers]
    outputs = self.forward(x_list)
    losses = {}
    for var in self.variables:
        y_hat = outputs[var]
        y = y_dict[var]
        loss = self.compute_loss(var, y, y_hat)
        losses[var] = loss
    total_loss = sum(losses.values())
    losses[&#39;val_loss&#39;] = total_loss
    self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True)
    return total_loss</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="flexynesis.models.DirectPredCNN"><code class="flex name class">
<span>class <span class="ident">DirectPredCNN</span></span>
<span>(</span><span>config, dataset, target_variables, batch_variables=None, val_size=0.2)</span>
</code></dt>
<dd>
<div class="desc"><p>Hooks to be used in LightningModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DirectPredCNN(pl.LightningModule):
    def __init__(self, config, dataset, target_variables, batch_variables=None, val_size=0.2):
        super().__init__()
        self.config = config
        self.dataset = dataset
        self.target_variables = target_variables
        self.batch_variables = batch_variables
        self.variables = target_variables + batch_variables if batch_variables else target_variables
        self.val_size = val_size
        self.dat_train, self.dat_val = self.prepare_data()
        self.feature_importances = {}
        # Init modality encoders
        layers = list(self.dataset.dat.keys())
        input_dims = [len(self.dataset.features[layers[i]]) for i in range(len(layers))]
        self.encoders = nn.ModuleList(
            [
                CNN(input_dim=input_dims[i], hidden_dim=self.config[&#34;hidden_dim&#34;], output_dim=self.config[&#34;latent_dim&#34;])
                for i in range(len(layers))
            ]
        )
        # Init output layers
        layers = list(self.dataset.dat.keys())
        self.MLPs = nn.ModuleDict()
        for var in self.target_variables:
            if self.dataset.variable_types[var] == &#34;numerical&#34;:
                num_class = 1
            else:
                num_class = len(np.unique(self.dataset.ann[var]))
            self.MLPs[var] = CNN(
                input_dim=self.config[&#34;latent_dim&#34;] * len(layers),
                hidden_dim=self.config[&#34;hidden_dim&#34;],
                output_dim=num_class,
            )

    def forward(self, x_list):
        embeddings_list = []
        # Process each input matrix with its corresponding Encoder
        for i, x in enumerate(x_list):
            embeddings_list.append(self.encoders[i](x))
        embeddings_concat = torch.cat(embeddings_list, dim=1)

        outputs = {}
        for var, mlp in self.MLPs.items():
            outputs[var] = mlp(embeddings_concat)
        return outputs

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.config[&#34;lr&#34;])
        return optimizer

    def compute_loss(self, var, y, y_hat):
        if self.dataset.variable_types[var] == &#34;numerical&#34;:
            # Ignore instances with missing labels for numerical variables
            valid_indices = ~torch.isnan(y)
            if valid_indices.sum() &gt; 0:  # only calculate loss if there are valid targets
                y_hat = y_hat[valid_indices]
                y = y[valid_indices]
                loss = F.mse_loss(torch.flatten(y_hat), y.float())
            else:
                loss = torch.tensor(0)  # if no valid labels, set loss to 0
        else:
            # Ignore instances with missing labels for categorical variables
            # Assuming that missing values were encoded as -1
            valid_indices = (y != -1) &amp; (~torch.isnan(y))
            if valid_indices.sum() &gt; 0:  # only calculate loss if there are valid targets
                y_hat = y_hat[valid_indices]
                y = y[valid_indices]
                loss = F.cross_entropy(y_hat, y.long())
            else:
                loss = torch.tensor(0)
        return loss

    def training_step(self, train_batch, batch_idx):
        dat, y_dict = train_batch
        layers = dat.keys()
        x_list = [dat[x] for x in layers]
        outputs = self.forward(x_list)
        losses = {}
        for var in self.target_variables:
            y_hat = outputs[var]
            y = y_dict[var]
            loss = self.compute_loss(var, y, y_hat)
            losses[var] = loss
        total_loss = sum(losses.values())
        losses[&#34;train_loss&#34;] = total_loss
        self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True)
        return total_loss

    def validation_step(self, val_batch, batch_idx):
        dat, y_dict = val_batch
        layers = dat.keys()
        x_list = [dat[x] for x in layers]
        outputs = self.forward(x_list)
        losses = {}
        for var in self.target_variables:
            y_hat = outputs[var]
            y = y_dict[var]
            loss = self.compute_loss(var, y, y_hat)
            losses[var] = loss
        total_loss = sum(losses.values())
        losses[&#34;val_loss&#34;] = total_loss
        self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True)
        return total_loss

    def prepare_data(self):
        lt = int(len(self.dataset) * (1 - self.val_size))
        lv = len(self.dataset) - lt
        dat_train, dat_val = random_split(self.dataset, [lt, lv], generator=torch.Generator().manual_seed(42))
        return dat_train, dat_val

    def train_dataloader(self):
        return DataLoader(
            self.dat_train,
            batch_size=int(self.config[&#34;batch_size&#34;]),
            num_workers=0,
            pin_memory=True,
            shuffle=True,
            drop_last=True,
        )

    def val_dataloader(self):
        return DataLoader(
            self.dat_val, batch_size=int(self.config[&#34;batch_size&#34;]), num_workers=0, pin_memory=True, shuffle=False
        )

    def predict(self, dataset):
        self.eval()
        layers = dataset.dat.keys()
        x_list = [dataset.dat[x] for x in layers]
        outputs = self.forward(x_list)

        predictions = {}
        for var in self.target_variables:
            y_pred = outputs[var].detach().numpy()
            if self.dataset.variable_types[var] == &#34;categorical&#34;:
                predictions[var] = np.argmax(y_pred, axis=1)
            else:
                predictions[var] = y_pred
        return predictions

    def transform(self, dataset):
        self.eval()
        embeddings_list = []
        # Process each input matrix with its corresponding Encoder
        for i, x in enumerate(dataset.dat.values()):
            embeddings_list.append(self.encoders[i](x))
        embeddings_concat = torch.cat(embeddings_list, dim=1)

        # Converting tensor to numpy array and then to DataFrame
        embeddings_df = pd.DataFrame(
            embeddings_concat.detach().numpy(),
            index=dataset.samples,
            columns=[f&#34;E{dim}&#34; for dim in range(embeddings_concat.shape[1])],
        )
        return embeddings_df

    # Adaptor forward function for captum integrated gradients.
    def forward_target(self, *args):
        input_data = list(args[:-2])  # one or more tensors (one per omics layer)
        target_var = args[-2]  # target variable of interest
        steps = args[-1]  # number of steps for IntegratedGradients().attribute
        outputs_list = []
        for i in range(steps):
            # get list of tensors for each step into a list of tensors
            x_step = [input_data[j][i] for j in range(len(input_data))]
            out = self.forward(x_step)
            outputs_list.append(out[target_var])
        return torch.cat(outputs_list, dim=0)

    def compute_feature_importance(self, target_var, steps=5):
        x_list = [self.dataset.dat[x] for x in self.dataset.dat.keys()]

        # Initialize the Integrated Gradients method
        ig = IntegratedGradients(self.forward_target)

        input_data = tuple([data.unsqueeze(0).requires_grad_() for data in x_list])

        # Define a baseline (you might need to adjust this depending on your actual data)
        baseline = tuple([torch.zeros_like(data) for data in input_data])

        # Get the number of classes for the target variable
        if self.dataset.variable_types[target_var] == &#34;numerical&#34;:
            num_class = 1
        else:
            num_class = len(np.unique(self.dataset.ann[target_var]))

        # Compute the feature importance for each class
        attributions = []
        if num_class &gt; 1:
            for target_class in range(num_class):
                attributions.append(
                    ig.attribute(
                        input_data,
                        baseline,
                        additional_forward_args=(target_var, steps),
                        target=target_class,
                        n_steps=steps,
                    )
                )
        else:
            attributions.append(
                ig.attribute(input_data, baseline, additional_forward_args=(target_var, steps), n_steps=steps)
            )

        # summarize feature importances
        # Compute absolute attributions
        abs_attr = [[torch.abs(a) for a in attr_class] for attr_class in attributions]
        # average over samples
        imp = [[a.mean(dim=1) for a in attr_class] for attr_class in abs_attr]

        # combine into a single data frame
        df_list = []
        layers = list(self.dataset.dat.keys())
        for i in range(num_class):
            for j in range(len(layers)):
                features = self.dataset.features[layers[j]]
                importances = imp[i][j][0].detach().numpy()
                df_list.append(
                    pd.DataFrame(
                        {
                            &#34;target_variable&#34;: target_var,
                            &#34;target_class&#34;: i,
                            &#34;layer&#34;: layers[j],
                            &#34;name&#34;: features,
                            &#34;importance&#34;: importances,
                        }
                    )
                )
        df_imp = pd.concat(df_list, ignore_index=True)

        # save the computed scores in the model
        self.feature_importances[target_var] = df_imp</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pytorch_lightning.core.module.LightningModule</li>
<li>lightning_fabric.utilities.device_dtype_mixin._DeviceDtypeModuleMixin</li>
<li>pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin</li>
<li>pytorch_lightning.core.hooks.ModelHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="flexynesis.models.DirectPredCNN.compute_feature_importance"><code class="name flex">
<span>def <span class="ident">compute_feature_importance</span></span>(<span>self, target_var, steps=5)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_feature_importance(self, target_var, steps=5):
    x_list = [self.dataset.dat[x] for x in self.dataset.dat.keys()]

    # Initialize the Integrated Gradients method
    ig = IntegratedGradients(self.forward_target)

    input_data = tuple([data.unsqueeze(0).requires_grad_() for data in x_list])

    # Define a baseline (you might need to adjust this depending on your actual data)
    baseline = tuple([torch.zeros_like(data) for data in input_data])

    # Get the number of classes for the target variable
    if self.dataset.variable_types[target_var] == &#34;numerical&#34;:
        num_class = 1
    else:
        num_class = len(np.unique(self.dataset.ann[target_var]))

    # Compute the feature importance for each class
    attributions = []
    if num_class &gt; 1:
        for target_class in range(num_class):
            attributions.append(
                ig.attribute(
                    input_data,
                    baseline,
                    additional_forward_args=(target_var, steps),
                    target=target_class,
                    n_steps=steps,
                )
            )
    else:
        attributions.append(
            ig.attribute(input_data, baseline, additional_forward_args=(target_var, steps), n_steps=steps)
        )

    # summarize feature importances
    # Compute absolute attributions
    abs_attr = [[torch.abs(a) for a in attr_class] for attr_class in attributions]
    # average over samples
    imp = [[a.mean(dim=1) for a in attr_class] for attr_class in abs_attr]

    # combine into a single data frame
    df_list = []
    layers = list(self.dataset.dat.keys())
    for i in range(num_class):
        for j in range(len(layers)):
            features = self.dataset.features[layers[j]]
            importances = imp[i][j][0].detach().numpy()
            df_list.append(
                pd.DataFrame(
                    {
                        &#34;target_variable&#34;: target_var,
                        &#34;target_class&#34;: i,
                        &#34;layer&#34;: layers[j],
                        &#34;name&#34;: features,
                        &#34;importance&#34;: importances,
                    }
                )
            )
    df_imp = pd.concat(df_list, ignore_index=True)

    # save the computed scores in the model
    self.feature_importances[target_var] = df_imp</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredCNN.compute_loss"><code class="name flex">
<span>def <span class="ident">compute_loss</span></span>(<span>self, var, y, y_hat)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_loss(self, var, y, y_hat):
    if self.dataset.variable_types[var] == &#34;numerical&#34;:
        # Ignore instances with missing labels for numerical variables
        valid_indices = ~torch.isnan(y)
        if valid_indices.sum() &gt; 0:  # only calculate loss if there are valid targets
            y_hat = y_hat[valid_indices]
            y = y[valid_indices]
            loss = F.mse_loss(torch.flatten(y_hat), y.float())
        else:
            loss = torch.tensor(0)  # if no valid labels, set loss to 0
    else:
        # Ignore instances with missing labels for categorical variables
        # Assuming that missing values were encoded as -1
        valid_indices = (y != -1) &amp; (~torch.isnan(y))
        if valid_indices.sum() &gt; 0:  # only calculate loss if there are valid targets
            y_hat = y_hat[valid_indices]
            y = y[valid_indices]
            loss = F.cross_entropy(y_hat, y.long())
        else:
            loss = torch.tensor(0)
    return loss</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredCNN.configure_optimizers"><code class="name flex">
<span>def <span class="ident">configure_optimizers</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one.
But in the case of GANs or similar you might have multiple. Optimization with multiple optimizers only works in
the manual optimization mode.</p>
<h2 id="return">Return</h2>
<p>Any of these 6 options.</p>
<ul>
<li><strong>Single optimizer</strong>.</li>
<li><strong>List or Tuple</strong> of optimizers.</li>
<li><strong>Two lists</strong> - The first list has multiple optimizers, and the second has multiple LR schedulers
(or multiple <code>lr_scheduler_config</code>).</li>
<li><strong>Dictionary</strong>, with an <code>"optimizer"</code> key, and (optionally) a <code>"lr_scheduler"</code>
key whose value is a single LR scheduler or <code>lr_scheduler_config</code>.</li>
<li><strong>None</strong> - Fit will run without any optimizer.</li>
</ul>
<p>The <code>lr_scheduler_config</code> is a dictionary which contains the scheduler and its associated configuration.
The default configuration is shown below.</p>
<p>.. code-block:: python</p>
<pre><code>lr_scheduler_config = {
    # REQUIRED: The scheduler instance
    "scheduler": lr_scheduler,
    # The unit of the scheduler's step size, could also be 'step'.
    # 'epoch' updates the scheduler on epoch end whereas 'step'
    # updates it after a optimizer update.
    "interval": "epoch",
    # How many epochs/steps should pass between calls to
    # &lt;code&gt;scheduler.step()&lt;/code&gt;. 1 corresponds to updating the learning
    # rate after every epoch/step.
    "frequency": 1,
    # Metric to to monitor for schedulers like &lt;code&gt;ReduceLROnPlateau&lt;/code&gt;
    "monitor": "val_loss",
    # If set to &lt;code&gt;True&lt;/code&gt;, will enforce that the value specified 'monitor'
    # is available when the scheduler is updated, thus stopping
    # training if not found. If set to &lt;code&gt;False&lt;/code&gt;, it will only produce a warning
    "strict": True,
    # If using the &lt;code&gt;LearningRateMonitor&lt;/code&gt; callback to monitor the
    # learning rate progress, this keyword can be used to specify
    # a custom logged name
    "name": None,
}
</code></pre>
<p>When there are schedulers in which the <code>.step()</code> method is conditioned on a value, such as the
:class:<code>torch.optim.lr_scheduler.ReduceLROnPlateau</code> scheduler, Lightning requires that the
<code>lr_scheduler_config</code> contains the keyword <code>"monitor"</code> set to the metric name that the scheduler
should be conditioned on.</p>
<div class="admonition testcode">
<p class="admonition-title">Testcode</p>
<h1 id="the-reducelronplateau-scheduler-requires-a-monitor">The ReduceLROnPlateau scheduler requires a monitor</h1>
<p>def configure_optimizers(self):
optimizer = Adam(&hellip;)
return {
"optimizer": optimizer,
"lr_scheduler": {
"scheduler": ReduceLROnPlateau(optimizer, &hellip;),
"monitor": "metric_to_track",
"frequency": "indicates how often the metric is updated"
# If "monitor" references validation metrics, then "frequency" should be set to a
# multiple of "trainer.check_val_every_n_epoch".
},
}</p>
<h1 id="in-the-case-of-two-optimizers-only-one-using-the-reducelronplateau-scheduler">In the case of two optimizers, only one using the ReduceLROnPlateau scheduler</h1>
<p>def configure_optimizers(self):
optimizer1 = Adam(&hellip;)
optimizer2 = SGD(&hellip;)
scheduler1 = ReduceLROnPlateau(optimizer1, &hellip;)
scheduler2 = LambdaLR(optimizer2, &hellip;)
return (
{
"optimizer": optimizer1,
"lr_scheduler": {
"scheduler": scheduler1,
"monitor": "metric_to_track",
},
},
{"optimizer": optimizer2, "lr_scheduler": scheduler2},
)</p>
</div>
<p>Metrics can be made available to monitor by simply logging it using
<code>self.log('metric_to_track', metric_val)</code> in your :class:<code>~pytorch_lightning.core.LightningModule</code>.</p>
<h2 id="note">Note</h2>
<p>Some things to know:</p>
<ul>
<li>Lightning calls <code>.backward()</code> and <code>.step()</code> automatically in case of automatic optimization.</li>
<li>If a learning rate scheduler is specified in <code>configure_optimizers()</code> with key
<code>"interval"</code> (default "epoch") in the scheduler configuration, Lightning will call
the scheduler's <code>.step()</code> method automatically in case of automatic optimization.</li>
<li>If you use 16-bit precision (<code>precision=16</code>), Lightning will automatically handle the optimizer.</li>
<li>If you use :class:<code>torch.optim.LBFGS</code>, Lightning handles the closure function automatically for you.</li>
<li>If you use multiple optimizers, you will have to switch to 'manual optimization' mode and step them
yourself.</li>
<li>If you need to control how often the optimizer steps, override the :meth:<code>optimizer_step</code> hook.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure_optimizers(self):
    optimizer = torch.optim.Adam(self.parameters(), lr=self.config[&#34;lr&#34;])
    return optimizer</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredCNN.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x_list) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Same as :meth:<code>torch.nn.Module.forward</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*args</code></strong></dt>
<dd>Whatever you decide to pass into the forward method.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments are also possible.</dd>
</dl>
<h2 id="return">Return</h2>
<p>Your model's output</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x_list):
    embeddings_list = []
    # Process each input matrix with its corresponding Encoder
    for i, x in enumerate(x_list):
        embeddings_list.append(self.encoders[i](x))
    embeddings_concat = torch.cat(embeddings_list, dim=1)

    outputs = {}
    for var, mlp in self.MLPs.items():
        outputs[var] = mlp(embeddings_concat)
    return outputs</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredCNN.forward_target"><code class="name flex">
<span>def <span class="ident">forward_target</span></span>(<span>self, *args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_target(self, *args):
    input_data = list(args[:-2])  # one or more tensors (one per omics layer)
    target_var = args[-2]  # target variable of interest
    steps = args[-1]  # number of steps for IntegratedGradients().attribute
    outputs_list = []
    for i in range(steps):
        # get list of tensors for each step into a list of tensors
        x_step = [input_data[j][i] for j in range(len(input_data))]
        out = self.forward(x_step)
        outputs_list.append(out[target_var])
    return torch.cat(outputs_list, dim=0)</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredCNN.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, dataset)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, dataset):
    self.eval()
    layers = dataset.dat.keys()
    x_list = [dataset.dat[x] for x in layers]
    outputs = self.forward(x_list)

    predictions = {}
    for var in self.target_variables:
        y_pred = outputs[var].detach().numpy()
        if self.dataset.variable_types[var] == &#34;categorical&#34;:
            predictions[var] = np.argmax(y_pred, axis=1)
        else:
            predictions[var] = y_pred
    return predictions</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredCNN.prepare_data"><code class="name flex">
<span>def <span class="ident">prepare_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Use this to download and prepare data. Downloading and saving data with multiple processes (distributed
settings) will result in corrupted data. Lightning ensures this method is called only within a single process,
so you can safely add your downloading logic within.</p>
<div class="admonition warning">
<p class="admonition-title">Warning:&ensp;DO NOT set state to the model (use <code>setup</code> instead)</p>
<p>since this is NOT called on every device</p>
</div>
<p>Example::</p>
<pre><code>def prepare_data(self):
    # good
    download_data()
    tokenize()
    etc()

    # bad
    self.split = data_split
    self.some_state = some_other_state()
</code></pre>
<p>In a distributed environment, <code>prepare_data</code> can be called in two ways
(using :ref:<code>prepare_data_per_node&lt;common/lightning_module:prepare_data_per_node&gt;</code>)</p>
<ol>
<li>Once per node. This is the default and is only called on LOCAL_RANK=0.</li>
<li>Once in total. Only called on GLOBAL_RANK=0.</li>
</ol>
<p>Example::</p>
<pre><code># DEFAULT
# called once per node on LOCAL_RANK=0 of that node
class LitDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()
        self.prepare_data_per_node = True


# call on GLOBAL_RANK=0 (great for shared file systems)
class LitDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()
        self.prepare_data_per_node = False
</code></pre>
<p>This is called before requesting the dataloaders:</p>
<p>.. code-block:: python</p>
<pre><code>model.prepare_data()
initialize_distributed()
model.setup(stage)
model.train_dataloader()
model.val_dataloader()
model.test_dataloader()
model.predict_dataloader()
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_data(self):
    lt = int(len(self.dataset) * (1 - self.val_size))
    lv = len(self.dataset) - lt
    dat_train, dat_val = random_split(self.dataset, [lt, lv], generator=torch.Generator().manual_seed(42))
    return dat_train, dat_val</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredCNN.train_dataloader"><code class="name flex">
<span>def <span class="ident">train_dataloader</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>An iterable or collection of iterables specifying training samples.</p>
<p>For more information about multiple dataloaders, see this :ref:<code>section &lt;multiple-dataloaders&gt;</code>.</p>
<p>The dataloader you return will not be reloaded unless you set
:paramref:<code>~pytorch_lightning.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs</code> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<pre><code>- download in :meth:&lt;code&gt;prepare\_data&lt;/code&gt;
- process and split in :meth:&lt;code&gt;setup&lt;/code&gt;
</code></pre>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning:&ensp;do not assign state in prepare_data</p>
</div>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.trainer.Trainer.fit</code></li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_dataloader(self):
    return DataLoader(
        self.dat_train,
        batch_size=int(self.config[&#34;batch_size&#34;]),
        num_workers=0,
        pin_memory=True,
        shuffle=True,
        drop_last=True,
    )</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredCNN.training_step"><code class="name flex">
<span>def <span class="ident">training_step</span></span>(<span>self, train_batch, batch_idx)</span>
</code></dt>
<dd>
<div class="desc"><p>Here you compute and return the training loss and some additional metrics for e.g. the progress bar or
logger.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>The output of your data iterable, normally a :class:<code>~torch.utils.data.DataLoader</code>.</dd>
<dt><strong><code>batch_idx</code></strong></dt>
<dd>The index of this batch.</dd>
<dt><strong><code>dataloader_idx</code></strong></dt>
<dd>The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</dd>
</dl>
<h2 id="return">Return</h2>
<ul>
<li>:class:<code>~torch.Tensor</code> - The loss tensor</li>
<li><code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code>.</li>
<li><code>None</code> - Skip to the next batch. This is only supported for automatic optimization.
This is not supported for multi-GPU, TPU, IPU, or DeepSpeed.</li>
</ul>
<p>In this step you'd normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<p>Example::</p>
<pre><code>def training_step(self, batch, batch_idx):
    x, y, z = batch
    out = self.encoder(x)
    loss = self.loss(out, x)
    return loss
</code></pre>
<p>To use multiple optimizers, you can switch to 'manual optimization' and control their stepping:</p>
<p>.. code-block:: python</p>
<pre><code>def __init__(self):
    super().__init__()
    self.automatic_optimization = False


# Multiple optimizers (e.g.: GANs)
def training_step(self, batch, batch_idx):
    opt1, opt2 = self.optimizers()

    # do training_step with encoder
    ...
    opt1.step()
    # do training_step with decoder
    ...
    opt2.step()
</code></pre>
<h2 id="note">Note</h2>
<p>When <code>accumulate_grad_batches</code> &gt; 1, the loss returned here will be automatically
normalized by <code>accumulate_grad_batches</code> internally.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def training_step(self, train_batch, batch_idx):
    dat, y_dict = train_batch
    layers = dat.keys()
    x_list = [dat[x] for x in layers]
    outputs = self.forward(x_list)
    losses = {}
    for var in self.target_variables:
        y_hat = outputs[var]
        y = y_dict[var]
        loss = self.compute_loss(var, y, y_hat)
        losses[var] = loss
    total_loss = sum(losses.values())
    losses[&#34;train_loss&#34;] = total_loss
    self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True)
    return total_loss</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredCNN.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, dataset)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, dataset):
    self.eval()
    embeddings_list = []
    # Process each input matrix with its corresponding Encoder
    for i, x in enumerate(dataset.dat.values()):
        embeddings_list.append(self.encoders[i](x))
    embeddings_concat = torch.cat(embeddings_list, dim=1)

    # Converting tensor to numpy array and then to DataFrame
    embeddings_df = pd.DataFrame(
        embeddings_concat.detach().numpy(),
        index=dataset.samples,
        columns=[f&#34;E{dim}&#34; for dim in range(embeddings_concat.shape[1])],
    )
    return embeddings_df</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredCNN.val_dataloader"><code class="name flex">
<span>def <span class="ident">val_dataloader</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>An iterable or collection of iterables specifying validation samples.</p>
<p>For more information about multiple dataloaders, see this :ref:<code>section &lt;multiple-dataloaders&gt;</code>.</p>
<p>The dataloader you return will not be reloaded unless you set
:paramref:<code>~pytorch_lightning.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs</code> to
a positive integer.</p>
<p>It's recommended that all data downloads and preparation happen in :meth:<code>prepare_data</code>.</p>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.trainer.Trainer.fit</code></li>
<li>:meth:<code>~pytorch_lightning.trainer.trainer.Trainer.validate</code></li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
<h2 id="note_1">Note</h2>
<p>If you don't need a validation dataset and a :meth:<code>validation_step</code>, you don't need to
implement this method.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def val_dataloader(self):
    return DataLoader(
        self.dat_val, batch_size=int(self.config[&#34;batch_size&#34;]), num_workers=0, pin_memory=True, shuffle=False
    )</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredCNN.validation_step"><code class="name flex">
<span>def <span class="ident">validation_step</span></span>(<span>self, val_batch, batch_idx)</span>
</code></dt>
<dd>
<div class="desc"><p>Operates on a single batch of data from the validation set. In this step you'd might generate examples or
calculate anything of interest like accuracy.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>The output of your data iterable, normally a :class:<code>~torch.utils.data.DataLoader</code>.</dd>
<dt><strong><code>batch_idx</code></strong></dt>
<dd>The index of this batch.</dd>
<dt><strong><code>dataloader_idx</code></strong></dt>
<dd>The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</dd>
</dl>
<h2 id="return">Return</h2>
<ul>
<li>:class:<code>~torch.Tensor</code> - The loss tensor</li>
<li><code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code>.</li>
<li><code>None</code> - Skip to the next batch.</li>
</ul>
<p>.. code-block:: python</p>
<pre><code># if you have one val dataloader:
def validation_step(self, batch, batch_idx):
    ...


# if you have multiple val dataloaders:
def validation_step(self, batch, batch_idx, dataloader_idx=0):
    ...
</code></pre>
<p>Examples::</p>
<pre><code># CASE 1: A single validation dataset
def validation_step(self, batch, batch_idx):
    x, y = batch

    # implement your own
    out = self(x)
    loss = self.loss(out, y)

    # log 6 example images
    # or generated text... or whatever
    sample_imgs = x[:6]
    grid = torchvision.utils.make_grid(sample_imgs)
    self.logger.experiment.add_image('example_images', grid, 0)

    # calculate acc
    labels_hat = torch.argmax(out, dim=1)
    val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)

    # log the outputs!
    self.log_dict({'val_loss': loss, 'val_acc': val_acc})
</code></pre>
<p>If you pass in multiple val dataloaders, :meth:<code>validation_step</code> will have an additional argument. We recommend
setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>
<p>.. code-block:: python</p>
<pre><code># CASE 2: multiple validation dataloaders
def validation_step(self, batch, batch_idx, dataloader_idx=0):
    # dataloader_idx tells you which dataset this is.
    ...
</code></pre>
<h2 id="note">Note</h2>
<p>If you don't need to validate you don't need to implement this method.</p>
<h2 id="note_1">Note</h2>
<p>When the :meth:<code>validation_step</code> is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validation_step(self, val_batch, batch_idx):
    dat, y_dict = val_batch
    layers = dat.keys()
    x_list = [dat[x] for x in layers]
    outputs = self.forward(x_list)
    losses = {}
    for var in self.target_variables:
        y_hat = outputs[var]
        y = y_dict[var]
        loss = self.compute_loss(var, y, y_hat)
        losses[var] = loss
    total_loss = sum(losses.values())
    losses[&#34;val_loss&#34;] = total_loss
    self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True)
    return total_loss</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="flexynesis.models.DirectPredGCNN"><code class="flex name class">
<span>class <span class="ident">DirectPredGCNN</span></span>
<span>(</span><span>config, dataset, target_variables, batch_variables=None, val_size=0.2, use_loss_weighting=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Hooks to be used in LightningModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DirectPredGCNN(pl.LightningModule):
    def __init__(self, config, dataset, target_variables, batch_variables=None, val_size=0.2, use_loss_weighting=True):
        super().__init__()
        self.config = config
        self.dataset = dataset
        self.target_variables = target_variables
        self.batch_variables = batch_variables
        self.variables = target_variables + batch_variables if batch_variables else target_variables
        self.val_size = val_size
        self.dat_train, self.dat_val = self.prepare_data()
        self.feature_importances = {}
        self.use_loss_weighting = use_loss_weighting

        if self.use_loss_weighting:
            # Initialize log variance parameters for uncertainty weighting
            self.log_vars = nn.ParameterDict()
            for var in self.variables:
                self.log_vars[var] = nn.Parameter(torch.zeros(1))

        # Init modality encoders
        layers = list(self.dataset.dat.keys())
        # NOTE: For now we use matrices, so number of node input features is 1.
        input_dims = [1 for i in range(len(layers))]

        self.encoders = nn.ModuleList([
            GCNN(
                input_dim=input_dims[i],
                hidden_dim=int(self.config[&#34;hidden_dim&#34;]),  # int because of pyg
                output_dim=self.config[&#34;latent_dim&#34;],
            )
            for i in range(len(layers))
        ])

        # Init output layers
        self.MLPs = nn.ModuleDict()
        for var in self.target_variables:
            if self.dataset.variable_types[var] == &#34;numerical&#34;:
                num_class = 1
            else:
                num_class = len(np.unique(self.dataset.ann[var]))
            self.MLPs[var] = MLP(
                input_dim=self.config[&#34;latent_dim&#34;] * len(layers),
                hidden_dim=self.config[&#34;hidden_dim&#34;],
                output_dim=num_class,
            )

    def forward(self, x_list):
        embeddings_list = []
        # Process each input matrix with its corresponding Encoder
        for i, x in enumerate(x_list):
            embeddings_list.append(self.encoders[i](x.x, x.edge_index, x.batch))
        embeddings_concat = torch.cat(embeddings_list, dim=1)

        outputs = {}
        for var, mlp in self.MLPs.items():
            outputs[var] = mlp(embeddings_concat)
        return outputs  

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.config[&#34;lr&#34;])
        return optimizer

    def compute_loss(self, var, y, y_hat):
        if self.dataset.variable_types[var] == &#34;numerical&#34;:
            # Ignore instances with missing labels for numerical variables
            valid_indices = ~torch.isnan(y)
            if valid_indices.sum() &gt; 0:  # only calculate loss if there are valid targets
                y_hat = y_hat[valid_indices]
                y = y[valid_indices]
                loss = F.mse_loss(torch.flatten(y_hat), y.float())
            else:
                loss = torch.tensor(0.0, device=y_hat.device, requires_grad=True) # if no valid labels, set loss to 0
        else:
            # Ignore instances with missing labels for categorical variables
            # Assuming that missing values were encoded as -1
            valid_indices = (y != -1) &amp; (~torch.isnan(y))
            if valid_indices.sum() &gt; 0:  # only calculate loss if there are valid targets
                y_hat = y_hat[valid_indices]
                y = y[valid_indices]
                loss = F.cross_entropy(y_hat, y.long())
            else:
                loss = torch.tensor(0.0, device=y_hat.device, requires_grad=True)
        return loss

    def compute_total_loss(self, losses):
        if self.use_loss_weighting and len(losses) &gt; 1:
            # Compute weighted loss for each loss 
            # Weighted loss = precision * loss + log-variance
            total_loss = sum(torch.exp(-self.log_vars[name]) * loss + self.log_vars[name] for name, loss in losses.items())
        else:
            # Compute unweighted total loss
            total_loss = sum(losses.values())
        return total_loss

    def training_step(self, train_batch, batch_idx):
        dat, y_dict = train_batch
        layers = dat.keys()
        x_list = [dat[x] for x in layers]

        outputs = self.forward(x_list)

        losses = {}
        for var in self.variables:
            y_hat = outputs[var]
            y = y_dict[var]
            loss = self.compute_loss(var, y, y_hat)
            losses[var] = loss

        total_loss = self.compute_total_loss(losses)
        losses[&#34;train_loss&#34;] = total_loss
        self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True, batch_size=int(x_list[0].batch_size))
        return total_loss

    def validation_step(self, val_batch, batch_idx):
        dat, y_dict = val_batch
        layers = dat.keys()
        x_list = [dat[x] for x in layers]

        outputs = self.forward(x_list)

        losses = {}
        for var in self.variables:
            y_hat = outputs[var]
            y = y_dict[var]
            loss = self.compute_loss(var, y, y_hat)
            losses[var] = loss

        total_loss = sum(losses.values())
        losses[&#34;val_loss&#34;] = total_loss
        self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True, batch_size=int(x_list[0].batch_size))
        return total_loss

    def prepare_data(self):
        lt = int(len(self.dataset) * (1 - self.val_size))
        lv = len(self.dataset) - lt
        dat_train, dat_val = random_split(self.dataset, [lt, lv], generator=torch.Generator().manual_seed(42))
        return dat_train, dat_val

    def train_dataloader(self):
        return DataLoader(self.dat_train, batch_size=int(self.config[&#34;batch_size&#34;]), num_workers=0, pin_memory=True, shuffle=True, drop_last=True)

    def val_dataloader(self):
        return DataLoader(self.dat_val, batch_size=int(self.config[&#34;batch_size&#34;]), num_workers=0, pin_memory=True, shuffle=False)

    def predict(self, dataset):
        self.eval()
        layers = dataset.dat.keys()
        x_list = [dataset.dat[x] for x in layers]
        outputs = self.forward(x_list)

        predictions = {}
        for var in self.target_variables:
            y_pred = outputs[var].detach().numpy()
            if self.dataset.variable_types[var] == &#34;categorical&#34;:
                predictions[var] = np.argmax(y_pred, axis=1)
            else:
                predictions[var] = y_pred
        return predictions

    def transform(self, dataset):
        self.eval()
        embeddings_list = []
        # Process each input matrix with its corresponding Encoder
        for i, x in enumerate(dataset.dat.values()):
            embeddings_list.append(self.encoders[i](x))
        embeddings_concat = torch.cat(embeddings_list, dim=1)

        # Converting tensor to numpy array and then to DataFrame
        embeddings_df = pd.DataFrame(
            embeddings_concat.detach().numpy(),
            index=dataset.samples,
            columns=[f&#34;E{dim}&#34; for dim in range(embeddings_concat.shape[1])],
        )
        return embeddings_df

    def forward_target(self, *args):
        &#34;&#34;&#34;Adaptor forward function for captum integrated gradients.
        &#34;&#34;&#34;
        raise NotImplementedError

    def compute_feature_importance(self, target_var, steps=5):
        raise NotImplementedError</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pytorch_lightning.core.module.LightningModule</li>
<li>lightning_fabric.utilities.device_dtype_mixin._DeviceDtypeModuleMixin</li>
<li>pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin</li>
<li>pytorch_lightning.core.hooks.ModelHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="flexynesis.models.DirectPredGCNN.compute_feature_importance"><code class="name flex">
<span>def <span class="ident">compute_feature_importance</span></span>(<span>self, target_var, steps=5)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_feature_importance(self, target_var, steps=5):
    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredGCNN.compute_loss"><code class="name flex">
<span>def <span class="ident">compute_loss</span></span>(<span>self, var, y, y_hat)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_loss(self, var, y, y_hat):
    if self.dataset.variable_types[var] == &#34;numerical&#34;:
        # Ignore instances with missing labels for numerical variables
        valid_indices = ~torch.isnan(y)
        if valid_indices.sum() &gt; 0:  # only calculate loss if there are valid targets
            y_hat = y_hat[valid_indices]
            y = y[valid_indices]
            loss = F.mse_loss(torch.flatten(y_hat), y.float())
        else:
            loss = torch.tensor(0.0, device=y_hat.device, requires_grad=True) # if no valid labels, set loss to 0
    else:
        # Ignore instances with missing labels for categorical variables
        # Assuming that missing values were encoded as -1
        valid_indices = (y != -1) &amp; (~torch.isnan(y))
        if valid_indices.sum() &gt; 0:  # only calculate loss if there are valid targets
            y_hat = y_hat[valid_indices]
            y = y[valid_indices]
            loss = F.cross_entropy(y_hat, y.long())
        else:
            loss = torch.tensor(0.0, device=y_hat.device, requires_grad=True)
    return loss</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredGCNN.compute_total_loss"><code class="name flex">
<span>def <span class="ident">compute_total_loss</span></span>(<span>self, losses)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_total_loss(self, losses):
    if self.use_loss_weighting and len(losses) &gt; 1:
        # Compute weighted loss for each loss 
        # Weighted loss = precision * loss + log-variance
        total_loss = sum(torch.exp(-self.log_vars[name]) * loss + self.log_vars[name] for name, loss in losses.items())
    else:
        # Compute unweighted total loss
        total_loss = sum(losses.values())
    return total_loss</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredGCNN.configure_optimizers"><code class="name flex">
<span>def <span class="ident">configure_optimizers</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one.
But in the case of GANs or similar you might have multiple. Optimization with multiple optimizers only works in
the manual optimization mode.</p>
<h2 id="return">Return</h2>
<p>Any of these 6 options.</p>
<ul>
<li><strong>Single optimizer</strong>.</li>
<li><strong>List or Tuple</strong> of optimizers.</li>
<li><strong>Two lists</strong> - The first list has multiple optimizers, and the second has multiple LR schedulers
(or multiple <code>lr_scheduler_config</code>).</li>
<li><strong>Dictionary</strong>, with an <code>"optimizer"</code> key, and (optionally) a <code>"lr_scheduler"</code>
key whose value is a single LR scheduler or <code>lr_scheduler_config</code>.</li>
<li><strong>None</strong> - Fit will run without any optimizer.</li>
</ul>
<p>The <code>lr_scheduler_config</code> is a dictionary which contains the scheduler and its associated configuration.
The default configuration is shown below.</p>
<p>.. code-block:: python</p>
<pre><code>lr_scheduler_config = {
    # REQUIRED: The scheduler instance
    "scheduler": lr_scheduler,
    # The unit of the scheduler's step size, could also be 'step'.
    # 'epoch' updates the scheduler on epoch end whereas 'step'
    # updates it after a optimizer update.
    "interval": "epoch",
    # How many epochs/steps should pass between calls to
    # &lt;code&gt;scheduler.step()&lt;/code&gt;. 1 corresponds to updating the learning
    # rate after every epoch/step.
    "frequency": 1,
    # Metric to to monitor for schedulers like &lt;code&gt;ReduceLROnPlateau&lt;/code&gt;
    "monitor": "val_loss",
    # If set to &lt;code&gt;True&lt;/code&gt;, will enforce that the value specified 'monitor'
    # is available when the scheduler is updated, thus stopping
    # training if not found. If set to &lt;code&gt;False&lt;/code&gt;, it will only produce a warning
    "strict": True,
    # If using the &lt;code&gt;LearningRateMonitor&lt;/code&gt; callback to monitor the
    # learning rate progress, this keyword can be used to specify
    # a custom logged name
    "name": None,
}
</code></pre>
<p>When there are schedulers in which the <code>.step()</code> method is conditioned on a value, such as the
:class:<code>torch.optim.lr_scheduler.ReduceLROnPlateau</code> scheduler, Lightning requires that the
<code>lr_scheduler_config</code> contains the keyword <code>"monitor"</code> set to the metric name that the scheduler
should be conditioned on.</p>
<div class="admonition testcode">
<p class="admonition-title">Testcode</p>
<h1 id="the-reducelronplateau-scheduler-requires-a-monitor">The ReduceLROnPlateau scheduler requires a monitor</h1>
<p>def configure_optimizers(self):
optimizer = Adam(&hellip;)
return {
"optimizer": optimizer,
"lr_scheduler": {
"scheduler": ReduceLROnPlateau(optimizer, &hellip;),
"monitor": "metric_to_track",
"frequency": "indicates how often the metric is updated"
# If "monitor" references validation metrics, then "frequency" should be set to a
# multiple of "trainer.check_val_every_n_epoch".
},
}</p>
<h1 id="in-the-case-of-two-optimizers-only-one-using-the-reducelronplateau-scheduler">In the case of two optimizers, only one using the ReduceLROnPlateau scheduler</h1>
<p>def configure_optimizers(self):
optimizer1 = Adam(&hellip;)
optimizer2 = SGD(&hellip;)
scheduler1 = ReduceLROnPlateau(optimizer1, &hellip;)
scheduler2 = LambdaLR(optimizer2, &hellip;)
return (
{
"optimizer": optimizer1,
"lr_scheduler": {
"scheduler": scheduler1,
"monitor": "metric_to_track",
},
},
{"optimizer": optimizer2, "lr_scheduler": scheduler2},
)</p>
</div>
<p>Metrics can be made available to monitor by simply logging it using
<code>self.log('metric_to_track', metric_val)</code> in your :class:<code>~pytorch_lightning.core.LightningModule</code>.</p>
<h2 id="note">Note</h2>
<p>Some things to know:</p>
<ul>
<li>Lightning calls <code>.backward()</code> and <code>.step()</code> automatically in case of automatic optimization.</li>
<li>If a learning rate scheduler is specified in <code>configure_optimizers()</code> with key
<code>"interval"</code> (default "epoch") in the scheduler configuration, Lightning will call
the scheduler's <code>.step()</code> method automatically in case of automatic optimization.</li>
<li>If you use 16-bit precision (<code>precision=16</code>), Lightning will automatically handle the optimizer.</li>
<li>If you use :class:<code>torch.optim.LBFGS</code>, Lightning handles the closure function automatically for you.</li>
<li>If you use multiple optimizers, you will have to switch to 'manual optimization' mode and step them
yourself.</li>
<li>If you need to control how often the optimizer steps, override the :meth:<code>optimizer_step</code> hook.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure_optimizers(self):
    optimizer = torch.optim.Adam(self.parameters(), lr=self.config[&#34;lr&#34;])
    return optimizer</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredGCNN.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x_list) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Same as :meth:<code>torch.nn.Module.forward</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*args</code></strong></dt>
<dd>Whatever you decide to pass into the forward method.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments are also possible.</dd>
</dl>
<h2 id="return">Return</h2>
<p>Your model's output</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x_list):
    embeddings_list = []
    # Process each input matrix with its corresponding Encoder
    for i, x in enumerate(x_list):
        embeddings_list.append(self.encoders[i](x.x, x.edge_index, x.batch))
    embeddings_concat = torch.cat(embeddings_list, dim=1)

    outputs = {}
    for var, mlp in self.MLPs.items():
        outputs[var] = mlp(embeddings_concat)
    return outputs  </code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredGCNN.forward_target"><code class="name flex">
<span>def <span class="ident">forward_target</span></span>(<span>self, *args)</span>
</code></dt>
<dd>
<div class="desc"><p>Adaptor forward function for captum integrated gradients.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_target(self, *args):
    &#34;&#34;&#34;Adaptor forward function for captum integrated gradients.
    &#34;&#34;&#34;
    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredGCNN.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, dataset)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, dataset):
    self.eval()
    layers = dataset.dat.keys()
    x_list = [dataset.dat[x] for x in layers]
    outputs = self.forward(x_list)

    predictions = {}
    for var in self.target_variables:
        y_pred = outputs[var].detach().numpy()
        if self.dataset.variable_types[var] == &#34;categorical&#34;:
            predictions[var] = np.argmax(y_pred, axis=1)
        else:
            predictions[var] = y_pred
    return predictions</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredGCNN.prepare_data"><code class="name flex">
<span>def <span class="ident">prepare_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Use this to download and prepare data. Downloading and saving data with multiple processes (distributed
settings) will result in corrupted data. Lightning ensures this method is called only within a single process,
so you can safely add your downloading logic within.</p>
<div class="admonition warning">
<p class="admonition-title">Warning:&ensp;DO NOT set state to the model (use <code>setup</code> instead)</p>
<p>since this is NOT called on every device</p>
</div>
<p>Example::</p>
<pre><code>def prepare_data(self):
    # good
    download_data()
    tokenize()
    etc()

    # bad
    self.split = data_split
    self.some_state = some_other_state()
</code></pre>
<p>In a distributed environment, <code>prepare_data</code> can be called in two ways
(using :ref:<code>prepare_data_per_node&lt;common/lightning_module:prepare_data_per_node&gt;</code>)</p>
<ol>
<li>Once per node. This is the default and is only called on LOCAL_RANK=0.</li>
<li>Once in total. Only called on GLOBAL_RANK=0.</li>
</ol>
<p>Example::</p>
<pre><code># DEFAULT
# called once per node on LOCAL_RANK=0 of that node
class LitDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()
        self.prepare_data_per_node = True


# call on GLOBAL_RANK=0 (great for shared file systems)
class LitDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()
        self.prepare_data_per_node = False
</code></pre>
<p>This is called before requesting the dataloaders:</p>
<p>.. code-block:: python</p>
<pre><code>model.prepare_data()
initialize_distributed()
model.setup(stage)
model.train_dataloader()
model.val_dataloader()
model.test_dataloader()
model.predict_dataloader()
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_data(self):
    lt = int(len(self.dataset) * (1 - self.val_size))
    lv = len(self.dataset) - lt
    dat_train, dat_val = random_split(self.dataset, [lt, lv], generator=torch.Generator().manual_seed(42))
    return dat_train, dat_val</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredGCNN.train_dataloader"><code class="name flex">
<span>def <span class="ident">train_dataloader</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>An iterable or collection of iterables specifying training samples.</p>
<p>For more information about multiple dataloaders, see this :ref:<code>section &lt;multiple-dataloaders&gt;</code>.</p>
<p>The dataloader you return will not be reloaded unless you set
:paramref:<code>~pytorch_lightning.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs</code> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<pre><code>- download in :meth:&lt;code&gt;prepare\_data&lt;/code&gt;
- process and split in :meth:&lt;code&gt;setup&lt;/code&gt;
</code></pre>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning:&ensp;do not assign state in prepare_data</p>
</div>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.trainer.Trainer.fit</code></li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_dataloader(self):
    return DataLoader(self.dat_train, batch_size=int(self.config[&#34;batch_size&#34;]), num_workers=0, pin_memory=True, shuffle=True, drop_last=True)</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredGCNN.training_step"><code class="name flex">
<span>def <span class="ident">training_step</span></span>(<span>self, train_batch, batch_idx)</span>
</code></dt>
<dd>
<div class="desc"><p>Here you compute and return the training loss and some additional metrics for e.g. the progress bar or
logger.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>The output of your data iterable, normally a :class:<code>~torch.utils.data.DataLoader</code>.</dd>
<dt><strong><code>batch_idx</code></strong></dt>
<dd>The index of this batch.</dd>
<dt><strong><code>dataloader_idx</code></strong></dt>
<dd>The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</dd>
</dl>
<h2 id="return">Return</h2>
<ul>
<li>:class:<code>~torch.Tensor</code> - The loss tensor</li>
<li><code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code>.</li>
<li><code>None</code> - Skip to the next batch. This is only supported for automatic optimization.
This is not supported for multi-GPU, TPU, IPU, or DeepSpeed.</li>
</ul>
<p>In this step you'd normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<p>Example::</p>
<pre><code>def training_step(self, batch, batch_idx):
    x, y, z = batch
    out = self.encoder(x)
    loss = self.loss(out, x)
    return loss
</code></pre>
<p>To use multiple optimizers, you can switch to 'manual optimization' and control their stepping:</p>
<p>.. code-block:: python</p>
<pre><code>def __init__(self):
    super().__init__()
    self.automatic_optimization = False


# Multiple optimizers (e.g.: GANs)
def training_step(self, batch, batch_idx):
    opt1, opt2 = self.optimizers()

    # do training_step with encoder
    ...
    opt1.step()
    # do training_step with decoder
    ...
    opt2.step()
</code></pre>
<h2 id="note">Note</h2>
<p>When <code>accumulate_grad_batches</code> &gt; 1, the loss returned here will be automatically
normalized by <code>accumulate_grad_batches</code> internally.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def training_step(self, train_batch, batch_idx):
    dat, y_dict = train_batch
    layers = dat.keys()
    x_list = [dat[x] for x in layers]

    outputs = self.forward(x_list)

    losses = {}
    for var in self.variables:
        y_hat = outputs[var]
        y = y_dict[var]
        loss = self.compute_loss(var, y, y_hat)
        losses[var] = loss

    total_loss = self.compute_total_loss(losses)
    losses[&#34;train_loss&#34;] = total_loss
    self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True, batch_size=int(x_list[0].batch_size))
    return total_loss</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredGCNN.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, dataset)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, dataset):
    self.eval()
    embeddings_list = []
    # Process each input matrix with its corresponding Encoder
    for i, x in enumerate(dataset.dat.values()):
        embeddings_list.append(self.encoders[i](x))
    embeddings_concat = torch.cat(embeddings_list, dim=1)

    # Converting tensor to numpy array and then to DataFrame
    embeddings_df = pd.DataFrame(
        embeddings_concat.detach().numpy(),
        index=dataset.samples,
        columns=[f&#34;E{dim}&#34; for dim in range(embeddings_concat.shape[1])],
    )
    return embeddings_df</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredGCNN.val_dataloader"><code class="name flex">
<span>def <span class="ident">val_dataloader</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>An iterable or collection of iterables specifying validation samples.</p>
<p>For more information about multiple dataloaders, see this :ref:<code>section &lt;multiple-dataloaders&gt;</code>.</p>
<p>The dataloader you return will not be reloaded unless you set
:paramref:<code>~pytorch_lightning.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs</code> to
a positive integer.</p>
<p>It's recommended that all data downloads and preparation happen in :meth:<code>prepare_data</code>.</p>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.trainer.Trainer.fit</code></li>
<li>:meth:<code>~pytorch_lightning.trainer.trainer.Trainer.validate</code></li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
<h2 id="note_1">Note</h2>
<p>If you don't need a validation dataset and a :meth:<code>validation_step</code>, you don't need to
implement this method.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def val_dataloader(self):
    return DataLoader(self.dat_val, batch_size=int(self.config[&#34;batch_size&#34;]), num_workers=0, pin_memory=True, shuffle=False)</code></pre>
</details>
</dd>
<dt id="flexynesis.models.DirectPredGCNN.validation_step"><code class="name flex">
<span>def <span class="ident">validation_step</span></span>(<span>self, val_batch, batch_idx)</span>
</code></dt>
<dd>
<div class="desc"><p>Operates on a single batch of data from the validation set. In this step you'd might generate examples or
calculate anything of interest like accuracy.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>The output of your data iterable, normally a :class:<code>~torch.utils.data.DataLoader</code>.</dd>
<dt><strong><code>batch_idx</code></strong></dt>
<dd>The index of this batch.</dd>
<dt><strong><code>dataloader_idx</code></strong></dt>
<dd>The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</dd>
</dl>
<h2 id="return">Return</h2>
<ul>
<li>:class:<code>~torch.Tensor</code> - The loss tensor</li>
<li><code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code>.</li>
<li><code>None</code> - Skip to the next batch.</li>
</ul>
<p>.. code-block:: python</p>
<pre><code># if you have one val dataloader:
def validation_step(self, batch, batch_idx):
    ...


# if you have multiple val dataloaders:
def validation_step(self, batch, batch_idx, dataloader_idx=0):
    ...
</code></pre>
<p>Examples::</p>
<pre><code># CASE 1: A single validation dataset
def validation_step(self, batch, batch_idx):
    x, y = batch

    # implement your own
    out = self(x)
    loss = self.loss(out, y)

    # log 6 example images
    # or generated text... or whatever
    sample_imgs = x[:6]
    grid = torchvision.utils.make_grid(sample_imgs)
    self.logger.experiment.add_image('example_images', grid, 0)

    # calculate acc
    labels_hat = torch.argmax(out, dim=1)
    val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)

    # log the outputs!
    self.log_dict({'val_loss': loss, 'val_acc': val_acc})
</code></pre>
<p>If you pass in multiple val dataloaders, :meth:<code>validation_step</code> will have an additional argument. We recommend
setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>
<p>.. code-block:: python</p>
<pre><code># CASE 2: multiple validation dataloaders
def validation_step(self, batch, batch_idx, dataloader_idx=0):
    # dataloader_idx tells you which dataset this is.
    ...
</code></pre>
<h2 id="note">Note</h2>
<p>If you don't need to validate you don't need to implement this method.</p>
<h2 id="note_1">Note</h2>
<p>When the :meth:<code>validation_step</code> is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validation_step(self, val_batch, batch_idx):
    dat, y_dict = val_batch
    layers = dat.keys()
    x_list = [dat[x] for x in layers]

    outputs = self.forward(x_list)

    losses = {}
    for var in self.variables:
        y_hat = outputs[var]
        y = y_dict[var]
        loss = self.compute_loss(var, y, y_hat)
        losses[var] = loss

    total_loss = sum(losses.values())
    losses[&#34;val_loss&#34;] = total_loss
    self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True, batch_size=int(x_list[0].batch_size))
    return total_loss</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="flexynesis.models.MultiTripletNetwork"><code class="flex name class">
<span>class <span class="ident">MultiTripletNetwork</span></span>
<span>(</span><span>config, dataset, target_variables, batch_variables=None, val_size=0.2, use_loss_weighting=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the MultiTripletNetwork with the given parameters.</p>
<h2 id="args">Args</h2>
<p>TODO</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiTripletNetwork(pl.LightningModule):
    &#34;&#34;&#34;
    &#34;&#34;&#34;
    def __init__(self, config, dataset, target_variables, batch_variables = None, val_size = 0.2, use_loss_weighting = True):
        &#34;&#34;&#34;
        Initialize the MultiTripletNetwork with the given parameters.

        Args:
            TODO
        &#34;&#34;&#34;
        super(MultiTripletNetwork, self).__init__()
        
        self.config = config
        self.target_variables = target_variables
        self.batch_variables = batch_variables
        self.variables = target_variables + batch_variables if batch_variables else target_variables
        self.val_size = val_size
        self.dataset = dataset
        self.ann = self.dataset.ann
        self.variable_types = self.dataset.variable_types
        self.feature_importances = {}
        
        layers = list(dataset.dat.keys())
        input_sizes = [len(dataset.features[layers[i]]) for i in range(len(layers))]
        hidden_sizes = [config[&#39;hidden_dim&#39;] for x in range(len(layers))]
        
        
        # The first target variable is the main variable that dictates the triplets 
        # it has to be a categorical variable 
        main_var = self.target_variables[0] 
        if self.dataset.variable_types[main_var] == &#39;numerical&#39;:
            raise ValueError(&#34;The first target variable&#34;,main_var,&#34; must be a categorical variable&#34;)
        
        self.use_loss_weighting = use_loss_weighting
        
        if self.use_loss_weighting:
            # Initialize log variance parameters for uncertainty weighting
            self.log_vars = nn.ParameterDict()
            for loss_type in itertools.chain(self.variables, [&#39;triplet_loss&#39;]):
                self.log_vars[loss_type] = nn.Parameter(torch.zeros(1))
        
        
        # create train/validation splits and convert TripletMultiOmicDataset format
        self.dataset = TripletMultiOmicDataset(self.dataset, main_var)
        self.dat_train, self.dat_val = self.prepare_data() 
        
        # define embedding network for data matrices 
        self.multi_embedding_network = MultiEmbeddingNetwork(input_sizes, hidden_sizes, config[&#39;latent_dim&#39;])

        # define supervisor heads for both target and batch variables 
        self.MLPs = nn.ModuleDict() # using ModuleDict to store multiple MLPs
        for var in self.variables:
            if self.variable_types[var] == &#39;numerical&#39;:
                num_class = 1
            else:
                num_class = len(np.unique(self.ann[var]))
            self.MLPs[var] = MLP(input_dim=self.config[&#39;latent_dim&#39;] * len(layers),
                                 hidden_dim=self.config[&#39;supervisor_hidden_dim&#39;],
                                 output_dim=num_class)
                                                                              
    def forward(self, anchor, positive, negative):
        &#34;&#34;&#34;
        Compute the forward pass of the MultiTripletNetwork and return the embeddings and predictions.

        Args:
            anchor (dict): A dictionary containing the anchor input tensors for each EmbeddingNetwork.
            positive (dict): A dictionary containing the positive input tensors for each EmbeddingNetwork.
            negative (dict): A dictionary containing the negative input tensors for each EmbeddingNetwork.

        Returns:
            tuple: A tuple containing the anchor, positive, and negative embeddings and the predicted class labels.
        &#34;&#34;&#34;
        # triplet encoding
        anchor_embedding = self.multi_embedding_network(anchor)
        positive_embedding = self.multi_embedding_network(positive)
        negative_embedding = self.multi_embedding_network(negative)
        
        #run the supervisor heads using the anchor embeddings as input
        outputs = {}
        for var, mlp in self.MLPs.items():
            outputs[var] = mlp(anchor_embedding)
        return anchor_embedding, positive_embedding, negative_embedding, outputs
    
    def configure_optimizers(self):
        &#34;&#34;&#34;
        Configure the optimizer for the MultiTripletNetwork.

        Returns:
            torch.optim.Optimizer: The configured optimizer.
        &#34;&#34;&#34;
        optimizer = torch.optim.Adam(self.parameters(), lr=self.config[&#39;lr&#39;])
        return optimizer
    
    def triplet_loss(self, anchor, positive, negative, margin=1.0):
        &#34;&#34;&#34;
        Compute the triplet loss for the given anchor, positive, and negative embeddings.

        Args:
            anchor (torch.Tensor): The anchor embedding tensor.
            positive (torch.Tensor): The positive embedding tensor.
            negative (torch.Tensor): The negative embedding tensor.
            margin (float, optional): The margin for the triplet loss. Default is 1.0.

        Returns:
            torch.Tensor: The computed triplet loss.
        &#34;&#34;&#34;
        distance_positive = (anchor - positive).pow(2).sum(1)
        distance_negative = (anchor - negative).pow(2).sum(1)
        losses = torch.relu(distance_positive - distance_negative + margin)
        return losses.mean()    

    def compute_loss(self, var, y, y_hat):
        if self.variable_types[var] == &#39;numerical&#39;:
            # Ignore instances with missing labels for numerical variables
            valid_indices = ~torch.isnan(y)
            if valid_indices.sum() &gt; 0:  # only calculate loss if there are valid targets
                y_hat = y_hat[valid_indices]
                y = y[valid_indices]

                loss = F.mse_loss(torch.flatten(y_hat), y.float())
            else:
                loss = torch.tensor(0.0, device=y_hat.device, requires_grad=True) # if no valid labels, set loss to 0
        else:
            # Ignore instances with missing labels for categorical variables
            # Assuming that missing values were encoded as -1
            valid_indices = (y != -1) &amp; (~torch.isnan(y))
            if valid_indices.sum() &gt; 0:  # only calculate loss if there are valid targets
                y_hat = y_hat[valid_indices]
                y = y[valid_indices]
                loss = F.cross_entropy(y_hat, y.long())
            else: 
                loss = torch.tensor(0.0, device=y_hat.device, requires_grad=True)
        return loss
    
    def compute_total_loss(self, losses):
        if self.use_loss_weighting and len(losses) &gt; 1:
            # Compute weighted loss for each loss 
            # Weighted loss = precision * loss + log-variance
            total_loss = sum(torch.exp(-self.log_vars[name]) * loss + self.log_vars[name] for name, loss in losses.items())
        else:
            # Compute unweighted total loss
            total_loss = sum(losses.values())
        return total_loss

    def training_step(self, train_batch, batch_idx):
        anchor, positive, negative, y_dict = train_batch[0], train_batch[1], train_batch[2], train_batch[3]
        anchor_embedding, positive_embedding, negative_embedding, outputs = self.forward(anchor, positive, negative)
        triplet_loss = self.triplet_loss(anchor_embedding, positive_embedding, negative_embedding)
        
        # compute loss values for the supervisor heads 
        losses = {&#39;triplet_loss&#39;: triplet_loss}
        for var in self.variables:
            y_hat = outputs[var]
            y = y_dict[var]
            loss = self.compute_loss(var, y, y_hat)
            losses[var] = loss

        total_loss = self.compute_total_loss(losses)
        # add total loss for logging 
        losses[&#39;train_loss&#39;] = total_loss
        self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True)
        return total_loss
    
    def validation_step(self, val_batch, batch_idx):
        anchor, positive, negative, y_dict = val_batch[0], val_batch[1], val_batch[2], val_batch[3]
        anchor_embedding, positive_embedding, negative_embedding, outputs = self.forward(anchor, positive, negative)
        triplet_loss = self.triplet_loss(anchor_embedding, positive_embedding, negative_embedding)
        
        # compute loss values for the supervisor heads 
        losses = {&#39;triplet_loss&#39;: triplet_loss}
        for var in self.variables:
            y_hat = outputs[var]
            y = y_dict[var]
            loss = self.compute_loss(var, y, y_hat)
            losses[var] = loss
        
        total_loss = sum(losses.values())
        losses[&#39;val_loss&#39;] = total_loss
        self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True)
        return total_loss
    
    def prepare_data(self):
        lt = int(len(self.dataset)*(1-self.val_size))
        lv = len(self.dataset)-lt
        dat_train, dat_val = random_split(self.dataset, [lt, lv], 
                                          generator=torch.Generator().manual_seed(42))
        return dat_train, dat_val

    def train_dataloader(self):
        return DataLoader(self.dat_train, batch_size=int(self.config[&#39;batch_size&#39;]), num_workers=0, pin_memory=True, shuffle=True, drop_last=True)

    def val_dataloader(self):
        return DataLoader(self.dat_val, batch_size=int(self.config[&#39;batch_size&#39;]), num_workers=0, pin_memory=True, shuffle=False)    
        
    # dataset: MultiOmicDataset
    def transform(self, dataset):
        &#34;&#34;&#34;
        Transforms the input dataset by generating embeddings and predictions.
        
        Args:
            dataset (MultiOmicDataset): An instance of the MultiOmicDataset class.
            
        Returns:
            z (pd.DataFrame): A dataframe containing the computed embeddings.
            y_pred (np.ndarray): A numpy array containing the predicted labels.
        &#34;&#34;&#34;
        self.eval()
        # get anchor embeddings 
        z = pd.DataFrame(self.multi_embedding_network(dataset.dat).detach().numpy())
        z.columns = [&#39;&#39;.join([&#39;E&#39;, str(x)]) for x in z.columns]
        z.index = dataset.samples
        return z

    def predict(self, dataset):
        &#34;&#34;&#34;
        Evaluate the model on a given dataset.

        Args:
            dataset: The dataset to evaluate the model on.

        Returns:
            A dictionary where each key is a target variable and the corresponding value is the predicted output for that variable.
        &#34;&#34;&#34;
        self.eval()
        # get anchor embedding
        anchor_embedding = self.multi_embedding_network(dataset.dat)
        # get MLP outputs for each var
        outputs = {}
        for var, mlp in self.MLPs.items():
            outputs[var] = mlp(anchor_embedding)
        
        # get predictions from the mlp outputs for each var
        predictions = {}
        for var in self.variables:
            y_pred = outputs[var].detach().numpy()
            if self.variable_types[var] == &#39;categorical&#39;:
                predictions[var] = np.argmax(y_pred, axis=1)
            else:
                predictions[var] = y_pred
        return predictions

    
    # Adaptor forward function for captum integrated gradients. 
    # layer_sizes: number of features in each omic layer 
    def forward_target(self, input_data, layer_sizes, target_var, steps):
        outputs_list = []
        for i in range(steps):
            # for each step, get anchor/positive/negative tensors 
            # (split the concatenated omics layers)
            anchor = input_data[i][0].split(layer_sizes, dim = 1)
            positive = input_data[i][1].split(layer_sizes, dim = 1)
            negative = input_data[i][2].split(layer_sizes, dim = 1)
            
            # convert to dict
            anchor = {k: anchor[k] for k in range(len(anchor))}
            positive = {k: anchor[k] for k in range(len(positive))}
            negative = {k: anchor[k] for k in range(len(negative))}
            anchor_embedding, positive_embedding, negative_embedding, outputs = self.forward(anchor, positive, negative)
            outputs_list.append(outputs[target_var])
        return torch.cat(outputs_list, dim = 0)
        
    def compute_feature_importance(self, target_var, steps = 5):
        &#34;&#34;&#34;
        Compute the feature importance.

        Args:
            input_data (torch.Tensor): The input data to compute the feature importance for.
            target_var (str): The target variable to compute the feature importance for.
        Returns:
            attributions (list of torch.Tensor): The feature importances for each class.
        &#34;&#34;&#34;
        
        # self.dataset is a TripletMultiomicDataset, which has a different 
        # structure than the MultiomicDataset. We use data loader to 
        # read the triplets and get anchor/positive/negative tensors
        # read the whole dataset
        dl = DataLoader(self.dataset, batch_size=len(self.dataset))
        it = iter(dl)
        anchor, positive, negative, y_dict = next(it) 
                
        # Initialize the Integrated Gradients method
        ig = IntegratedGradients(self.forward_target)

        anchor = [data.requires_grad_() for data in list(anchor.values())]
        positive = [data.requires_grad_() for data in list(positive.values())]
        negative = [data.requires_grad_() for data in list(negative.values())]
        
        # concatenate multiomic layers of each list element
        # then stack the anchor/positive/negative 
        # the purpose is to get a single tensor
        input_data = torch.stack([torch.cat(sublist, dim = 1) for sublist in [anchor, positive, negative]]).unsqueeze(0)

        # layer sizes will be needed to revert the concatenated tensor 
        # anchor/positive/negative have the same shape
        layer_sizes = [anchor[i].shape[1] for i in range(len(anchor))] 

        # Define a baseline 
        baseline = torch.zeros_like(input_data)       

        # Get the number of classes for the target variable
        if self.variable_types[target_var] == &#39;numerical&#39;:
            num_class = 1
        else:
            num_class = len(np.unique(self.ann[target_var]))

        # Compute the feature importance for each class
        attributions = []
        if num_class &gt; 1:
            for target_class in range(num_class):
                attributions.append(ig.attribute(input_data, baseline, additional_forward_args=(layer_sizes, target_var, steps), target=target_class, n_steps=steps))
        else:
            attributions.append(ig.attribute(input_data, baseline, additional_forward_args=(layer_sizes, target_var, steps), n_steps=steps))

        # summarize feature importances
        # Compute absolute attributions
        abs_attr = [[torch.abs(a) for a in attr_class] for attr_class in attributions]
        # average over samples 
        imp = [[a.mean(dim=1) for a in attr_class] for attr_class in abs_attr]

        # combine into a single data frame 
        df_list = []
        layers = list(self.dataset.dataset.dat.keys())  # accessing multiomicdataset within tripletmultiomic dataset here
        for i in range(num_class):
            imp_layerwise = imp[i][0].split(layer_sizes, dim = 1)
            for j in range(len(layers)):
                features = self.dataset.dataset.features[layers[j]] # accessing multiomicdataset within tripletmultiomic dataset here
                importances = imp_layerwise[j][0].detach().numpy() # 0 =&gt; extract importances only for the anchor 
                df_list.append(pd.DataFrame({&#39;target_variable&#39;: target_var, &#39;target_class&#39;: i, &#39;layer&#39;: layers[j], &#39;name&#39;: features, &#39;importance&#39;: importances}))    
        df_imp = pd.concat(df_list, ignore_index = True)
        
        # save scores in model
        self.feature_importances[target_var] = df_imp</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pytorch_lightning.core.module.LightningModule</li>
<li>lightning_fabric.utilities.device_dtype_mixin._DeviceDtypeModuleMixin</li>
<li>pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin</li>
<li>pytorch_lightning.core.hooks.ModelHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="flexynesis.models.MultiTripletNetwork.compute_feature_importance"><code class="name flex">
<span>def <span class="ident">compute_feature_importance</span></span>(<span>self, target_var, steps=5)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the feature importance.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_data</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The input data to compute the feature importance for.</dd>
<dt><strong><code>target_var</code></strong> :&ensp;<code>str</code></dt>
<dd>The target variable to compute the feature importance for.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>attributions (list of torch.Tensor): The feature importances for each class.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_feature_importance(self, target_var, steps = 5):
    &#34;&#34;&#34;
    Compute the feature importance.

    Args:
        input_data (torch.Tensor): The input data to compute the feature importance for.
        target_var (str): The target variable to compute the feature importance for.
    Returns:
        attributions (list of torch.Tensor): The feature importances for each class.
    &#34;&#34;&#34;
    
    # self.dataset is a TripletMultiomicDataset, which has a different 
    # structure than the MultiomicDataset. We use data loader to 
    # read the triplets and get anchor/positive/negative tensors
    # read the whole dataset
    dl = DataLoader(self.dataset, batch_size=len(self.dataset))
    it = iter(dl)
    anchor, positive, negative, y_dict = next(it) 
            
    # Initialize the Integrated Gradients method
    ig = IntegratedGradients(self.forward_target)

    anchor = [data.requires_grad_() for data in list(anchor.values())]
    positive = [data.requires_grad_() for data in list(positive.values())]
    negative = [data.requires_grad_() for data in list(negative.values())]
    
    # concatenate multiomic layers of each list element
    # then stack the anchor/positive/negative 
    # the purpose is to get a single tensor
    input_data = torch.stack([torch.cat(sublist, dim = 1) for sublist in [anchor, positive, negative]]).unsqueeze(0)

    # layer sizes will be needed to revert the concatenated tensor 
    # anchor/positive/negative have the same shape
    layer_sizes = [anchor[i].shape[1] for i in range(len(anchor))] 

    # Define a baseline 
    baseline = torch.zeros_like(input_data)       

    # Get the number of classes for the target variable
    if self.variable_types[target_var] == &#39;numerical&#39;:
        num_class = 1
    else:
        num_class = len(np.unique(self.ann[target_var]))

    # Compute the feature importance for each class
    attributions = []
    if num_class &gt; 1:
        for target_class in range(num_class):
            attributions.append(ig.attribute(input_data, baseline, additional_forward_args=(layer_sizes, target_var, steps), target=target_class, n_steps=steps))
    else:
        attributions.append(ig.attribute(input_data, baseline, additional_forward_args=(layer_sizes, target_var, steps), n_steps=steps))

    # summarize feature importances
    # Compute absolute attributions
    abs_attr = [[torch.abs(a) for a in attr_class] for attr_class in attributions]
    # average over samples 
    imp = [[a.mean(dim=1) for a in attr_class] for attr_class in abs_attr]

    # combine into a single data frame 
    df_list = []
    layers = list(self.dataset.dataset.dat.keys())  # accessing multiomicdataset within tripletmultiomic dataset here
    for i in range(num_class):
        imp_layerwise = imp[i][0].split(layer_sizes, dim = 1)
        for j in range(len(layers)):
            features = self.dataset.dataset.features[layers[j]] # accessing multiomicdataset within tripletmultiomic dataset here
            importances = imp_layerwise[j][0].detach().numpy() # 0 =&gt; extract importances only for the anchor 
            df_list.append(pd.DataFrame({&#39;target_variable&#39;: target_var, &#39;target_class&#39;: i, &#39;layer&#39;: layers[j], &#39;name&#39;: features, &#39;importance&#39;: importances}))    
    df_imp = pd.concat(df_list, ignore_index = True)
    
    # save scores in model
    self.feature_importances[target_var] = df_imp</code></pre>
</details>
</dd>
<dt id="flexynesis.models.MultiTripletNetwork.compute_loss"><code class="name flex">
<span>def <span class="ident">compute_loss</span></span>(<span>self, var, y, y_hat)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_loss(self, var, y, y_hat):
    if self.variable_types[var] == &#39;numerical&#39;:
        # Ignore instances with missing labels for numerical variables
        valid_indices = ~torch.isnan(y)
        if valid_indices.sum() &gt; 0:  # only calculate loss if there are valid targets
            y_hat = y_hat[valid_indices]
            y = y[valid_indices]

            loss = F.mse_loss(torch.flatten(y_hat), y.float())
        else:
            loss = torch.tensor(0.0, device=y_hat.device, requires_grad=True) # if no valid labels, set loss to 0
    else:
        # Ignore instances with missing labels for categorical variables
        # Assuming that missing values were encoded as -1
        valid_indices = (y != -1) &amp; (~torch.isnan(y))
        if valid_indices.sum() &gt; 0:  # only calculate loss if there are valid targets
            y_hat = y_hat[valid_indices]
            y = y[valid_indices]
            loss = F.cross_entropy(y_hat, y.long())
        else: 
            loss = torch.tensor(0.0, device=y_hat.device, requires_grad=True)
    return loss</code></pre>
</details>
</dd>
<dt id="flexynesis.models.MultiTripletNetwork.compute_total_loss"><code class="name flex">
<span>def <span class="ident">compute_total_loss</span></span>(<span>self, losses)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_total_loss(self, losses):
    if self.use_loss_weighting and len(losses) &gt; 1:
        # Compute weighted loss for each loss 
        # Weighted loss = precision * loss + log-variance
        total_loss = sum(torch.exp(-self.log_vars[name]) * loss + self.log_vars[name] for name, loss in losses.items())
    else:
        # Compute unweighted total loss
        total_loss = sum(losses.values())
    return total_loss</code></pre>
</details>
</dd>
<dt id="flexynesis.models.MultiTripletNetwork.configure_optimizers"><code class="name flex">
<span>def <span class="ident">configure_optimizers</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Configure the optimizer for the MultiTripletNetwork.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.optim.Optimizer</code></dt>
<dd>The configured optimizer.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure_optimizers(self):
    &#34;&#34;&#34;
    Configure the optimizer for the MultiTripletNetwork.

    Returns:
        torch.optim.Optimizer: The configured optimizer.
    &#34;&#34;&#34;
    optimizer = torch.optim.Adam(self.parameters(), lr=self.config[&#39;lr&#39;])
    return optimizer</code></pre>
</details>
</dd>
<dt id="flexynesis.models.MultiTripletNetwork.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, anchor, positive, negative) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the forward pass of the MultiTripletNetwork and return the embeddings and predictions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>anchor</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary containing the anchor input tensors for each EmbeddingNetwork.</dd>
<dt><strong><code>positive</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary containing the positive input tensors for each EmbeddingNetwork.</dd>
<dt><strong><code>negative</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary containing the negative input tensors for each EmbeddingNetwork.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>A tuple containing the anchor, positive, and negative embeddings and the predicted class labels.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, anchor, positive, negative):
    &#34;&#34;&#34;
    Compute the forward pass of the MultiTripletNetwork and return the embeddings and predictions.

    Args:
        anchor (dict): A dictionary containing the anchor input tensors for each EmbeddingNetwork.
        positive (dict): A dictionary containing the positive input tensors for each EmbeddingNetwork.
        negative (dict): A dictionary containing the negative input tensors for each EmbeddingNetwork.

    Returns:
        tuple: A tuple containing the anchor, positive, and negative embeddings and the predicted class labels.
    &#34;&#34;&#34;
    # triplet encoding
    anchor_embedding = self.multi_embedding_network(anchor)
    positive_embedding = self.multi_embedding_network(positive)
    negative_embedding = self.multi_embedding_network(negative)
    
    #run the supervisor heads using the anchor embeddings as input
    outputs = {}
    for var, mlp in self.MLPs.items():
        outputs[var] = mlp(anchor_embedding)
    return anchor_embedding, positive_embedding, negative_embedding, outputs</code></pre>
</details>
</dd>
<dt id="flexynesis.models.MultiTripletNetwork.forward_target"><code class="name flex">
<span>def <span class="ident">forward_target</span></span>(<span>self, input_data, layer_sizes, target_var, steps)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_target(self, input_data, layer_sizes, target_var, steps):
    outputs_list = []
    for i in range(steps):
        # for each step, get anchor/positive/negative tensors 
        # (split the concatenated omics layers)
        anchor = input_data[i][0].split(layer_sizes, dim = 1)
        positive = input_data[i][1].split(layer_sizes, dim = 1)
        negative = input_data[i][2].split(layer_sizes, dim = 1)
        
        # convert to dict
        anchor = {k: anchor[k] for k in range(len(anchor))}
        positive = {k: anchor[k] for k in range(len(positive))}
        negative = {k: anchor[k] for k in range(len(negative))}
        anchor_embedding, positive_embedding, negative_embedding, outputs = self.forward(anchor, positive, negative)
        outputs_list.append(outputs[target_var])
    return torch.cat(outputs_list, dim = 0)</code></pre>
</details>
</dd>
<dt id="flexynesis.models.MultiTripletNetwork.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, dataset)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the model on a given dataset.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset</code></strong></dt>
<dd>The dataset to evaluate the model on.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A dictionary where each key is a target variable and the corresponding value is the predicted output for that variable.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, dataset):
    &#34;&#34;&#34;
    Evaluate the model on a given dataset.

    Args:
        dataset: The dataset to evaluate the model on.

    Returns:
        A dictionary where each key is a target variable and the corresponding value is the predicted output for that variable.
    &#34;&#34;&#34;
    self.eval()
    # get anchor embedding
    anchor_embedding = self.multi_embedding_network(dataset.dat)
    # get MLP outputs for each var
    outputs = {}
    for var, mlp in self.MLPs.items():
        outputs[var] = mlp(anchor_embedding)
    
    # get predictions from the mlp outputs for each var
    predictions = {}
    for var in self.variables:
        y_pred = outputs[var].detach().numpy()
        if self.variable_types[var] == &#39;categorical&#39;:
            predictions[var] = np.argmax(y_pred, axis=1)
        else:
            predictions[var] = y_pred
    return predictions</code></pre>
</details>
</dd>
<dt id="flexynesis.models.MultiTripletNetwork.prepare_data"><code class="name flex">
<span>def <span class="ident">prepare_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Use this to download and prepare data. Downloading and saving data with multiple processes (distributed
settings) will result in corrupted data. Lightning ensures this method is called only within a single process,
so you can safely add your downloading logic within.</p>
<div class="admonition warning">
<p class="admonition-title">Warning:&ensp;DO NOT set state to the model (use <code>setup</code> instead)</p>
<p>since this is NOT called on every device</p>
</div>
<p>Example::</p>
<pre><code>def prepare_data(self):
    # good
    download_data()
    tokenize()
    etc()

    # bad
    self.split = data_split
    self.some_state = some_other_state()
</code></pre>
<p>In a distributed environment, <code>prepare_data</code> can be called in two ways
(using :ref:<code>prepare_data_per_node&lt;common/lightning_module:prepare_data_per_node&gt;</code>)</p>
<ol>
<li>Once per node. This is the default and is only called on LOCAL_RANK=0.</li>
<li>Once in total. Only called on GLOBAL_RANK=0.</li>
</ol>
<p>Example::</p>
<pre><code># DEFAULT
# called once per node on LOCAL_RANK=0 of that node
class LitDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()
        self.prepare_data_per_node = True


# call on GLOBAL_RANK=0 (great for shared file systems)
class LitDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()
        self.prepare_data_per_node = False
</code></pre>
<p>This is called before requesting the dataloaders:</p>
<p>.. code-block:: python</p>
<pre><code>model.prepare_data()
initialize_distributed()
model.setup(stage)
model.train_dataloader()
model.val_dataloader()
model.test_dataloader()
model.predict_dataloader()
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_data(self):
    lt = int(len(self.dataset)*(1-self.val_size))
    lv = len(self.dataset)-lt
    dat_train, dat_val = random_split(self.dataset, [lt, lv], 
                                      generator=torch.Generator().manual_seed(42))
    return dat_train, dat_val</code></pre>
</details>
</dd>
<dt id="flexynesis.models.MultiTripletNetwork.train_dataloader"><code class="name flex">
<span>def <span class="ident">train_dataloader</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>An iterable or collection of iterables specifying training samples.</p>
<p>For more information about multiple dataloaders, see this :ref:<code>section &lt;multiple-dataloaders&gt;</code>.</p>
<p>The dataloader you return will not be reloaded unless you set
:paramref:<code>~pytorch_lightning.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs</code> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<pre><code>- download in :meth:&lt;code&gt;prepare\_data&lt;/code&gt;
- process and split in :meth:&lt;code&gt;setup&lt;/code&gt;
</code></pre>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning:&ensp;do not assign state in prepare_data</p>
</div>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.trainer.Trainer.fit</code></li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_dataloader(self):
    return DataLoader(self.dat_train, batch_size=int(self.config[&#39;batch_size&#39;]), num_workers=0, pin_memory=True, shuffle=True, drop_last=True)</code></pre>
</details>
</dd>
<dt id="flexynesis.models.MultiTripletNetwork.training_step"><code class="name flex">
<span>def <span class="ident">training_step</span></span>(<span>self, train_batch, batch_idx)</span>
</code></dt>
<dd>
<div class="desc"><p>Here you compute and return the training loss and some additional metrics for e.g. the progress bar or
logger.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>The output of your data iterable, normally a :class:<code>~torch.utils.data.DataLoader</code>.</dd>
<dt><strong><code>batch_idx</code></strong></dt>
<dd>The index of this batch.</dd>
<dt><strong><code>dataloader_idx</code></strong></dt>
<dd>The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</dd>
</dl>
<h2 id="return">Return</h2>
<ul>
<li>:class:<code>~torch.Tensor</code> - The loss tensor</li>
<li><code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code>.</li>
<li><code>None</code> - Skip to the next batch. This is only supported for automatic optimization.
This is not supported for multi-GPU, TPU, IPU, or DeepSpeed.</li>
</ul>
<p>In this step you'd normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<p>Example::</p>
<pre><code>def training_step(self, batch, batch_idx):
    x, y, z = batch
    out = self.encoder(x)
    loss = self.loss(out, x)
    return loss
</code></pre>
<p>To use multiple optimizers, you can switch to 'manual optimization' and control their stepping:</p>
<p>.. code-block:: python</p>
<pre><code>def __init__(self):
    super().__init__()
    self.automatic_optimization = False


# Multiple optimizers (e.g.: GANs)
def training_step(self, batch, batch_idx):
    opt1, opt2 = self.optimizers()

    # do training_step with encoder
    ...
    opt1.step()
    # do training_step with decoder
    ...
    opt2.step()
</code></pre>
<h2 id="note">Note</h2>
<p>When <code>accumulate_grad_batches</code> &gt; 1, the loss returned here will be automatically
normalized by <code>accumulate_grad_batches</code> internally.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def training_step(self, train_batch, batch_idx):
    anchor, positive, negative, y_dict = train_batch[0], train_batch[1], train_batch[2], train_batch[3]
    anchor_embedding, positive_embedding, negative_embedding, outputs = self.forward(anchor, positive, negative)
    triplet_loss = self.triplet_loss(anchor_embedding, positive_embedding, negative_embedding)
    
    # compute loss values for the supervisor heads 
    losses = {&#39;triplet_loss&#39;: triplet_loss}
    for var in self.variables:
        y_hat = outputs[var]
        y = y_dict[var]
        loss = self.compute_loss(var, y, y_hat)
        losses[var] = loss

    total_loss = self.compute_total_loss(losses)
    # add total loss for logging 
    losses[&#39;train_loss&#39;] = total_loss
    self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True)
    return total_loss</code></pre>
</details>
</dd>
<dt id="flexynesis.models.MultiTripletNetwork.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, dataset)</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms the input dataset by generating embeddings and predictions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>MultiOmicDataset</code></dt>
<dd>An instance of the MultiOmicDataset class.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>z (pd.DataFrame): A dataframe containing the computed embeddings.
y_pred (np.ndarray): A numpy array containing the predicted labels.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, dataset):
    &#34;&#34;&#34;
    Transforms the input dataset by generating embeddings and predictions.
    
    Args:
        dataset (MultiOmicDataset): An instance of the MultiOmicDataset class.
        
    Returns:
        z (pd.DataFrame): A dataframe containing the computed embeddings.
        y_pred (np.ndarray): A numpy array containing the predicted labels.
    &#34;&#34;&#34;
    self.eval()
    # get anchor embeddings 
    z = pd.DataFrame(self.multi_embedding_network(dataset.dat).detach().numpy())
    z.columns = [&#39;&#39;.join([&#39;E&#39;, str(x)]) for x in z.columns]
    z.index = dataset.samples
    return z</code></pre>
</details>
</dd>
<dt id="flexynesis.models.MultiTripletNetwork.triplet_loss"><code class="name flex">
<span>def <span class="ident">triplet_loss</span></span>(<span>self, anchor, positive, negative, margin=1.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the triplet loss for the given anchor, positive, and negative embeddings.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>anchor</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The anchor embedding tensor.</dd>
<dt><strong><code>positive</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The positive embedding tensor.</dd>
<dt><strong><code>negative</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The negative embedding tensor.</dd>
<dt><strong><code>margin</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The margin for the triplet loss. Default is 1.0.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>The computed triplet loss.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def triplet_loss(self, anchor, positive, negative, margin=1.0):
    &#34;&#34;&#34;
    Compute the triplet loss for the given anchor, positive, and negative embeddings.

    Args:
        anchor (torch.Tensor): The anchor embedding tensor.
        positive (torch.Tensor): The positive embedding tensor.
        negative (torch.Tensor): The negative embedding tensor.
        margin (float, optional): The margin for the triplet loss. Default is 1.0.

    Returns:
        torch.Tensor: The computed triplet loss.
    &#34;&#34;&#34;
    distance_positive = (anchor - positive).pow(2).sum(1)
    distance_negative = (anchor - negative).pow(2).sum(1)
    losses = torch.relu(distance_positive - distance_negative + margin)
    return losses.mean()    </code></pre>
</details>
</dd>
<dt id="flexynesis.models.MultiTripletNetwork.val_dataloader"><code class="name flex">
<span>def <span class="ident">val_dataloader</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>An iterable or collection of iterables specifying validation samples.</p>
<p>For more information about multiple dataloaders, see this :ref:<code>section &lt;multiple-dataloaders&gt;</code>.</p>
<p>The dataloader you return will not be reloaded unless you set
:paramref:<code>~pytorch_lightning.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs</code> to
a positive integer.</p>
<p>It's recommended that all data downloads and preparation happen in :meth:<code>prepare_data</code>.</p>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.trainer.Trainer.fit</code></li>
<li>:meth:<code>~pytorch_lightning.trainer.trainer.Trainer.validate</code></li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
<h2 id="note_1">Note</h2>
<p>If you don't need a validation dataset and a :meth:<code>validation_step</code>, you don't need to
implement this method.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def val_dataloader(self):
    return DataLoader(self.dat_val, batch_size=int(self.config[&#39;batch_size&#39;]), num_workers=0, pin_memory=True, shuffle=False)    </code></pre>
</details>
</dd>
<dt id="flexynesis.models.MultiTripletNetwork.validation_step"><code class="name flex">
<span>def <span class="ident">validation_step</span></span>(<span>self, val_batch, batch_idx)</span>
</code></dt>
<dd>
<div class="desc"><p>Operates on a single batch of data from the validation set. In this step you'd might generate examples or
calculate anything of interest like accuracy.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>The output of your data iterable, normally a :class:<code>~torch.utils.data.DataLoader</code>.</dd>
<dt><strong><code>batch_idx</code></strong></dt>
<dd>The index of this batch.</dd>
<dt><strong><code>dataloader_idx</code></strong></dt>
<dd>The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</dd>
</dl>
<h2 id="return">Return</h2>
<ul>
<li>:class:<code>~torch.Tensor</code> - The loss tensor</li>
<li><code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code>.</li>
<li><code>None</code> - Skip to the next batch.</li>
</ul>
<p>.. code-block:: python</p>
<pre><code># if you have one val dataloader:
def validation_step(self, batch, batch_idx):
    ...


# if you have multiple val dataloaders:
def validation_step(self, batch, batch_idx, dataloader_idx=0):
    ...
</code></pre>
<p>Examples::</p>
<pre><code># CASE 1: A single validation dataset
def validation_step(self, batch, batch_idx):
    x, y = batch

    # implement your own
    out = self(x)
    loss = self.loss(out, y)

    # log 6 example images
    # or generated text... or whatever
    sample_imgs = x[:6]
    grid = torchvision.utils.make_grid(sample_imgs)
    self.logger.experiment.add_image('example_images', grid, 0)

    # calculate acc
    labels_hat = torch.argmax(out, dim=1)
    val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)

    # log the outputs!
    self.log_dict({'val_loss': loss, 'val_acc': val_acc})
</code></pre>
<p>If you pass in multiple val dataloaders, :meth:<code>validation_step</code> will have an additional argument. We recommend
setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>
<p>.. code-block:: python</p>
<pre><code># CASE 2: multiple validation dataloaders
def validation_step(self, batch, batch_idx, dataloader_idx=0):
    # dataloader_idx tells you which dataset this is.
    ...
</code></pre>
<h2 id="note">Note</h2>
<p>If you don't need to validate you don't need to implement this method.</p>
<h2 id="note_1">Note</h2>
<p>When the :meth:<code>validation_step</code> is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validation_step(self, val_batch, batch_idx):
    anchor, positive, negative, y_dict = val_batch[0], val_batch[1], val_batch[2], val_batch[3]
    anchor_embedding, positive_embedding, negative_embedding, outputs = self.forward(anchor, positive, negative)
    triplet_loss = self.triplet_loss(anchor_embedding, positive_embedding, negative_embedding)
    
    # compute loss values for the supervisor heads 
    losses = {&#39;triplet_loss&#39;: triplet_loss}
    for var in self.variables:
        y_hat = outputs[var]
        y = y_dict[var]
        loss = self.compute_loss(var, y, y_hat)
        losses[var] = loss
    
    total_loss = sum(losses.values())
    losses[&#39;val_loss&#39;] = total_loss
    self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True)
    return total_loss</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="flexynesis.models.supervised_vae"><code class="flex name class">
<span>class <span class="ident">supervised_vae</span></span>
<span>(</span><span>config, dataset, target_variables, batch_variables=None, val_size=0.2, use_loss_weighting=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Supervised Variational Auto-encoder for multi-omics data fusion and prediction.</p>
<p>This class implements a deep learning model for fusing and predicting from multiple omics layers/matrices.
Each omics layer is encoded separately using an Encoder. The resulting latent representations are then
concatenated and passed through a fully connected network (fusion layer) to make predictions. The model
also includes a supervisor head for supervised learning.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>num_layers</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of omics layers/matrices.</dd>
<dt><strong><code>input_dims</code></strong> :&ensp;<code>list</code> of <code>int</code></dt>
<dd>A list of input dimensions for each omics layer.</dd>
<dt><strong><code>hidden_dims</code></strong> :&ensp;<code>list</code> of <code>int</code></dt>
<dd>A list of hidden dimensions for the Encoder and Decoder.</dd>
<dt><strong><code>latent_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>The dimension of the latent space for each encoder.</dd>
<dt><strong><code>num_class</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of output classes for the prediction task.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments to be passed to the MLP encoders.</dd>
</dl>
<h2 id="example">Example</h2>
<h1 id="instantiate-a-supervised_vae-model-with-2-omics-layers-and-input-dimensions-of-100-and-200">Instantiate a supervised_vae model with 2 omics layers and input dimensions of 100 and 200</h1>
<p>model = supervised_vae(num_layers=2, input_dims=[100, 200], hidden_dims=[64, 32], latent_dim=16, num_class=1)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class supervised_vae(pl.LightningModule):
    &#34;&#34;&#34;
    Supervised Variational Auto-encoder for multi-omics data fusion and prediction.

    This class implements a deep learning model for fusing and predicting from multiple omics layers/matrices.
    Each omics layer is encoded separately using an Encoder. The resulting latent representations are then
    concatenated and passed through a fully connected network (fusion layer) to make predictions. The model
    also includes a supervisor head for supervised learning.

    Args:
        num_layers (int): Number of omics layers/matrices.
        input_dims (list of int): A list of input dimensions for each omics layer.
        hidden_dims (list of int): A list of hidden dimensions for the Encoder and Decoder.
        latent_dim (int): The dimension of the latent space for each encoder.
        num_class (int): Number of output classes for the prediction task.
        **kwargs: Additional keyword arguments to be passed to the MLP encoders.

    Example:

        # Instantiate a supervised_vae model with 2 omics layers and input dimensions of 100 and 200
        model = supervised_vae(num_layers=2, input_dims=[100, 200], hidden_dims=[64, 32], latent_dim=16, num_class=1)

    &#34;&#34;&#34;
    def __init__(self,  config, dataset, target_variables, batch_variables = None, val_size = 0.2, use_loss_weighting = True):
        super(supervised_vae, self).__init__()
        self.config = config
        self.dataset = dataset
        self.target_variables = target_variables
        self.batch_variables = batch_variables
        self.variables = target_variables + batch_variables if batch_variables else target_variables
        self.val_size = val_size

        self.dat_train, self.dat_val = self.prepare_data()
        self.feature_importances = {}
        
        # sometimes the model may have exploding/vanishing gradients leading to NaN values
        self.nan_detected = False 
        
        self.use_loss_weighting = use_loss_weighting
        
        if self.use_loss_weighting:
            # Initialize log variance parameters for uncertainty weighting
            self.log_vars = nn.ParameterDict()
            for loss_type in itertools.chain(self.variables, [&#39;mmd_loss&#39;]):
                self.log_vars[loss_type] = nn.Parameter(torch.zeros(1))
        
        layers = list(dataset.dat.keys())
        input_dims = [len(dataset.features[layers[i]]) for i in range(len(layers))]
        # create a list of Encoder instances for separately encoding each omics layer
        self.encoders = nn.ModuleList([Encoder(input_dims[i], [config[&#39;hidden_dim&#39;]], config[&#39;latent_dim&#39;]) for i in range(len(layers))])
        # Fully connected layers for concatenated means and log_vars
        self.FC_mean = nn.Linear(len(layers) * config[&#39;latent_dim&#39;], config[&#39;latent_dim&#39;])
        self.FC_log_var = nn.Linear(len(layers) * config[&#39;latent_dim&#39;], config[&#39;latent_dim&#39;])
        # list of decoders to decode each omics layer separately 
        self.decoders = nn.ModuleList([Decoder(config[&#39;latent_dim&#39;], [config[&#39;hidden_dim&#39;]], input_dims[i]) for i in range(len(layers))])

        # define supervisor heads
        # using ModuleDict to store multiple MLPs
        self.MLPs = nn.ModuleDict()         
        for var in self.variables:
            if self.dataset.variable_types[var] == &#39;numerical&#39;:
                num_class = 1
            else:
                num_class = len(np.unique(self.dataset.ann[var]))
            self.MLPs[var] = MLP(input_dim = config[&#39;latent_dim&#39;], 
                                 hidden_dim = config[&#39;supervisor_hidden_dim&#39;], 
                                 output_dim = num_class)
                                       
    def multi_encoder(self, x_list):
        &#34;&#34;&#34;
        Encode each input matrix separately using the corresponding Encoder.

        Args:
            x_list (list of torch.Tensor): List of input matrices for each omics layer.

        Returns:
            tuple: Tuple containing:
                - mean (torch.Tensor): Concatenated mean values from each encoder.
                - log_var (torch.Tensor): Concatenated log variance values from each encoder.
        &#34;&#34;&#34;
        means, log_vars = [], []
        # Process each input matrix with its corresponding Encoder
        for i, x in enumerate(x_list):
            mean, log_var = self.encoders[i](x)
            means.append(mean)
            log_vars.append(log_var)

        # Concatenate means and log_vars
        # Push concatenated means and log_vars through the fully connected layers
        mean = self.FC_mean(torch.cat(means, dim=1))
        log_var = self.FC_log_var(torch.cat(log_vars, dim=1))
        return mean, log_var
    
    def forward(self, x_list):
        &#34;&#34;&#34;
        Forward pass through the model.

        Args:
            x_list (list of torch.Tensor): List of input matrices for each omics layer.

        Returns:
            tuple: Tuple containing:
                - x_hat_list (list of torch.Tensor): List of reconstructed matrices for each omics layer.
                - z (torch.Tensor): Latent representation.
                - mean (torch.Tensor): Concatenated mean values from each encoder.
                - log_var (torch.Tensor): Concatenated log variance values from each encoder.
                - y_pred (torch.Tensor): Predicted output.
        &#34;&#34;&#34;
        mean, log_var = self.multi_encoder(x_list)
        
        # generate latent layer
        z = self.reparameterization(mean, log_var)

        # Decode each latent variable with its corresponding Decoder
        x_hat_list = [self.decoders[i](z) for i in range(len(x_list))]

        #run the supervisor heads using the latent layer as input
        outputs = {}
        for var, mlp in self.MLPs.items():
            outputs[var] = mlp(z)
            
        return x_hat_list, z, mean, log_var, outputs
        
    def reparameterization(self, mean, var):
        &#34;&#34;&#34;
        Reparameterize the mean and variance values.

        Args:
            mean (torch.Tensor): Mean values from the encoders.
            var (torch.Tensor): Variance values from the encoders.

        Returns:
            torch.Tensor: Latent representation.
        &#34;&#34;&#34;
        epsilon = torch.randn_like(var)       
        z = mean + var*epsilon                         
        return z
    
    def configure_optimizers(self):
        &#34;&#34;&#34;
        Configure the optimizer for the model.

        Returns:
            torch.optim.Adam: Adam optimizer with learning rate 1e-3.
        &#34;&#34;&#34;
        optimizer = torch.optim.Adam(self.parameters(), lr=self.config[&#39;lr&#39;])
        return optimizer
    
    def compute_loss(self, var, y, y_hat):
        if self.dataset.variable_types[var] == &#39;numerical&#39;:
            # Ignore instances with missing labels for numerical variables
            valid_indices = ~torch.isnan(y)
            if valid_indices.sum() &gt; 0:  # only calculate loss if there are valid targets
                y_hat = y_hat[valid_indices]
                y = y[valid_indices]
                loss = F.mse_loss(torch.flatten(y_hat), y.float())
            else:
                loss = torch.tensor(0.0, device=y_hat.device, requires_grad=True) # if no valid labels, set loss to 0
        else:
            # Ignore instances with missing labels for categorical variables
            # Assuming that missing values were encoded as -1
            valid_indices = (y != -1) &amp; (~torch.isnan(y))
            if valid_indices.sum() &gt; 0:  # only calculate loss if there are valid targets
                y_hat = y_hat[valid_indices]
                y = y[valid_indices]
                loss = F.cross_entropy(y_hat, y.long())
            else: 
                loss = torch.tensor(0.0, device=y_hat.device, requires_grad=True)
        return loss
    
    def compute_total_loss(self, losses):
        if self.use_loss_weighting and len(losses) &gt; 1:
            # Compute weighted loss for each loss 
            # Weighted loss = precision * loss + log-variance
            total_loss = sum(torch.exp(-self.log_vars[name]) * loss + self.log_vars[name] for name, loss in losses.items())
        else:
            # Compute unweighted total loss
            total_loss = sum(losses.values())
        return total_loss

    
    def training_step(self, train_batch, batch_idx):
        dat, y_dict = train_batch
        layers = dat.keys()
        x_list = [dat[x] for x in layers]
        
        x_hat_list, z, mean, log_var, outputs = self.forward(x_list)
        
        # compute mmd loss for each layer and take average
        mmd_loss_list = [self.MMD_loss(z.shape[1], z, x_hat_list[i], x_list[i]) for i in range(len(layers))]
        mmd_loss = torch.mean(torch.stack(mmd_loss_list))

        # compute loss values for the supervisor heads 
        losses = {&#39;mmd_loss&#39;: mmd_loss}
        
        for var in self.variables:
            y_hat = outputs[var]
            y = y_dict[var]
            loss = self.compute_loss(var, y, y_hat)
            losses[var] = loss
            
        total_loss = self.compute_total_loss(losses)
        # add total loss for logging 
        losses[&#39;train_loss&#39;] = total_loss
        self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True)
        return total_loss
    
    def validation_step(self, val_batch, batch_idx):
        dat, y_dict = val_batch
        layers = dat.keys()
        x_list = [dat[x] for x in layers]
        
        x_hat_list, z, mean, log_var, outputs = self.forward(x_list)
        
        # compute mmd loss for each layer and take average
        mmd_loss_list = [self.MMD_loss(z.shape[1], z, x_hat_list[i], x_list[i]) for i in range(len(layers))]
        mmd_loss = torch.mean(torch.stack(mmd_loss_list))

        # compute loss values for the supervisor heads 
        losses = {&#39;mmd_loss&#39;: mmd_loss}
        for var in self.variables:
            y_hat = outputs[var]
            y = y_dict[var]
            loss = self.compute_loss(var, y, y_hat)
            losses[var] = loss
            
        total_loss = sum(losses.values())
        losses[&#39;val_loss&#39;] = total_loss
        self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True)
        return total_loss
                                       
    def prepare_data(self):
        lt = int(len(self.dataset)*(1-self.val_size))
        lv = len(self.dataset)-lt
        dat_train, dat_val = random_split(self.dataset, [lt, lv], 
                                          generator=torch.Generator().manual_seed(42))
        return dat_train, dat_val
    
    def train_dataloader(self):
        return DataLoader(self.dat_train, batch_size=int(self.config[&#39;batch_size&#39;]), num_workers=0, pin_memory=True, shuffle=True, drop_last=True)

    def val_dataloader(self):
        return DataLoader(self.dat_val, batch_size=int(self.config[&#39;batch_size&#39;]), num_workers=0, pin_memory=True, shuffle=False)
        
    def transform(self, dataset):
        &#34;&#34;&#34;
        Transform the input dataset to latent representation.

        Args:
            dataset (MultiOmicDataset): MultiOmicDataset containing input matrices for each omics layer.

        Returns:
            pd.DataFrame: Transformed dataset as a pandas DataFrame.
        &#34;&#34;&#34;
        self.eval()
        layers = list(dataset.dat.keys())
        x_list = [dataset.dat[x] for x in layers]
        M = self.forward(x_list)[1].detach().numpy()
        z = pd.DataFrame(M)
        z.columns = [&#39;&#39;.join([&#39;E&#39;, str(x)]) for x in z.columns]
        z.index = dataset.samples
        return z
    
    def predict(self, dataset):
        &#34;&#34;&#34;
        Evaluate the model on a dataset.

        Args:
            dataset (CustomDataset): Custom dataset containing input matrices for each omics layer.

        Returns:
            predicted values.
        &#34;&#34;&#34;
        self.eval()
        layers = list(dataset.dat.keys())
        x_list = [dataset.dat[x] for x in layers]
        X_hat, z, mean, log_var, outputs = self.forward(x_list)
        
        predictions = {}
        for var in self.variables:
            y_pred = outputs[var].detach().numpy()
            if self.dataset.variable_types[var] == &#39;categorical&#39;:
                predictions[var] = np.argmax(y_pred, axis=1)
            else:
                predictions[var] = y_pred

        return predictions
    
    def compute_kernel(self, x, y):
        &#34;&#34;&#34;
        Compute the Gaussian kernel matrix between two sets of vectors.

        Args:
            x (torch.Tensor): A tensor of shape (x_size, dim) representing the first set of vectors.
            y (torch.Tensor): A tensor of shape (y_size, dim) representing the second set of vectors.

        Returns:
            torch.Tensor: The Gaussian kernel matrix of shape (x_size, y_size) computed between x and y.
        &#34;&#34;&#34;
        x_size = x.size(0)
        y_size = y.size(0)
        dim = x.size(1)
        x = x.unsqueeze(1) # (x_size, 1, dim)
        y = y.unsqueeze(0) # (1, y_size, dim)
        tiled_x = x.expand(x_size, y_size, dim)
        tiled_y = y.expand(x_size, y_size, dim)
        kernel_input = (tiled_x - tiled_y).pow(2).mean(2)/float(dim)
        return torch.exp(-kernel_input) # (x_size, y_size)

    def compute_mmd(self, x, y):
        &#34;&#34;&#34;
        Compute the maximum mean discrepancy (MMD) between two sets of vectors.

        Args:
            x (torch.Tensor): A tensor of shape (x_size, dim) representing the first set of vectors.
            y (torch.Tensor): A tensor of shape (y_size, dim) representing the second set of vectors.

        Returns:
            torch.Tensor: A scalar tensor representing the MMD between x and y.
        &#34;&#34;&#34;
        x_kernel = self.compute_kernel(x, x)
        y_kernel = self.compute_kernel(y, y)
        xy_kernel = self.compute_kernel(x, y)
        mmd = x_kernel.mean() + y_kernel.mean() - 2*xy_kernel.mean()
        return mmd

    def MMD_loss(self, latent_dim, z, xhat, x):
        &#34;&#34;&#34;
        Compute the loss function based on maximum mean discrepancy (MMD) and negative log likelihood (NLL).

        Args:
            latent_dim (int): The dimensionality of the latent space.
            z (torch.Tensor): A tensor of shape (batch_size, latent_dim) representing the latent codes.
            xhat (torch.Tensor): A tensor of shape (batch_size, dim) representing the reconstructed data.
            x (torch.Tensor): A tensor of shape (batch_size, dim) representing the original data.

        Returns:
            torch.Tensor: A scalar tensor representing the MMD loss.
        &#34;&#34;&#34;
        true_samples = torch.randn(200, latent_dim)
        mmd = self.compute_mmd(true_samples, z) # compute maximum mean discrepancy (MMD)
        nll = (xhat - x).pow(2).mean() #negative log likelihood
        return mmd+nll

    # Adaptor forward function for captum integrated gradients. 
    def forward_target(self, *args):
        input_data = list(args[:-2])  # one or more tensors (one per omics layer)
        target_var = args[-2]  # target variable of interest
        steps = args[-1]  # number of steps for IntegratedGradients().attribute 
        outputs_list = []
        for i in range(steps):
            # get list of tensors for each step into a list of tensors
            x_step = [input_data[j][i] for j in range(len(input_data))]
            x_hat_list, z, mean, log_var, outputs = self.forward(x_step)
            outputs_list.append(outputs[target_var])
        return torch.cat(outputs_list, dim = 0)
        
    def compute_feature_importance(self, target_var, steps = 5):
        &#34;&#34;&#34;
        Compute the feature importance.

        Args:
            input_data (torch.Tensor): The input data to compute the feature importance for.
            target_var (str): The target variable to compute the feature importance for.
        Returns:
            attributions (list of torch.Tensor): The feature importances for each class.
        &#34;&#34;&#34;
        x_list = [self.dataset.dat[x] for x in self.dataset.dat.keys()]
                
        # Initialize the Integrated Gradients method
        ig = IntegratedGradients(self.forward_target)

        input_data = tuple([data.unsqueeze(0).requires_grad_() for data in x_list])

        # Define a baseline (you might need to adjust this depending on your actual data)
        baseline = tuple([torch.zeros_like(data) for data in input_data])

        # Get the number of classes for the target variable
        if self.dataset.variable_types[target_var] == &#39;numerical&#39;:
            num_class = 1
        else:
            num_class = len(np.unique(self.dataset.ann[target_var]))

        # Compute the feature importance for each class
        attributions = []
        if num_class &gt; 1:
            for target_class in range(num_class):
                attributions.append(ig.attribute(input_data, baseline, additional_forward_args=(target_var, steps), target=target_class, n_steps=steps))
        else:
            attributions.append(ig.attribute(input_data, baseline, additional_forward_args=(target_var, steps), n_steps=steps))

        # summarize feature importances
        # Compute absolute attributions
        abs_attr = [[torch.abs(a) for a in attr_class] for attr_class in attributions]
        # average over samples 
        imp = [[a.mean(dim=1) for a in attr_class] for attr_class in abs_attr]

        # combine into a single data frame 
        df_list = []
        layers = list(self.dataset.dat.keys())
        for i in range(num_class):
            for j in range(len(layers)):
                features = self.dataset.features[layers[j]]
                importances = imp[i][j][0].detach().numpy()
                df_list.append(pd.DataFrame({&#39;target_variable&#39;: target_var, &#39;target_class&#39;: i, &#39;layer&#39;: layers[j], &#39;name&#39;: features, &#39;importance&#39;: importances}))    
        df_imp = pd.concat(df_list, ignore_index = True)
        
        # save scores in model
        self.feature_importances[target_var] = df_imp</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>pytorch_lightning.core.module.LightningModule</li>
<li>lightning_fabric.utilities.device_dtype_mixin._DeviceDtypeModuleMixin</li>
<li>pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin</li>
<li>pytorch_lightning.core.hooks.ModelHooks</li>
<li>pytorch_lightning.core.hooks.DataHooks</li>
<li>pytorch_lightning.core.hooks.CheckpointHooks</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="flexynesis.models.supervised_vae.MMD_loss"><code class="name flex">
<span>def <span class="ident">MMD_loss</span></span>(<span>self, latent_dim, z, xhat, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the loss function based on maximum mean discrepancy (MMD) and negative log likelihood (NLL).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>latent_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>The dimensionality of the latent space.</dd>
<dt><strong><code>z</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>A tensor of shape (batch_size, latent_dim) representing the latent codes.</dd>
<dt><strong><code>xhat</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>A tensor of shape (batch_size, dim) representing the reconstructed data.</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>A tensor of shape (batch_size, dim) representing the original data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>A scalar tensor representing the MMD loss.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def MMD_loss(self, latent_dim, z, xhat, x):
    &#34;&#34;&#34;
    Compute the loss function based on maximum mean discrepancy (MMD) and negative log likelihood (NLL).

    Args:
        latent_dim (int): The dimensionality of the latent space.
        z (torch.Tensor): A tensor of shape (batch_size, latent_dim) representing the latent codes.
        xhat (torch.Tensor): A tensor of shape (batch_size, dim) representing the reconstructed data.
        x (torch.Tensor): A tensor of shape (batch_size, dim) representing the original data.

    Returns:
        torch.Tensor: A scalar tensor representing the MMD loss.
    &#34;&#34;&#34;
    true_samples = torch.randn(200, latent_dim)
    mmd = self.compute_mmd(true_samples, z) # compute maximum mean discrepancy (MMD)
    nll = (xhat - x).pow(2).mean() #negative log likelihood
    return mmd+nll</code></pre>
</details>
</dd>
<dt id="flexynesis.models.supervised_vae.compute_feature_importance"><code class="name flex">
<span>def <span class="ident">compute_feature_importance</span></span>(<span>self, target_var, steps=5)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the feature importance.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_data</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The input data to compute the feature importance for.</dd>
<dt><strong><code>target_var</code></strong> :&ensp;<code>str</code></dt>
<dd>The target variable to compute the feature importance for.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>attributions (list of torch.Tensor): The feature importances for each class.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_feature_importance(self, target_var, steps = 5):
    &#34;&#34;&#34;
    Compute the feature importance.

    Args:
        input_data (torch.Tensor): The input data to compute the feature importance for.
        target_var (str): The target variable to compute the feature importance for.
    Returns:
        attributions (list of torch.Tensor): The feature importances for each class.
    &#34;&#34;&#34;
    x_list = [self.dataset.dat[x] for x in self.dataset.dat.keys()]
            
    # Initialize the Integrated Gradients method
    ig = IntegratedGradients(self.forward_target)

    input_data = tuple([data.unsqueeze(0).requires_grad_() for data in x_list])

    # Define a baseline (you might need to adjust this depending on your actual data)
    baseline = tuple([torch.zeros_like(data) for data in input_data])

    # Get the number of classes for the target variable
    if self.dataset.variable_types[target_var] == &#39;numerical&#39;:
        num_class = 1
    else:
        num_class = len(np.unique(self.dataset.ann[target_var]))

    # Compute the feature importance for each class
    attributions = []
    if num_class &gt; 1:
        for target_class in range(num_class):
            attributions.append(ig.attribute(input_data, baseline, additional_forward_args=(target_var, steps), target=target_class, n_steps=steps))
    else:
        attributions.append(ig.attribute(input_data, baseline, additional_forward_args=(target_var, steps), n_steps=steps))

    # summarize feature importances
    # Compute absolute attributions
    abs_attr = [[torch.abs(a) for a in attr_class] for attr_class in attributions]
    # average over samples 
    imp = [[a.mean(dim=1) for a in attr_class] for attr_class in abs_attr]

    # combine into a single data frame 
    df_list = []
    layers = list(self.dataset.dat.keys())
    for i in range(num_class):
        for j in range(len(layers)):
            features = self.dataset.features[layers[j]]
            importances = imp[i][j][0].detach().numpy()
            df_list.append(pd.DataFrame({&#39;target_variable&#39;: target_var, &#39;target_class&#39;: i, &#39;layer&#39;: layers[j], &#39;name&#39;: features, &#39;importance&#39;: importances}))    
    df_imp = pd.concat(df_list, ignore_index = True)
    
    # save scores in model
    self.feature_importances[target_var] = df_imp</code></pre>
</details>
</dd>
<dt id="flexynesis.models.supervised_vae.compute_kernel"><code class="name flex">
<span>def <span class="ident">compute_kernel</span></span>(<span>self, x, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the Gaussian kernel matrix between two sets of vectors.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>A tensor of shape (x_size, dim) representing the first set of vectors.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>A tensor of shape (y_size, dim) representing the second set of vectors.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>The Gaussian kernel matrix of shape (x_size, y_size) computed between x and y.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_kernel(self, x, y):
    &#34;&#34;&#34;
    Compute the Gaussian kernel matrix between two sets of vectors.

    Args:
        x (torch.Tensor): A tensor of shape (x_size, dim) representing the first set of vectors.
        y (torch.Tensor): A tensor of shape (y_size, dim) representing the second set of vectors.

    Returns:
        torch.Tensor: The Gaussian kernel matrix of shape (x_size, y_size) computed between x and y.
    &#34;&#34;&#34;
    x_size = x.size(0)
    y_size = y.size(0)
    dim = x.size(1)
    x = x.unsqueeze(1) # (x_size, 1, dim)
    y = y.unsqueeze(0) # (1, y_size, dim)
    tiled_x = x.expand(x_size, y_size, dim)
    tiled_y = y.expand(x_size, y_size, dim)
    kernel_input = (tiled_x - tiled_y).pow(2).mean(2)/float(dim)
    return torch.exp(-kernel_input) # (x_size, y_size)</code></pre>
</details>
</dd>
<dt id="flexynesis.models.supervised_vae.compute_loss"><code class="name flex">
<span>def <span class="ident">compute_loss</span></span>(<span>self, var, y, y_hat)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_loss(self, var, y, y_hat):
    if self.dataset.variable_types[var] == &#39;numerical&#39;:
        # Ignore instances with missing labels for numerical variables
        valid_indices = ~torch.isnan(y)
        if valid_indices.sum() &gt; 0:  # only calculate loss if there are valid targets
            y_hat = y_hat[valid_indices]
            y = y[valid_indices]
            loss = F.mse_loss(torch.flatten(y_hat), y.float())
        else:
            loss = torch.tensor(0.0, device=y_hat.device, requires_grad=True) # if no valid labels, set loss to 0
    else:
        # Ignore instances with missing labels for categorical variables
        # Assuming that missing values were encoded as -1
        valid_indices = (y != -1) &amp; (~torch.isnan(y))
        if valid_indices.sum() &gt; 0:  # only calculate loss if there are valid targets
            y_hat = y_hat[valid_indices]
            y = y[valid_indices]
            loss = F.cross_entropy(y_hat, y.long())
        else: 
            loss = torch.tensor(0.0, device=y_hat.device, requires_grad=True)
    return loss</code></pre>
</details>
</dd>
<dt id="flexynesis.models.supervised_vae.compute_mmd"><code class="name flex">
<span>def <span class="ident">compute_mmd</span></span>(<span>self, x, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the maximum mean discrepancy (MMD) between two sets of vectors.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>A tensor of shape (x_size, dim) representing the first set of vectors.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>A tensor of shape (y_size, dim) representing the second set of vectors.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>A scalar tensor representing the MMD between x and y.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_mmd(self, x, y):
    &#34;&#34;&#34;
    Compute the maximum mean discrepancy (MMD) between two sets of vectors.

    Args:
        x (torch.Tensor): A tensor of shape (x_size, dim) representing the first set of vectors.
        y (torch.Tensor): A tensor of shape (y_size, dim) representing the second set of vectors.

    Returns:
        torch.Tensor: A scalar tensor representing the MMD between x and y.
    &#34;&#34;&#34;
    x_kernel = self.compute_kernel(x, x)
    y_kernel = self.compute_kernel(y, y)
    xy_kernel = self.compute_kernel(x, y)
    mmd = x_kernel.mean() + y_kernel.mean() - 2*xy_kernel.mean()
    return mmd</code></pre>
</details>
</dd>
<dt id="flexynesis.models.supervised_vae.compute_total_loss"><code class="name flex">
<span>def <span class="ident">compute_total_loss</span></span>(<span>self, losses)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_total_loss(self, losses):
    if self.use_loss_weighting and len(losses) &gt; 1:
        # Compute weighted loss for each loss 
        # Weighted loss = precision * loss + log-variance
        total_loss = sum(torch.exp(-self.log_vars[name]) * loss + self.log_vars[name] for name, loss in losses.items())
    else:
        # Compute unweighted total loss
        total_loss = sum(losses.values())
    return total_loss</code></pre>
</details>
</dd>
<dt id="flexynesis.models.supervised_vae.configure_optimizers"><code class="name flex">
<span>def <span class="ident">configure_optimizers</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Configure the optimizer for the model.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.optim.Adam</code></dt>
<dd>Adam optimizer with learning rate 1e-3.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure_optimizers(self):
    &#34;&#34;&#34;
    Configure the optimizer for the model.

    Returns:
        torch.optim.Adam: Adam optimizer with learning rate 1e-3.
    &#34;&#34;&#34;
    optimizer = torch.optim.Adam(self.parameters(), lr=self.config[&#39;lr&#39;])
    return optimizer</code></pre>
</details>
</dd>
<dt id="flexynesis.models.supervised_vae.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x_list) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass through the model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x_list</code></strong> :&ensp;<code>list</code> of <code>torch.Tensor</code></dt>
<dd>List of input matrices for each omics layer.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>Tuple containing:
- x_hat_list (list of torch.Tensor): List of reconstructed matrices for each omics layer.
- z (torch.Tensor): Latent representation.
- mean (torch.Tensor): Concatenated mean values from each encoder.
- log_var (torch.Tensor): Concatenated log variance values from each encoder.
- y_pred (torch.Tensor): Predicted output.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x_list):
    &#34;&#34;&#34;
    Forward pass through the model.

    Args:
        x_list (list of torch.Tensor): List of input matrices for each omics layer.

    Returns:
        tuple: Tuple containing:
            - x_hat_list (list of torch.Tensor): List of reconstructed matrices for each omics layer.
            - z (torch.Tensor): Latent representation.
            - mean (torch.Tensor): Concatenated mean values from each encoder.
            - log_var (torch.Tensor): Concatenated log variance values from each encoder.
            - y_pred (torch.Tensor): Predicted output.
    &#34;&#34;&#34;
    mean, log_var = self.multi_encoder(x_list)
    
    # generate latent layer
    z = self.reparameterization(mean, log_var)

    # Decode each latent variable with its corresponding Decoder
    x_hat_list = [self.decoders[i](z) for i in range(len(x_list))]

    #run the supervisor heads using the latent layer as input
    outputs = {}
    for var, mlp in self.MLPs.items():
        outputs[var] = mlp(z)
        
    return x_hat_list, z, mean, log_var, outputs</code></pre>
</details>
</dd>
<dt id="flexynesis.models.supervised_vae.forward_target"><code class="name flex">
<span>def <span class="ident">forward_target</span></span>(<span>self, *args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_target(self, *args):
    input_data = list(args[:-2])  # one or more tensors (one per omics layer)
    target_var = args[-2]  # target variable of interest
    steps = args[-1]  # number of steps for IntegratedGradients().attribute 
    outputs_list = []
    for i in range(steps):
        # get list of tensors for each step into a list of tensors
        x_step = [input_data[j][i] for j in range(len(input_data))]
        x_hat_list, z, mean, log_var, outputs = self.forward(x_step)
        outputs_list.append(outputs[target_var])
    return torch.cat(outputs_list, dim = 0)</code></pre>
</details>
</dd>
<dt id="flexynesis.models.supervised_vae.multi_encoder"><code class="name flex">
<span>def <span class="ident">multi_encoder</span></span>(<span>self, x_list)</span>
</code></dt>
<dd>
<div class="desc"><p>Encode each input matrix separately using the corresponding Encoder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x_list</code></strong> :&ensp;<code>list</code> of <code>torch.Tensor</code></dt>
<dd>List of input matrices for each omics layer.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>Tuple containing:
- mean (torch.Tensor): Concatenated mean values from each encoder.
- log_var (torch.Tensor): Concatenated log variance values from each encoder.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def multi_encoder(self, x_list):
    &#34;&#34;&#34;
    Encode each input matrix separately using the corresponding Encoder.

    Args:
        x_list (list of torch.Tensor): List of input matrices for each omics layer.

    Returns:
        tuple: Tuple containing:
            - mean (torch.Tensor): Concatenated mean values from each encoder.
            - log_var (torch.Tensor): Concatenated log variance values from each encoder.
    &#34;&#34;&#34;
    means, log_vars = [], []
    # Process each input matrix with its corresponding Encoder
    for i, x in enumerate(x_list):
        mean, log_var = self.encoders[i](x)
        means.append(mean)
        log_vars.append(log_var)

    # Concatenate means and log_vars
    # Push concatenated means and log_vars through the fully connected layers
    mean = self.FC_mean(torch.cat(means, dim=1))
    log_var = self.FC_log_var(torch.cat(log_vars, dim=1))
    return mean, log_var</code></pre>
</details>
</dd>
<dt id="flexynesis.models.supervised_vae.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, dataset)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the model on a dataset.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>CustomDataset</code></dt>
<dd>Custom dataset containing input matrices for each omics layer.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>predicted values.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, dataset):
    &#34;&#34;&#34;
    Evaluate the model on a dataset.

    Args:
        dataset (CustomDataset): Custom dataset containing input matrices for each omics layer.

    Returns:
        predicted values.
    &#34;&#34;&#34;
    self.eval()
    layers = list(dataset.dat.keys())
    x_list = [dataset.dat[x] for x in layers]
    X_hat, z, mean, log_var, outputs = self.forward(x_list)
    
    predictions = {}
    for var in self.variables:
        y_pred = outputs[var].detach().numpy()
        if self.dataset.variable_types[var] == &#39;categorical&#39;:
            predictions[var] = np.argmax(y_pred, axis=1)
        else:
            predictions[var] = y_pred

    return predictions</code></pre>
</details>
</dd>
<dt id="flexynesis.models.supervised_vae.prepare_data"><code class="name flex">
<span>def <span class="ident">prepare_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Use this to download and prepare data. Downloading and saving data with multiple processes (distributed
settings) will result in corrupted data. Lightning ensures this method is called only within a single process,
so you can safely add your downloading logic within.</p>
<div class="admonition warning">
<p class="admonition-title">Warning:&ensp;DO NOT set state to the model (use <code>setup</code> instead)</p>
<p>since this is NOT called on every device</p>
</div>
<p>Example::</p>
<pre><code>def prepare_data(self):
    # good
    download_data()
    tokenize()
    etc()

    # bad
    self.split = data_split
    self.some_state = some_other_state()
</code></pre>
<p>In a distributed environment, <code>prepare_data</code> can be called in two ways
(using :ref:<code>prepare_data_per_node&lt;common/lightning_module:prepare_data_per_node&gt;</code>)</p>
<ol>
<li>Once per node. This is the default and is only called on LOCAL_RANK=0.</li>
<li>Once in total. Only called on GLOBAL_RANK=0.</li>
</ol>
<p>Example::</p>
<pre><code># DEFAULT
# called once per node on LOCAL_RANK=0 of that node
class LitDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()
        self.prepare_data_per_node = True


# call on GLOBAL_RANK=0 (great for shared file systems)
class LitDataModule(LightningDataModule):
    def __init__(self):
        super().__init__()
        self.prepare_data_per_node = False
</code></pre>
<p>This is called before requesting the dataloaders:</p>
<p>.. code-block:: python</p>
<pre><code>model.prepare_data()
initialize_distributed()
model.setup(stage)
model.train_dataloader()
model.val_dataloader()
model.test_dataloader()
model.predict_dataloader()
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_data(self):
    lt = int(len(self.dataset)*(1-self.val_size))
    lv = len(self.dataset)-lt
    dat_train, dat_val = random_split(self.dataset, [lt, lv], 
                                      generator=torch.Generator().manual_seed(42))
    return dat_train, dat_val</code></pre>
</details>
</dd>
<dt id="flexynesis.models.supervised_vae.reparameterization"><code class="name flex">
<span>def <span class="ident">reparameterization</span></span>(<span>self, mean, var)</span>
</code></dt>
<dd>
<div class="desc"><p>Reparameterize the mean and variance values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mean</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Mean values from the encoders.</dd>
<dt><strong><code>var</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Variance values from the encoders.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Latent representation.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reparameterization(self, mean, var):
    &#34;&#34;&#34;
    Reparameterize the mean and variance values.

    Args:
        mean (torch.Tensor): Mean values from the encoders.
        var (torch.Tensor): Variance values from the encoders.

    Returns:
        torch.Tensor: Latent representation.
    &#34;&#34;&#34;
    epsilon = torch.randn_like(var)       
    z = mean + var*epsilon                         
    return z</code></pre>
</details>
</dd>
<dt id="flexynesis.models.supervised_vae.train_dataloader"><code class="name flex">
<span>def <span class="ident">train_dataloader</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>An iterable or collection of iterables specifying training samples.</p>
<p>For more information about multiple dataloaders, see this :ref:<code>section &lt;multiple-dataloaders&gt;</code>.</p>
<p>The dataloader you return will not be reloaded unless you set
:paramref:<code>~pytorch_lightning.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs</code> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<pre><code>- download in :meth:&lt;code&gt;prepare\_data&lt;/code&gt;
- process and split in :meth:&lt;code&gt;setup&lt;/code&gt;
</code></pre>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning:&ensp;do not assign state in prepare_data</p>
</div>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.trainer.Trainer.fit</code></li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_dataloader(self):
    return DataLoader(self.dat_train, batch_size=int(self.config[&#39;batch_size&#39;]), num_workers=0, pin_memory=True, shuffle=True, drop_last=True)</code></pre>
</details>
</dd>
<dt id="flexynesis.models.supervised_vae.training_step"><code class="name flex">
<span>def <span class="ident">training_step</span></span>(<span>self, train_batch, batch_idx)</span>
</code></dt>
<dd>
<div class="desc"><p>Here you compute and return the training loss and some additional metrics for e.g. the progress bar or
logger.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>The output of your data iterable, normally a :class:<code>~torch.utils.data.DataLoader</code>.</dd>
<dt><strong><code>batch_idx</code></strong></dt>
<dd>The index of this batch.</dd>
<dt><strong><code>dataloader_idx</code></strong></dt>
<dd>The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</dd>
</dl>
<h2 id="return">Return</h2>
<ul>
<li>:class:<code>~torch.Tensor</code> - The loss tensor</li>
<li><code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code>.</li>
<li><code>None</code> - Skip to the next batch. This is only supported for automatic optimization.
This is not supported for multi-GPU, TPU, IPU, or DeepSpeed.</li>
</ul>
<p>In this step you'd normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<p>Example::</p>
<pre><code>def training_step(self, batch, batch_idx):
    x, y, z = batch
    out = self.encoder(x)
    loss = self.loss(out, x)
    return loss
</code></pre>
<p>To use multiple optimizers, you can switch to 'manual optimization' and control their stepping:</p>
<p>.. code-block:: python</p>
<pre><code>def __init__(self):
    super().__init__()
    self.automatic_optimization = False


# Multiple optimizers (e.g.: GANs)
def training_step(self, batch, batch_idx):
    opt1, opt2 = self.optimizers()

    # do training_step with encoder
    ...
    opt1.step()
    # do training_step with decoder
    ...
    opt2.step()
</code></pre>
<h2 id="note">Note</h2>
<p>When <code>accumulate_grad_batches</code> &gt; 1, the loss returned here will be automatically
normalized by <code>accumulate_grad_batches</code> internally.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def training_step(self, train_batch, batch_idx):
    dat, y_dict = train_batch
    layers = dat.keys()
    x_list = [dat[x] for x in layers]
    
    x_hat_list, z, mean, log_var, outputs = self.forward(x_list)
    
    # compute mmd loss for each layer and take average
    mmd_loss_list = [self.MMD_loss(z.shape[1], z, x_hat_list[i], x_list[i]) for i in range(len(layers))]
    mmd_loss = torch.mean(torch.stack(mmd_loss_list))

    # compute loss values for the supervisor heads 
    losses = {&#39;mmd_loss&#39;: mmd_loss}
    
    for var in self.variables:
        y_hat = outputs[var]
        y = y_dict[var]
        loss = self.compute_loss(var, y, y_hat)
        losses[var] = loss
        
    total_loss = self.compute_total_loss(losses)
    # add total loss for logging 
    losses[&#39;train_loss&#39;] = total_loss
    self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True)
    return total_loss</code></pre>
</details>
</dd>
<dt id="flexynesis.models.supervised_vae.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, dataset)</span>
</code></dt>
<dd>
<div class="desc"><p>Transform the input dataset to latent representation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>MultiOmicDataset</code></dt>
<dd>MultiOmicDataset containing input matrices for each omics layer.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>Transformed dataset as a pandas DataFrame.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, dataset):
    &#34;&#34;&#34;
    Transform the input dataset to latent representation.

    Args:
        dataset (MultiOmicDataset): MultiOmicDataset containing input matrices for each omics layer.

    Returns:
        pd.DataFrame: Transformed dataset as a pandas DataFrame.
    &#34;&#34;&#34;
    self.eval()
    layers = list(dataset.dat.keys())
    x_list = [dataset.dat[x] for x in layers]
    M = self.forward(x_list)[1].detach().numpy()
    z = pd.DataFrame(M)
    z.columns = [&#39;&#39;.join([&#39;E&#39;, str(x)]) for x in z.columns]
    z.index = dataset.samples
    return z</code></pre>
</details>
</dd>
<dt id="flexynesis.models.supervised_vae.val_dataloader"><code class="name flex">
<span>def <span class="ident">val_dataloader</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>An iterable or collection of iterables specifying validation samples.</p>
<p>For more information about multiple dataloaders, see this :ref:<code>section &lt;multiple-dataloaders&gt;</code>.</p>
<p>The dataloader you return will not be reloaded unless you set
:paramref:<code>~pytorch_lightning.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs</code> to
a positive integer.</p>
<p>It's recommended that all data downloads and preparation happen in :meth:<code>prepare_data</code>.</p>
<ul>
<li>:meth:<code>~pytorch_lightning.trainer.trainer.Trainer.fit</code></li>
<li>:meth:<code>~pytorch_lightning.trainer.trainer.Trainer.validate</code></li>
<li>:meth:<code>prepare_data</code></li>
<li>:meth:<code>setup</code></li>
</ul>
<h2 id="note">Note</h2>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
<h2 id="note_1">Note</h2>
<p>If you don't need a validation dataset and a :meth:<code>validation_step</code>, you don't need to
implement this method.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def val_dataloader(self):
    return DataLoader(self.dat_val, batch_size=int(self.config[&#39;batch_size&#39;]), num_workers=0, pin_memory=True, shuffle=False)</code></pre>
</details>
</dd>
<dt id="flexynesis.models.supervised_vae.validation_step"><code class="name flex">
<span>def <span class="ident">validation_step</span></span>(<span>self, val_batch, batch_idx)</span>
</code></dt>
<dd>
<div class="desc"><p>Operates on a single batch of data from the validation set. In this step you'd might generate examples or
calculate anything of interest like accuracy.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>The output of your data iterable, normally a :class:<code>~torch.utils.data.DataLoader</code>.</dd>
<dt><strong><code>batch_idx</code></strong></dt>
<dd>The index of this batch.</dd>
<dt><strong><code>dataloader_idx</code></strong></dt>
<dd>The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</dd>
</dl>
<h2 id="return">Return</h2>
<ul>
<li>:class:<code>~torch.Tensor</code> - The loss tensor</li>
<li><code>dict</code> - A dictionary. Can include any keys, but must include the key <code>'loss'</code>.</li>
<li><code>None</code> - Skip to the next batch.</li>
</ul>
<p>.. code-block:: python</p>
<pre><code># if you have one val dataloader:
def validation_step(self, batch, batch_idx):
    ...


# if you have multiple val dataloaders:
def validation_step(self, batch, batch_idx, dataloader_idx=0):
    ...
</code></pre>
<p>Examples::</p>
<pre><code># CASE 1: A single validation dataset
def validation_step(self, batch, batch_idx):
    x, y = batch

    # implement your own
    out = self(x)
    loss = self.loss(out, y)

    # log 6 example images
    # or generated text... or whatever
    sample_imgs = x[:6]
    grid = torchvision.utils.make_grid(sample_imgs)
    self.logger.experiment.add_image('example_images', grid, 0)

    # calculate acc
    labels_hat = torch.argmax(out, dim=1)
    val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)

    # log the outputs!
    self.log_dict({'val_loss': loss, 'val_acc': val_acc})
</code></pre>
<p>If you pass in multiple val dataloaders, :meth:<code>validation_step</code> will have an additional argument. We recommend
setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>
<p>.. code-block:: python</p>
<pre><code># CASE 2: multiple validation dataloaders
def validation_step(self, batch, batch_idx, dataloader_idx=0):
    # dataloader_idx tells you which dataset this is.
    ...
</code></pre>
<h2 id="note">Note</h2>
<p>If you don't need to validate you don't need to implement this method.</p>
<h2 id="note_1">Note</h2>
<p>When the :meth:<code>validation_step</code> is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validation_step(self, val_batch, batch_idx):
    dat, y_dict = val_batch
    layers = dat.keys()
    x_list = [dat[x] for x in layers]
    
    x_hat_list, z, mean, log_var, outputs = self.forward(x_list)
    
    # compute mmd loss for each layer and take average
    mmd_loss_list = [self.MMD_loss(z.shape[1], z, x_hat_list[i], x_list[i]) for i in range(len(layers))]
    mmd_loss = torch.mean(torch.stack(mmd_loss_list))

    # compute loss values for the supervisor heads 
    losses = {&#39;mmd_loss&#39;: mmd_loss}
    for var in self.variables:
        y_hat = outputs[var]
        y = y_dict[var]
        loss = self.compute_loss(var, y, y_hat)
        losses[var] = loss
        
    total_loss = sum(losses.values())
    losses[&#39;val_loss&#39;] = total_loss
    self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True)
    return total_loss</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="flexynesis" href="../index.html">flexynesis</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="flexynesis.models.direct_pred" href="direct_pred.html">flexynesis.models.direct_pred</a></code></li>
<li><code><a title="flexynesis.models.direct_pred_cnn" href="direct_pred_cnn.html">flexynesis.models.direct_pred_cnn</a></code></li>
<li><code><a title="flexynesis.models.direct_pred_gcnn" href="direct_pred_gcnn.html">flexynesis.models.direct_pred_gcnn</a></code></li>
<li><code><a title="flexynesis.models.triplet_encoder" href="triplet_encoder.html">flexynesis.models.triplet_encoder</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="flexynesis.models.DirectPred" href="#flexynesis.models.DirectPred">DirectPred</a></code></h4>
<ul class="">
<li><code><a title="flexynesis.models.DirectPred.compute_feature_importance" href="#flexynesis.models.DirectPred.compute_feature_importance">compute_feature_importance</a></code></li>
<li><code><a title="flexynesis.models.DirectPred.compute_loss" href="#flexynesis.models.DirectPred.compute_loss">compute_loss</a></code></li>
<li><code><a title="flexynesis.models.DirectPred.compute_total_loss" href="#flexynesis.models.DirectPred.compute_total_loss">compute_total_loss</a></code></li>
<li><code><a title="flexynesis.models.DirectPred.configure_optimizers" href="#flexynesis.models.DirectPred.configure_optimizers">configure_optimizers</a></code></li>
<li><code><a title="flexynesis.models.DirectPred.forward" href="#flexynesis.models.DirectPred.forward">forward</a></code></li>
<li><code><a title="flexynesis.models.DirectPred.forward_target" href="#flexynesis.models.DirectPred.forward_target">forward_target</a></code></li>
<li><code><a title="flexynesis.models.DirectPred.predict" href="#flexynesis.models.DirectPred.predict">predict</a></code></li>
<li><code><a title="flexynesis.models.DirectPred.prepare_data" href="#flexynesis.models.DirectPred.prepare_data">prepare_data</a></code></li>
<li><code><a title="flexynesis.models.DirectPred.train_dataloader" href="#flexynesis.models.DirectPred.train_dataloader">train_dataloader</a></code></li>
<li><code><a title="flexynesis.models.DirectPred.training_step" href="#flexynesis.models.DirectPred.training_step">training_step</a></code></li>
<li><code><a title="flexynesis.models.DirectPred.transform" href="#flexynesis.models.DirectPred.transform">transform</a></code></li>
<li><code><a title="flexynesis.models.DirectPred.val_dataloader" href="#flexynesis.models.DirectPred.val_dataloader">val_dataloader</a></code></li>
<li><code><a title="flexynesis.models.DirectPred.validation_step" href="#flexynesis.models.DirectPred.validation_step">validation_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flexynesis.models.DirectPredCNN" href="#flexynesis.models.DirectPredCNN">DirectPredCNN</a></code></h4>
<ul class="">
<li><code><a title="flexynesis.models.DirectPredCNN.compute_feature_importance" href="#flexynesis.models.DirectPredCNN.compute_feature_importance">compute_feature_importance</a></code></li>
<li><code><a title="flexynesis.models.DirectPredCNN.compute_loss" href="#flexynesis.models.DirectPredCNN.compute_loss">compute_loss</a></code></li>
<li><code><a title="flexynesis.models.DirectPredCNN.configure_optimizers" href="#flexynesis.models.DirectPredCNN.configure_optimizers">configure_optimizers</a></code></li>
<li><code><a title="flexynesis.models.DirectPredCNN.forward" href="#flexynesis.models.DirectPredCNN.forward">forward</a></code></li>
<li><code><a title="flexynesis.models.DirectPredCNN.forward_target" href="#flexynesis.models.DirectPredCNN.forward_target">forward_target</a></code></li>
<li><code><a title="flexynesis.models.DirectPredCNN.predict" href="#flexynesis.models.DirectPredCNN.predict">predict</a></code></li>
<li><code><a title="flexynesis.models.DirectPredCNN.prepare_data" href="#flexynesis.models.DirectPredCNN.prepare_data">prepare_data</a></code></li>
<li><code><a title="flexynesis.models.DirectPredCNN.train_dataloader" href="#flexynesis.models.DirectPredCNN.train_dataloader">train_dataloader</a></code></li>
<li><code><a title="flexynesis.models.DirectPredCNN.training_step" href="#flexynesis.models.DirectPredCNN.training_step">training_step</a></code></li>
<li><code><a title="flexynesis.models.DirectPredCNN.transform" href="#flexynesis.models.DirectPredCNN.transform">transform</a></code></li>
<li><code><a title="flexynesis.models.DirectPredCNN.val_dataloader" href="#flexynesis.models.DirectPredCNN.val_dataloader">val_dataloader</a></code></li>
<li><code><a title="flexynesis.models.DirectPredCNN.validation_step" href="#flexynesis.models.DirectPredCNN.validation_step">validation_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flexynesis.models.DirectPredGCNN" href="#flexynesis.models.DirectPredGCNN">DirectPredGCNN</a></code></h4>
<ul class="">
<li><code><a title="flexynesis.models.DirectPredGCNN.compute_feature_importance" href="#flexynesis.models.DirectPredGCNN.compute_feature_importance">compute_feature_importance</a></code></li>
<li><code><a title="flexynesis.models.DirectPredGCNN.compute_loss" href="#flexynesis.models.DirectPredGCNN.compute_loss">compute_loss</a></code></li>
<li><code><a title="flexynesis.models.DirectPredGCNN.compute_total_loss" href="#flexynesis.models.DirectPredGCNN.compute_total_loss">compute_total_loss</a></code></li>
<li><code><a title="flexynesis.models.DirectPredGCNN.configure_optimizers" href="#flexynesis.models.DirectPredGCNN.configure_optimizers">configure_optimizers</a></code></li>
<li><code><a title="flexynesis.models.DirectPredGCNN.forward" href="#flexynesis.models.DirectPredGCNN.forward">forward</a></code></li>
<li><code><a title="flexynesis.models.DirectPredGCNN.forward_target" href="#flexynesis.models.DirectPredGCNN.forward_target">forward_target</a></code></li>
<li><code><a title="flexynesis.models.DirectPredGCNN.predict" href="#flexynesis.models.DirectPredGCNN.predict">predict</a></code></li>
<li><code><a title="flexynesis.models.DirectPredGCNN.prepare_data" href="#flexynesis.models.DirectPredGCNN.prepare_data">prepare_data</a></code></li>
<li><code><a title="flexynesis.models.DirectPredGCNN.train_dataloader" href="#flexynesis.models.DirectPredGCNN.train_dataloader">train_dataloader</a></code></li>
<li><code><a title="flexynesis.models.DirectPredGCNN.training_step" href="#flexynesis.models.DirectPredGCNN.training_step">training_step</a></code></li>
<li><code><a title="flexynesis.models.DirectPredGCNN.transform" href="#flexynesis.models.DirectPredGCNN.transform">transform</a></code></li>
<li><code><a title="flexynesis.models.DirectPredGCNN.val_dataloader" href="#flexynesis.models.DirectPredGCNN.val_dataloader">val_dataloader</a></code></li>
<li><code><a title="flexynesis.models.DirectPredGCNN.validation_step" href="#flexynesis.models.DirectPredGCNN.validation_step">validation_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flexynesis.models.MultiTripletNetwork" href="#flexynesis.models.MultiTripletNetwork">MultiTripletNetwork</a></code></h4>
<ul class="">
<li><code><a title="flexynesis.models.MultiTripletNetwork.compute_feature_importance" href="#flexynesis.models.MultiTripletNetwork.compute_feature_importance">compute_feature_importance</a></code></li>
<li><code><a title="flexynesis.models.MultiTripletNetwork.compute_loss" href="#flexynesis.models.MultiTripletNetwork.compute_loss">compute_loss</a></code></li>
<li><code><a title="flexynesis.models.MultiTripletNetwork.compute_total_loss" href="#flexynesis.models.MultiTripletNetwork.compute_total_loss">compute_total_loss</a></code></li>
<li><code><a title="flexynesis.models.MultiTripletNetwork.configure_optimizers" href="#flexynesis.models.MultiTripletNetwork.configure_optimizers">configure_optimizers</a></code></li>
<li><code><a title="flexynesis.models.MultiTripletNetwork.forward" href="#flexynesis.models.MultiTripletNetwork.forward">forward</a></code></li>
<li><code><a title="flexynesis.models.MultiTripletNetwork.forward_target" href="#flexynesis.models.MultiTripletNetwork.forward_target">forward_target</a></code></li>
<li><code><a title="flexynesis.models.MultiTripletNetwork.predict" href="#flexynesis.models.MultiTripletNetwork.predict">predict</a></code></li>
<li><code><a title="flexynesis.models.MultiTripletNetwork.prepare_data" href="#flexynesis.models.MultiTripletNetwork.prepare_data">prepare_data</a></code></li>
<li><code><a title="flexynesis.models.MultiTripletNetwork.train_dataloader" href="#flexynesis.models.MultiTripletNetwork.train_dataloader">train_dataloader</a></code></li>
<li><code><a title="flexynesis.models.MultiTripletNetwork.training_step" href="#flexynesis.models.MultiTripletNetwork.training_step">training_step</a></code></li>
<li><code><a title="flexynesis.models.MultiTripletNetwork.transform" href="#flexynesis.models.MultiTripletNetwork.transform">transform</a></code></li>
<li><code><a title="flexynesis.models.MultiTripletNetwork.triplet_loss" href="#flexynesis.models.MultiTripletNetwork.triplet_loss">triplet_loss</a></code></li>
<li><code><a title="flexynesis.models.MultiTripletNetwork.val_dataloader" href="#flexynesis.models.MultiTripletNetwork.val_dataloader">val_dataloader</a></code></li>
<li><code><a title="flexynesis.models.MultiTripletNetwork.validation_step" href="#flexynesis.models.MultiTripletNetwork.validation_step">validation_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="flexynesis.models.supervised_vae" href="#flexynesis.models.supervised_vae">supervised_vae</a></code></h4>
<ul class="">
<li><code><a title="flexynesis.models.supervised_vae.MMD_loss" href="#flexynesis.models.supervised_vae.MMD_loss">MMD_loss</a></code></li>
<li><code><a title="flexynesis.models.supervised_vae.compute_feature_importance" href="#flexynesis.models.supervised_vae.compute_feature_importance">compute_feature_importance</a></code></li>
<li><code><a title="flexynesis.models.supervised_vae.compute_kernel" href="#flexynesis.models.supervised_vae.compute_kernel">compute_kernel</a></code></li>
<li><code><a title="flexynesis.models.supervised_vae.compute_loss" href="#flexynesis.models.supervised_vae.compute_loss">compute_loss</a></code></li>
<li><code><a title="flexynesis.models.supervised_vae.compute_mmd" href="#flexynesis.models.supervised_vae.compute_mmd">compute_mmd</a></code></li>
<li><code><a title="flexynesis.models.supervised_vae.compute_total_loss" href="#flexynesis.models.supervised_vae.compute_total_loss">compute_total_loss</a></code></li>
<li><code><a title="flexynesis.models.supervised_vae.configure_optimizers" href="#flexynesis.models.supervised_vae.configure_optimizers">configure_optimizers</a></code></li>
<li><code><a title="flexynesis.models.supervised_vae.forward" href="#flexynesis.models.supervised_vae.forward">forward</a></code></li>
<li><code><a title="flexynesis.models.supervised_vae.forward_target" href="#flexynesis.models.supervised_vae.forward_target">forward_target</a></code></li>
<li><code><a title="flexynesis.models.supervised_vae.multi_encoder" href="#flexynesis.models.supervised_vae.multi_encoder">multi_encoder</a></code></li>
<li><code><a title="flexynesis.models.supervised_vae.predict" href="#flexynesis.models.supervised_vae.predict">predict</a></code></li>
<li><code><a title="flexynesis.models.supervised_vae.prepare_data" href="#flexynesis.models.supervised_vae.prepare_data">prepare_data</a></code></li>
<li><code><a title="flexynesis.models.supervised_vae.reparameterization" href="#flexynesis.models.supervised_vae.reparameterization">reparameterization</a></code></li>
<li><code><a title="flexynesis.models.supervised_vae.train_dataloader" href="#flexynesis.models.supervised_vae.train_dataloader">train_dataloader</a></code></li>
<li><code><a title="flexynesis.models.supervised_vae.training_step" href="#flexynesis.models.supervised_vae.training_step">training_step</a></code></li>
<li><code><a title="flexynesis.models.supervised_vae.transform" href="#flexynesis.models.supervised_vae.transform">transform</a></code></li>
<li><code><a title="flexynesis.models.supervised_vae.val_dataloader" href="#flexynesis.models.supervised_vae.val_dataloader">val_dataloader</a></code></li>
<li><code><a title="flexynesis.models.supervised_vae.validation_step" href="#flexynesis.models.supervised_vae.validation_step">validation_step</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>